"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ (V12)
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-01-29 (ë°ì´í„° ëˆ„ì  ì €ì¥ + ìµœì‹  íŒŒì¼ ìë™ ë¡œë“œ + KST)
"""

import os
import json
import time
import glob
from datetime import datetime, timedelta, timezone
import difflib
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜ ë° ì‹œê°„
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"

# í•œêµ­ ì‹œê°„(KST)
KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
FILE_TIMESTAMP = NOW.strftime("%Y%m%d_%H%M%S")
DISPLAY_DATE = NOW.strftime("%Y-%m-%d")
DISPLAY_TIME = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup');
            popups.forEach(function(element) { element.remove(); });
        """)
    except:
        pass

def scroll_to_bottom(driver):
    """ìŠ¤í¬ë¡¤ 10ë²ˆ (ë°ì´í„° ë¡œë”© ìœ ë„)"""
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(10): 
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
    except:
        pass

def clean_html(html_source):
    soup = BeautifulSoup(html_source, 'html.parser')
    
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside']):
        tag.decompose()

    for hidden in soup.find_all(attrs={"style": True}):
        if "display:none" in hidden["style"].replace(" ", "").lower():
            hidden.decompose()
            
    trash_ids = ['across_adn_container', 'criteo-tags-div', 'kakao-pixel-id', 'facebook-pixel-id']
    for t_id in trash_ids:
        tag = soup.find(id=t_id)
        if tag: tag.decompose()
        
    body = soup.find('body')
    return body.prettify() if body else "No Content"

# [NEW] ê°€ì¥ ìµœê·¼ì— ì €ì¥ëœ ë°ì´í„° íŒŒì¼ì„ ì°¾ì•„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
def load_previous_data():
    """
    data í´ë”ì—ì„œ 'data_YYYYMMDD_HHMMSS.json' íŒ¨í„´ì˜ íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ
    ê°€ì¥ ìµœì‹  íŒŒì¼ í•˜ë‚˜ë¥¼ ë¡œë“œí•˜ì—¬ ë°˜í™˜í•¨.
    """
    json_files = glob.glob(os.path.join(DATA_DIR, "data_*.json"))
    
    if not json_files:
        print("âš ï¸ ì´ì „ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ìµœì´ˆ ì‹¤í–‰)")
        return {}
    
    # íŒŒì¼ëª… ì •ë ¬ (ë‚ ì§œ/ì‹œê°„ì´ ì´ë¦„ì— ìˆìœ¼ë¯€ë¡œ ë¬¸ìì—´ ì •ë ¬í•˜ë©´ ì‹œê°„ìˆœ ì •ë ¬ë¨)
    json_files.sort()
    latest_file = json_files[-1] # ê°€ì¥ ë§ˆì§€ë§‰ íŒŒì¼ì´ ìµœì‹ 
    
    print(f"ğŸ“‚ ì´ì „ ë°ì´í„° ë¡œë“œ: {os.path.basename(latest_file)}")
    
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸ ì´ì „ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def analyze_content_changes(old_html, new_html):
    soup_old = BeautifulSoup(old_html, 'html.parser')
    soup_new = BeautifulSoup(new_html, 'html.parser')
    summary = []
    
    if soup_old.get_text().strip() != soup_new.get_text().strip():
        summary.append("âœï¸ ìƒì„¸ë‚´ìš©(í…ìŠ¤íŠ¸) ìˆ˜ì •")
    
    imgs_old = set([i['src'] for i in soup_old.find_all('img') if i.get('src')])
    imgs_new = set([i['src'] for i in soup_new.find_all('img') if i.get('src')])
    if imgs_old != imgs_new:
        summary.append("ğŸ–¼ï¸ ìƒì„¸ì´ë¯¸ì§€ êµì²´")
        
    return " / ".join(summary) if summary else "ğŸ¨ ë””ìì¸/ë ˆì´ì•„ì›ƒ ë³€ê²½"

def extract_cards_smartly(driver, container_selector):
    cards_data = {} 
    
    try:
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, container_selector))
        )
        
        items = container.find_elements(By.TAG_NAME, "li")
        if not items:
            items = container.find_elements(By.XPATH, "./div") 
            
        if not items:
            raise Exception("No items found")

        print(f"    found {len(items)} items (cards)")

        for item in items:
            try:
                link_el = None
                try:
                    link_el = item.find_element(By.TAG_NAME, "a")
                except:
                    continue
                
                href = link_el.get_attribute('href')
                if not href or href.startswith("javascript"): continue

                title = item.text.strip().split("\n")[0]
                if not title:
                    imgs = item.find_elements(By.TAG_NAME, "img")
                    if imgs: title = imgs[0].get_attribute("alt")
                if not title: title = "ì œëª© ì—†ìŒ"

                img_src = ""
                imgs = item.find_elements(By.TAG_NAME, "img")
                for img in imgs:
                    src = img.get_attribute("src")
                    if src and "logo" not in src and "icon" not in src:
                        img_src = src
                        break
                
                cards_data[href] = {"title": title, "img": img_src}

            except Exception as e:
                continue
                
        return cards_data

    except Exception as e:
        print(f"    âš ï¸ ì¹´ë“œ ì¶”ì¶œ ì‹¤íŒ¨ ({e}) -> ì¼ë°˜ ë§í¬ ìˆ˜ì§‘ìœ¼ë¡œ ì „í™˜")
        return None 

def extract_links_fallback(driver, base_url):
    links = driver.find_elements(By.TAG_NAME, "a")
    extracted = {}
    for link in links:
        try:
            href = link.get_attribute('href')
            if href and ('event' in href or 'view' in href or 'detail' in href) and not href.startswith('#'):
                if href.startswith('/'):
                    from urllib.parse import urljoin
                    href = urljoin(base_url, href)
                extracted[href] = {"title": link.text.strip() or "ì œëª© ì—†ìŒ", "img": ""}
        except:
            continue
    return extracted

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    collected_items = {} 
    last_page_urls = []
    page = 1
    
    while True:
        if pagination_param:
            connector = '&' if '?' in base_url else '?'
            target_url = f"{base_url}{connector}{pagination_param}={page}"
        else:
            target_url = base_url
            
        try:
            driver.get(target_url)
            time.sleep(5)
            remove_popups(driver)
            scroll_to_bottom(driver)
            
            page_data = {}
            if target_selector:
                page_data = extract_cards_smartly(driver, target_selector)
            
            if not page_data: 
                page_data = extract_links_fallback(driver, base_url)
            
            clean_page_data = {}
            for href, info in page_data.items():
                if href.startswith('/'):
                    from urllib.parse import urljoin
                    href = urljoin(base_url, href)
                clean_page_data[href] = info

            if page == 1:
                print(f"  - Page {page}: {len(clean_page_data)}ê°œ í•­ëª© ë°œê²¬")
            
            if not clean_page_data: break
            
            current_urls = sorted(list(clean_page_data.keys()))
            if current_urls == sorted(last_page_urls): break
            
            collected_items.update(clean_page_data)
            
            if not pagination_param: break
            last_page_urls = current_urls
            page += 1
            if page > 10: break

        except Exception as e:
            print(f"  âš ï¸ ì˜¤ë¥˜: {e}")
            break

    print(f"  ğŸ” ìƒì„¸ ë¶„ì„ ì¤‘ ({len(collected_items)}ê±´)...")
    
    for url, info in collected_items.items():
        try:
            driver.get(url)
            time.sleep(1)
            remove_popups(driver)
            collected_items[url]['content'] = clean_html(driver.page_source)
        except:
            collected_items[url]['content'] = "" 
            
    return collected_items

def update_index_page():
    report_files = glob.glob(os.path.join(REPORT_DIR, "report_*.html"))
    report_files.sort(reverse=True)
    
    index_html = f"""
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§</title>
        <style>
            body {{ font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background: #f9f9f9; }}
            h1 {{ border-bottom: 2px solid #0056b3; padding-bottom: 10px; }}
            .card {{ background: white; padding: 15px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }}
            a {{ text-decoration: none; color: #0056b3; font-weight: bold; }}
            .badge {{ background: #28a745; color: white; padding: 3px 8px; border-radius: 10px; font-size: 0.8em; }}
        </style>
    </head>
    <body>
        <h1>ğŸ“Š ëª¨ë‹ˆí„°ë§ ì•„ì¹´ì´ë¸Œ</h1>
        <p>í˜„ì¬ ì‹œê°: {DISPLAY_DATE} {DISPLAY_TIME} (KST)</p>
    """
    
    if not report_files: index_html += "<p>ë°ì´í„° ì—†ìŒ</p>"
    
    for f in report_files:
        name = os.path.basename(f)
        ts = name.replace("report_", "").replace(".html", "")
        try:
            dt = datetime.strptime(ts, "%Y%m%d_%H%M%S")
            disp = dt.strftime("%Y-%m-%d %H:%M:%S")
        except: disp = ts
        
        badge = '<span class="badge">NEW</span>' if disp.startswith(DISPLAY_DATE) else ''
        index_html += f"<div class='card'><a href='reports/{name}'>ğŸ“„ {disp} ë¦¬í¬íŠ¸</a> {badge}</div>"
        
    index_html += "</body></html>"
    with open(os.path.join(DOCS_DIR, "index.html"), "w", encoding="utf-8") as f:
        f.write(index_html)

def main():
    driver = setup_driver()
    
    competitors = [
        {"name": "SKT ë‹¤ì´ë ‰íŠ¸", "url": "https://shop.tworld.co.kr/exhibition/submain", "param": None, "selector": "#wrap > div.container > div > div.event-list-wrap > div > ul"},
        {"name": "SKT Air", "url": "https://sktair-event.com/", "param": None, "selector": "#app > div > section.content"},
        {"name": "KTM ëª¨ë°”ì¼", "url": "https://www.ktmmobile.com/event/eventBoardList.do", "param": None, "selector": "#listArea1"},
        {"name": "U+ ìœ ëª¨ë°”ì¼", "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", "param": None, "selector": "#wrap > main > div > section"},
        {"name": "í—¬ë¡œëª¨ë°”ì¼", "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli&category=USIM", "param": "pageIndex", "selector": "#contentWrap > div.event-list-wrap > section > div.list-wrap > ul"},
        {"name": "ìŠ¤ì¹´ì´ë¼ì´í”„", "url": "https://www.skylife.co.kr/event?category=mobile", "param": "p", "selector": "body > div.pb-50 > div.m-auto > div > div > div.pt-14 > div > div.grid"}
    ]
    
    today_results = {}
    for comp in competitors:
        try:
            today_results[comp['name']] = crawl_site_logic(driver, comp['name'], comp['url'], comp['param'], comp['selector'])
        except Exception as e:
            print(f"âŒ {comp['name']} ì‹¤íŒ¨: {e}")
    
    driver.quit()
    
    # [ìˆ˜ì •] ê°€ì¥ ìµœì‹ (ì§ì „) ë°ì´í„° ë¡œë“œ
    yesterday_results = load_previous_data()
            
    report_body = ""
    total_change_count = 0
    company_summary = []
    
    for name, pages in today_results.items():
        site_changes = ""
        site_change_count = 0 
        old_pages = yesterday_results.get(name, {})
        
        all_urls = set(pages.keys()) | set(old_pages.keys())
        
        for url in all_urls:
            is_changed = False
            change_type = ""
            reason = ""
            
            curr = pages.get(url, {"title": "?", "img": "", "content": ""})
            prev = old_pages.get(url, {"title": "?", "img": "", "content": ""})
            
            if isinstance(prev, str): prev = {"title": "Old", "img": "", "content": prev}

            # 1. ì‹ ê·œ/ì‚­ì œ ì²´í¬
            if url in pages and url not in old_pages:
                is_changed = True
                change_type = "NEW"
                reason = "ì‹ ê·œ ì´ë²¤íŠ¸ ë“±ë¡"
            elif url not in pages and url in old_pages:
                is_changed = True
                change_type = "DELETED"
                reason = "ì´ë²¤íŠ¸ ì¢…ë£Œ/ì‚­ì œ"
            else:
                # 2. ë¦¬ìŠ¤íŠ¸(1ë‹¨ê³„) ë³€ê²½ ì²´í¬
                if curr['title'] != prev['title']:
                    is_changed = True
                    change_type = "UPDATED"
                    reason = f"ì œëª© ë³€ê²½: {prev['title']} -> {curr['title']}"
                elif curr['img'] != prev['img']:
                    is_changed = True
                    change_type = "UPDATED"
                    reason = "ì¸ë„¤ì¼/ë°°ë„ˆ ì´ë¯¸ì§€ ë³€ê²½"
                # 3. ìƒì„¸(2ë‹¨ê³„) ë³€ê²½ ì²´í¬
                elif curr['content'].replace(" ","") != prev['content'].replace(" ",""):
                    is_changed = True
                    change_type = "UPDATED"
                    reason = analyze_content_changes(prev['content'], curr['content'])

            if is_changed:
                color = "green" if change_type == "NEW" else "red" if change_type == "DELETED" else "orange"
                img_html = f"<img src='{curr['img']}' style='height:50px; vertical-align:middle; margin-right:10px;'>" if curr['img'] else ""
                
                site_changes += f"""
                <div style="border-left: 5px solid {color}; padding: 10px; margin-bottom: 10px; background: #fff; box-shadow: 0 1px 2px rgba(0,0,0,0.1);">
                    <h3 style="margin: 0 0 5px 0;">
                        <span style="color:{color}; font-weight:bold;">[{change_type}]</span> {curr['title']}
                    </h3>
                    <div style="display:flex; align-items:center;">
                        {img_html}
                        <div style="font-size: 0.9em; color: #555;">
                            <b>ë³€ê²½ ì‚¬ìœ :</b> {reason}<br>
                            <a href="{url}" target="_blank">ğŸ”— ë°”ë¡œê°€ê¸°</a>
                        </div>
                    </div>
                </div>
                """
                site_change_count += 1
        
        if site_changes:
            report_body += f"<h2>{name} ({site_change_count}ê±´)</h2>{site_changes}<hr>"
            total_change_count += site_change_count
            company_summary.append(f"{name}({site_change_count})")

    full_list_html = f"<h1>ğŸ“‚ {DISPLAY_DATE} ì „ì²´ ëª©ë¡ ({DISPLAY_TIME} KST)</h1><hr>"
    for name, pages in today_results.items():
        full_list_html += f"<h3>{name} ({len(pages)}ê°œ)</h3><div style='display:grid; grid-template-columns: repeat(auto-fill, minmax(200px, 1fr)); gap:10px;'>"
        for url, data in pages.items():
            img_tag = f"<img src='{data['img']}' style='width:100%; height:100px; object-fit:cover; border-radius:5px;'>" if data['img'] else ""
            full_list_html += f"<div style='border:1px solid #ddd; padding:10px; border-radius:8px;'><a href='{url}' target='_blank'>{img_tag}<p style='font-size:0.9em; margin-top:5px;'>{data['title']}</p></a></div>"
        full_list_html += "</div><hr>"
    
    list_filename = f"list_{FILE_TIMESTAMP}.html"
    with open(os.path.join(REPORT_DIR, list_filename), "w", encoding="utf-8") as f:
        f.write(full_list_html)

    summary_text = f"ì´ {total_change_count}ê±´ ì—…ë°ì´íŠ¸ ({', '.join(company_summary)})" if total_change_count > 0 else "íŠ¹ì´ì‚¬í•­ ì—†ìŒ"
    
    report_header = f"""
    <h1>ğŸ“… {DISPLAY_DATE} ë¦¬í¬íŠ¸ <span style="font-size:0.6em; color:#888;">({DISPLAY_TIME} KST)</span></h1>
    <div style='background-color:#f4f4f4; padding:15px; border-radius:10px; border:1px solid #ddd;'>
        <h3>ğŸ“Š {summary_text}</h3>
        <p>
            <a href="../index.html">ğŸ”™ ëŒ€ì‹œë³´ë“œ</a> | 
            <a href="{list_filename}" target="_blank">ğŸ—‚ï¸ ì „ì²´ ìˆ˜ì§‘ ëª©ë¡(ì´ë¯¸ì§€ í¬í•¨) ë³´ê¸°</a>
        </p>
    </div>
    <hr>
    """
    full_report = report_header + (report_body if total_change_count > 0 else "<p>âœ… ê¸ˆì¼ ë³€ë™ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.</p>")
    
    filename = f"report_{FILE_TIMESTAMP}.html"
    with open(os.path.join(REPORT_DIR, filename), "w", encoding="utf-8") as f:
        f.write(full_report)
    
    # [ìˆ˜ì •] ë°ì´í„° íŒŒì¼ë„ íƒ€ì„ìŠ¤íƒ¬í”„ ì°ì–´ì„œ ì €ì¥ (ëˆ„ì )
    data_filename = f"data_{FILE_TIMESTAMP}.json"
    with open(os.path.join(DATA_DIR, data_filename), "w", encoding="utf-8") as f:
        json.dump(today_results, f, ensure_ascii=False)

    update_index_page()

    dashboard_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/"
    report_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{filename}"
    list_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{list_filename}"
    
    if total_change_count > 0:
        payload = {
            "text": f"ğŸ“¢ *[KST {DISPLAY_TIME}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… *ìš”ì•½:* {summary_text}\n\nğŸ‘‰ *ë³€ê²½ ë¦¬í¬íŠ¸:* {report_url}\nğŸ—‚ï¸ *ì „ì²´ ëª©ë¡:* {list_url}\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ:* {dashboard_url}"
        }
    else:
        payload = {
            "text": f"ğŸ“‹ *[KST {DISPLAY_TIME}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… íŠ¹ì´ì‚¬í•­ ì—†ìŒ\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ:* {dashboard_url}"
        }
        
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json=payload)
        print("âœ… ìŠ¬ë™ ì•Œë¦¼ ì™„ë£Œ")

if __name__ == "__main__":
    main()
