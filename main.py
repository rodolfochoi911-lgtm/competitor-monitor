"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì„¤ëª…] 
ì£¼ìš” ê²½ìŸì‚¬(í†µì‹ ì‚¬/ì•Œëœ°í°)ì˜ ì´ë²¤íŠ¸ ê²Œì‹œíŒì„ ì •ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§í•˜ì—¬,
ì‹ ê·œ ì´ë²¤íŠ¸ ëŸ°ì¹­, ì¢…ë£Œ, ë‚´ìš© ë³€ê²½(HTML Diff)ì„ ê°ì§€í•˜ê³  Slackìœ¼ë¡œ ë¦¬í¬íŒ…í•©ë‹ˆë‹¤.
GitHub Actions í™˜ê²½ì—ì„œ êµ¬ë™ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import os
import json
import time
import random
import difflib
import requests
from datetime import datetime
from bs4 import BeautifulSoup

# Selenium ë° Webdriver Manager ë¼ì´ë¸ŒëŸ¬ë¦¬ (ë¸Œë¼ìš°ì € ìë™í™”)
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜ ë° ë¦¬í¬ì§€í† ë¦¬ ì •ë³´ êµ¬ì„±
# =========================================================
# ë¦¬í¬íŠ¸ ë§í¬ ìƒì„±ì„ ìœ„í•œ GitHub ê³„ì • ì •ë³´
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 

# Slack Webhook URL (GitHub Secrets í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œí•˜ì—¬ ë³´ì•ˆ ìœ ì§€)
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

# ë°ì´í„° ë° ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
DATA_DIR = "data"
REPORT_DIR = "docs/reports"
TODAY_STR = datetime.now().strftime("%Y-%m-%d")

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ì„ ê²½ìš° ìë™ ìƒì„±
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    """
    [ê¸°ëŠ¥] Selenium Chrome Driver ì´ˆê¸°í™” ë° ì„¤ì •
    [ì„¤ëª…] GitHub Actions(Linux/Server) í™˜ê²½ì— ë§ì¶° Headless ëª¨ë“œ ë° í•„ìˆ˜ ì˜µì…˜ì„ ì ìš©í•©ë‹ˆë‹¤.
    """
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # GUI ì—†ì´ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    chrome_options.add_argument("--no-sandbox") # ë¦¬ëˆ…ìŠ¤ í™˜ê²½ ìƒŒë“œë°•ìŠ¤ ë¹„í™œì„±í™” (ê¶Œí•œ ë¬¸ì œ ë°©ì§€)
    chrome_options.add_argument("--disable-dev-shm-usage") # ë©”ëª¨ë¦¬ ê³µìœ  ë¬¸ì œ ë°©ì§€
    # ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•œ User-Agent ì„¤ì • (ì¼ë°˜ ìœˆë„ìš° í™˜ê²½ìœ¼ë¡œ ìœ„ì¥)
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    # Chrome Driver ìë™ ì„¤ì¹˜ ë° ì„œë¹„ìŠ¤ ì‹¤í–‰ (ë²„ì „ ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°©ì§€)
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def remove_popups(driver):
    """
    [ê¸°ëŠ¥] ì›¹í˜ì´ì§€ ë‚´ ë¶ˆí•„ìš”í•œ ë ˆì´ì–´ ì œê±°
    [ì„¤ëª…] í¬ë¡¤ë§ ì‹œ í´ë¦­ì´ë‚˜ ë°ì´í„° ìˆ˜ì§‘ì„ ë°©í•´í•˜ëŠ” íŒì—…, ëª¨ë‹¬, ë°°ë„ˆ ë“±ì„ JavaScriptë¡œ ê°•ì œ ì œê±°í•©ë‹ˆë‹¤.
    """
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup');
            popups.forEach(function(element) { element.remove(); });
        """)
    except:
        pass # íŒì—…ì´ ì—†ëŠ” ê²½ìš° ì˜ˆì™¸ ë¬´ì‹œ

def clean_html(html_source):
    """
    [ê¸°ëŠ¥] HTML ë°ì´í„° ì „ì²˜ë¦¬ (Data Cleaning)
    [ì„¤ëª…] ë¹„êµ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë³¸ë¬¸ê³¼ ë¬´ê´€í•œ íƒœê·¸(Script, Style, Nav ë“±)ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    soup = BeautifulSoup(html_source, 'html.parser')
    
    # ë¶„ì„ì— ë¶ˆí•„ìš”í•œ íƒœê·¸ ëª©ë¡ ì œê±°
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside']):
        tag.decompose()
        
    body = soup.find('body')
    # ì „ì²˜ë¦¬ëœ HTMLì„ ë¬¸ìì—´ë¡œ ë°˜í™˜ (Bodyê°€ ì—†ì„ ê²½ìš° ëŒ€ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜)
    return body.prettify() if body else "No Content"

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    """
    [ê¸°ëŠ¥] ê°œë³„ ê²½ìŸì‚¬ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë¡œì§ ìˆ˜í–‰
    [íŒŒë¼ë¯¸í„°]
      - driver: Selenium Driver ê°ì²´
      - site_name: ê²½ìŸì‚¬ëª…
      - base_url: ëŒ€ìƒ URL
      - pagination_param: í˜ì´ì§€ë„¤ì´ì…˜ íŒŒë¼ë¯¸í„°ëª… (Noneì¼ ê²½ìš° ë‹¨ì¼ í˜ì´ì§€)
      - target_selector: ê°ì‹œ ëŒ€ìƒ CSS Selector (Noneì¼ ê²½ìš° ì „ì²´ í˜ì´ì§€)
    """
    print(f"ğŸš€ [{site_name}] ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    collected_links = []
    last_page_links = []
    page = 1
    
    # --- [Step 1] ì´ë²¤íŠ¸ ëª©ë¡ ìˆ˜ì§‘ (í˜ì´ì§€ ìˆœíšŒ) ---
    while True:
        # URL íŒŒë¼ë¯¸í„° êµ¬ì„± (í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬)
        if pagination_param:
            connector = '&' if '?' in base_url else '?'
            target_url = f"{base_url}{connector}{pagination_param}={page}"
        else:
            target_url = base_url
            
        try:
            driver.get(target_url)
            time.sleep(3) # ë™ì  ë Œë”ë§ ëŒ€ê¸°
            remove_popups(driver)
            
            # íŠ¹ì • ì˜ì—­(Selector)ì´ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ì˜ì—­ ë¡œë”© ëŒ€ê¸° ë° íƒìƒ‰
            if target_selector:
                try:
                    # ì§€ì •ëœ ìš”ì†Œê°€ DOMì— ë¡œë“œë  ë•Œê¹Œì§€ ìµœëŒ€ 5ì´ˆê°„ ëŒ€ê¸°
                    container = WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, target_selector))
                    )
                    links = container.find_elements(By.TAG_NAME, "a")
                    print(f"  ğŸ¯ [íƒ€ê²Ÿ ê°ì§€] ì§€ì •ëœ ì˜ì—­({target_selector}) ë‚´ë¶€ ìŠ¤ìº” ìˆ˜í–‰")
                except Exception as e:
                    print(f"  âš ï¸ íƒ€ê²Ÿ ì˜ì—­ íƒìƒ‰ ì‹¤íŒ¨. ì „ì²´ ë²”ìœ„ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤. ({e})")
                    links = driver.find_elements(By.TAG_NAME, "a")
            else:
                # Selector ë¯¸ì§€ì • ì‹œ ì „ì²´ ë¬¸ì„œì—ì„œ íƒìƒ‰
                links = driver.find_elements(By.TAG_NAME, "a")

            current_page_links = []
            
            # ìœ íš¨ ë§í¬ í•„í„°ë§ ë¡œì§
            for link in links:
                try:
                    href = link.get_attribute('href')
                    # ì´ë²¤íŠ¸, ê³µì§€ì‚¬í•­ ë“± ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë§í¬ë§Œ ìˆ˜ì§‘
                    if href and ('event' in href or 'view' in href or 'detail' in href or 'notice' in href) and not href.startswith('#') and 'javascript' not in href:
                        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if href.startswith('/'):
                            from urllib.parse import urljoin
                            href = urljoin(base_url, href)
                        
                        if href not in current_page_links:
                            current_page_links.append(href)
                except:
                    continue
            
            print(f"  - Page {page}: {len(current_page_links)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")

            # [íƒˆì¶œ ì¡°ê±´ 1] ê²Œì‹œê¸€ì´ ì—†ëŠ” ê²½ìš° ì¢…ë£Œ
            if not current_page_links: break
            
            # [íƒˆì¶œ ì¡°ê±´ 2] ì´ì „ í˜ì´ì§€ì™€ ê²°ê³¼ê°€ ë™ì¼í•œ ê²½ìš° (ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬) ì¢…ë£Œ
            if sorted(current_page_links) == sorted(last_page_links): break

            for lnk in current_page_links:
                if lnk not in collected_links:
                    collected_links.append(lnk)

            if not pagination_param: break # ë‹¨ì¼ í˜ì´ì§€ ì‚¬ì´íŠ¸ëŠ” 1íšŒ ìˆ˜í–‰ í›„ ì¢…ë£Œ
            
            last_page_links = current_page_links
            page += 1
            if page > 10: break # ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „ ì¥ì¹˜ (ìµœëŒ€ 10í˜ì´ì§€)

        except Exception as e:
            print(f"  âš ï¸ í˜ì´ì§€ ìˆœíšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            break

    # --- [Step 2] ìƒì„¸ í˜ì´ì§€ ë”¥ë‹¤ì´ë¸Œ (ë‚´ìš© ìˆ˜ì§‘) ---
    # [ìˆ˜ì •ë¨] ê¸°ì¡´ 5ê°œ ì œí•œ([:5])ì„ ì œê±°í•˜ì—¬ ì „ìˆ˜ ì¡°ì‚¬ ìˆ˜í–‰
    print(f"  ğŸ” ìƒì„¸ í˜ì´ì§€ ì •ë°€ ë¶„ì„ ì‹œì‘ (ì´ {len(collected_links)}ê±´)...")
    site_data = {}
    
    for link in collected_links:
        try:
            driver.get(link)
            time.sleep(1) # ì•ˆì •ì ì¸ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ëŒ€ê¸°
            remove_popups(driver)
            site_data[link] = clean_html(driver.page_source)
        except Exception as e:
            print(f"  âŒ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {link}")
            pass
            
    return site_data

def main():
    driver = setup_driver()
    
    # [ì„¤ì •] ê²½ìŸì‚¬ë³„ ëª¨ë‹ˆí„°ë§ íƒ€ê²Ÿ URL ë° CSS Selector ì •ì˜
    competitors = [
        {
            "name": "SKT ë‹¤ì´ë ‰íŠ¸", 
            "url": "https://shop.tworld.co.kr/exhibition/submain", 
            "param": None, 
            "selector": "#wrap > div.container > div > div.event-list-wrap > div > ul"
        },
        {
            "name": "KTM ëª¨ë°”ì¼", 
            "url": "https://www.ktmmobile.com/event/eventBoardList.do", 
            "param": None, 
            "selector": "#listArea1"
        },
        {
            "name": "U+ ìœ ëª¨ë°”ì¼", 
            "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", 
            "param": None, 
            "selector": "#wrap > main > div > section"
        },
        {
            "name": "í—¬ë¡œëª¨ë°”ì¼", 
            "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli&category=USIM", 
            "param": "pageIndex", 
            "selector": "#contentWrap > div.event-list-wrap > section > div.list-wrap > ul"
        },
        {
            "name": "ìŠ¤ì¹´ì´ë¼ì´í”„", 
            "url": "https://www.skylife.co.kr/event?category=mobile", 
            "param": "page", 
            "selector": "body > div.pb-50.min-w-\[1248px\] > div.m-auto.max-w-\[1248px\].pt-20 > div > div > div.pt-14 > div > div.grid.grid-cols-3.gap-6.pt-4"
        }
    ]
    
    today_results = {}
    
    # ê²½ìŸì‚¬ ìˆœì°¨ í¬ë¡¤ë§ ìˆ˜í–‰
    for comp in competitors:
        try:
            today_results[comp['name']] = crawl_site_logic(driver, comp['name'], comp['url'], comp['param'], comp['selector'])
        except Exception as e:
            print(f"âŒ [{comp['name']}] í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨: {e}")
    
    driver.quit() # ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ í•´ì œ
    
    # --- [Step 3] ë°ì´í„° ë¹„êµ ë° ë¦¬í¬íŠ¸ ìƒì„± ---
    latest_file = os.path.join(DATA_DIR, "latest_data.json")
    yesterday_results = {}
    
    # ì´ì „ ë°ì´í„° ë¡œë“œ
    if os.path.exists(latest_file):
        with open(latest_file, "r", encoding="utf-8") as f:
            yesterday_results = json.load(f)
            
    report_body = ""
    total_change_count = 0
    company_summary = [] # ìš”ì•½ ì •ë³´ ì €ì¥ìš© (ì˜ˆ: SKT(2ê±´))
    
    # ì‚¬ì´íŠ¸ë³„ ë°ì´í„° ë¹„êµ ìˆ˜í–‰
    for name, pages in today_results.items():
        site_changes = ""
        site_change_count = 0 
        
        old_pages = yesterday_results.get(name, {})
        all_urls = set(pages.keys()) | set(old_pages.keys())
        
        for url in all_urls:
            is_changed = False
            change_html = ""
            
            # Case 1: ì‹ ê·œ ì´ë²¤íŠ¸ ê°ì§€
            if url in pages and url not in old_pages:
                is_changed = True
                change_html = f"<h3 style='color:green'>[NEW] <a href='{url}' target='_blank'>ìƒˆ ì´ë²¤íŠ¸ ëŸ°ì¹­</a></h3><br>"
            
            # Case 2: ì´ë²¤íŠ¸ ì¢…ë£Œ ê°ì§€
            elif url not in pages and url in old_pages:
                is_changed = True
                change_html = f"<h3 style='color:red'>[DELETED] <a href='{url}' target='_blank'>ì´ë²¤íŠ¸ ì¢…ë£Œ</a></h3><br>"
            
            # Case 3: ë‚´ìš© ë³€ê²½ ê°ì§€ (HTML Diff)
            elif pages[url].replace(" ","") != old_pages[url].replace(" ",""):
                is_changed = True
                diff = difflib.HtmlDiff().make_table(old_pages[url].splitlines(), pages[url].splitlines(), context=True, numlines=3)
                change_html = f"<h3 style='color:orange'>[UPDATED] <a href='{url}' target='_blank'>ìƒì„¸ ë‚´ìš© ë³€ê²½</a></h3>{diff}<br>"
            
            if is_changed:
                site_changes += change_html
                site_change_count += 1
        
        if site_changes:
            report_body += f"<h2>{name} (ë³€ê²½ {site_change_count}ê±´)</h2>{site_changes}<hr>"
            total_change_count += site_change_count
            company_summary.append(f"{name}({site_change_count}ê±´)")

    # --- [Step 4] ê²°ê³¼ ì €ì¥ ë° ì•Œë¦¼ ë°œì†¡ ---
    
    # [ìˆ˜ì •ë¨] ë³€ë™ ì‚¬í•­ ìœ ë¬´ì™€ ê´€ê³„ì—†ì´ ë¡œì§ ë¶„ê¸° ì²˜ë¦¬
    if total_change_count > 0:
        # 1) ë³€ë™ ì‚¬í•­ì´ ìˆì„ ê²½ìš°: ë¦¬í¬íŠ¸ ìƒì„± ë° ìƒì„¸ ì•Œë¦¼ ë°œì†¡
        summary_text = f"ì´ {total_change_count}ê±´ ì—…ë°ì´íŠ¸ ({', '.join(company_summary)})"
        
        report_header = f"""
        <h1>ğŸ“… {TODAY_STR} ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸</h1>
        <div style='background-color:#f4f4f4; padding:15px; border-radius:10px; border:1px solid #ddd;'>
            <h3>ğŸ“Š Executive Summary: {summary_text}</h3>
        </div>
        <hr>
        """
        full_report = report_header + report_body
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        filename = f"report_{TODAY_STR}.html"
        with open(os.path.join(REPORT_DIR, filename), "w", encoding="utf-8") as f:
            f.write(full_report)
            
        # ìµœì‹  ë°ì´í„° ê°±ì‹  (DB ì—…ë°ì´íŠ¸)
        with open(latest_file, "w", encoding="utf-8") as f:
            json.dump(today_results, f, ensure_ascii=False)
            
        report_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{filename}"
        
        # Slack ì•Œë¦¼ (ë³€ë™ ë°œìƒ ì‹œ)
        payload = {
            "text": f"ğŸ“¢ *[{TODAY_STR}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… *ìš”ì•½:* {summary_text}\nğŸ‘‰ *ìƒì„¸ ë¦¬í¬íŠ¸ í™•ì¸:* {report_url}"
        }
        
        if SLACK_WEBHOOK_URL:
            requests.post(SLACK_WEBHOOK_URL, json=payload)
            print("âœ… [ì•Œë¦¼] ë³€ë™ ì‚¬í•­ ìŠ¬ë™ ì „ì†¡ ì™„ë£Œ")
        else:
            print("âš ï¸ [ê²½ê³ ] Slack Webhook URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
    else:
        # 2) ë³€ë™ ì‚¬í•­ì´ ì—†ì„ ê²½ìš°: "íŠ¹ì´ì‚¬í•­ ì—†ìŒ" ì•Œë¦¼ ë°œì†¡ (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
        print("âœ… ê¸ˆì¼ ë³€ë™ ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„°ëŠ” ê°±ì‹  (ì˜¤ëŠ˜ ë‚ ì§œì˜ ë°ì´í„°ê°€ ìµœì‹  ê¸°ì¤€ì´ ë˜ë„ë¡)
        with open(latest_file, "w", encoding="utf-8") as f:
            json.dump(today_results, f, ensure_ascii=False)

        # Slack ì•Œë¦¼ (ë³€ë™ ì—†ì„ ì‹œ)
        payload = {
            "text": f"ğŸ“‹ *[{TODAY_STR}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… ê¸ˆì¼ ê°ì§€ëœ ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ë³€ë™ ì‚¬í•­/íŠ¹ì´ ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤."
        }
        
        if SLACK_WEBHOOK_URL:
            requests.post(SLACK_WEBHOOK_URL, json=payload)
            print("âœ… [ì•Œë¦¼] 'ë³€ë™ ì—†ìŒ' ë©”ì‹œì§€ ìŠ¬ë™ ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    main()
