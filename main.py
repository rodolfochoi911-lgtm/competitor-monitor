"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-01-29 (ì´ˆ ë‹¨ìœ„ ì•„ì¹´ì´ë¹™ + KST + SKT Air ì „ì²´ê²€ìƒ‰)
"""

import os
import json
import time
import glob
from datetime import datetime, timedelta, timezone
import difflib
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •]
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"

# [ì¤‘ìš”] í•œêµ­ ì‹œê°„(KST) ì„¤ì •
KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)

# íŒŒì¼ëª…ì— ì“¸ íƒ€ì„ìŠ¤íƒ¬í”„ (ì˜ˆ: 20260129_143005) -> ë®ì–´ì“°ê¸° ë°©ì§€
FILE_TIMESTAMP = NOW.strftime("%Y%m%d_%H%M%S")

# í™”ë©´ í‘œì‹œìš© ë¬¸ìì—´ (ì˜ˆ: 2026-01-29)
DISPLAY_DATE = NOW.strftime("%Y-%m-%d")
DISPLAY_TIME = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup');
            popups.forEach(function(element) { element.remove(); });
        """)
    except:
        pass

def scroll_to_bottom(driver):
    """ìŠ¤í¬ë¡¤ì„ 10ë²ˆ ë‚´ë ¤ì„œ ëª¨ë“  ë°ì´í„° ë¡œë”© ìœ ë„"""
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(10): 
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
    except:
        pass

def clean_html(html_source):
    soup = BeautifulSoup(html_source, 'html.parser')
    
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside']):
        tag.decompose()

    for hidden in soup.find_all(attrs={"style": True}):
        if "display:none" in hidden["style"].replace(" ", "").lower():
            hidden.decompose()
            
    trash_ids = ['across_adn_container', 'criteo-tags-div', 'kakao-pixel-id', 'facebook-pixel-id']
    for t_id in trash_ids:
        tag = soup.find(id=t_id)
        if tag: tag.decompose()
        
    body = soup.find('body')
    return body.prettify() if body else "No Content"

def analyze_changes(old_html, new_html):
    soup_old = BeautifulSoup(old_html, 'html.parser')
    soup_new = BeautifulSoup(new_html, 'html.parser')
    
    summary_tags = []
    
    imgs_old = set([img.get('src') for img in soup_old.find_all('img') if img.get('src')])
    imgs_new = set([img.get('src') for img in soup_new.find_all('img') if img.get('src')])
    if imgs_old != imgs_new:
        summary_tags.append("ğŸ–¼ï¸ <b>ì´ë¯¸ì§€/ë°°ë„ˆ ë³€ê²½</b>")
        
    if soup_old.get_text().strip() != soup_new.get_text().strip():
        summary_tags.append("âœï¸ <b>í…ìŠ¤íŠ¸(ë‚´ìš©) ìˆ˜ì •</b>")
        
    links_old = set([a.get('href') for a in soup_old.find_all('a') if a.get('href')])
    links_new = set([a.get('href') for a in soup_new.find_all('a') if a.get('href')])
    if links_old != links_new:
        summary_tags.append("ğŸ”— <b>ì—°ê²° ë§í¬ ë³€ê²½</b>")

    if not summary_tags and old_html != new_html:
        summary_tags.append("ğŸ¨ <b>ë””ìì¸/ìŠ¤íƒ€ì¼ ë³€ê²½</b>")
        
    if not summary_tags:
        return "ğŸ” ë¯¸ì„¸í•œ ì½”ë“œ ë³€ê²½"
    
    return " / ".join(summary_tags)

def extract_links_safely(driver, base_url, target_selector):
    links = []
    method = "Selector"
    
    if target_selector:
        try:
            container = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, target_selector))
            )
            found_links = container.find_elements(By.TAG_NAME, "a")
            if len(found_links) == 0:
                print(f"    âš ï¸ ì„ íƒì({target_selector}) ì‹¤íŒ¨/0ê±´ -> ì „ì²´ ê²€ìƒ‰ ì „í™˜")
                links = driver.find_elements(By.TAG_NAME, "a")
                method = "Fallback (All)"
            else:
                links = found_links
        except:
            print(f"    âš ï¸ ì„ íƒì ì—ëŸ¬ -> ì „ì²´ ê²€ìƒ‰ ì „í™˜")
            links = driver.find_elements(By.TAG_NAME, "a")
            method = "Fallback (All)"
    else:
        links = driver.find_elements(By.TAG_NAME, "a")
        method = "Full Page"

    current_page_urls = []
    extracted_data = {} 

    for link in links:
        try:
            href = link.get_attribute('href')
            title = link.text.strip()
            if not title:
                img = link.find_element(By.TAG_NAME, "img")
                title = img.get_attribute("alt") if img else "ì œëª© ì—†ìŒ"
            
            if href and ('event' in href or 'view' in href or 'detail' in href or 'notice' in href) and not href.startswith('#') and 'javascript' not in href:
                if href.startswith('/'):
                    from urllib.parse import urljoin
                    href = urljoin(base_url, href)
                
                if href not in current_page_urls:
                    current_page_urls.append(href)
                    if href not in extracted_data:
                        extracted_data[href] = title
        except:
            continue
            
    return extracted_data, method

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    collected_links = {} 
    last_page_links = []
    page = 1
    
    while True:
        if pagination_param:
            connector = '&' if '?' in base_url else '?'
            target_url = f"{base_url}{connector}{pagination_param}={page}"
        else:
            target_url = base_url
            
        try:
            driver.get(target_url)
            time.sleep(5) 
            remove_popups(driver)
            scroll_to_bottom(driver)
            
            page_data, method = extract_links_safely(driver, base_url, target_selector)
            
            if page == 1:
                print(f"  - [{method}] Page {page}: {len(page_data)}ê°œ ë°œê²¬")
            
            if not page_data: break
            
            current_urls = sorted(list(page_data.keys()))
            if current_urls == sorted(last_page_links): 
                break
            
            collected_links.update(page_data)
            
            if not pagination_param: break
            
            last_page_links = current_urls
            page += 1
            if page > 10: break

        except Exception as e:
            print(f"  âš ï¸ ì˜¤ë¥˜: {e}")
            break

    print(f"  ğŸ” ìƒì„¸ ë¶„ì„ ì¤‘ ({len(collected_links)}ê±´)...")
    site_data = {}
    
    for link, title in collected_links.items():
        try:
            driver.get(link)
            time.sleep(1)
            remove_popups(driver)
            site_data[link] = {
                "title": title if title else "ì œëª© ì—†ìŒ",
                "content": clean_html(driver.page_source)
            }
        except:
            pass
            
    return site_data

def update_index_page():
    # íŒŒì¼ëª…ì— ì‹œê°„ì´ ë“¤ì–´ê°€ë¯€ë¡œ, ì´ë¦„ ì—­ìˆœìœ¼ë¡œ ì •ë ¬í•˜ë©´ ìµœì‹  íŒŒì¼ì´ ë§¨ ìœ„ë¡œ ì˜´
    report_files = glob.glob(os.path.join(REPORT_DIR, "report_*.html"))
    report_files.sort(reverse=True)
    
    index_html = f"""
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ</title>
        <style>
            body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background-color: #f9f9f9; }}
            h1 {{ color: #333; border-bottom: 2px solid #0056b3; padding-bottom: 10px; }}
            .card {{ background: white; padding: 15px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); display: flex; justify-content: space-between; align-items: center; }}
            .card a {{ text-decoration: none; color: #0056b3; font-weight: bold; font-size: 1.1em; }}
            .card a:hover {{ text-decoration: underline; }}
            .date {{ color: #666; font-size: 0.9em; }}
            .badge {{ background-color: #28a745; color: white; padding: 3px 8px; border-radius: 12px; font-size: 0.8em; }}
            .sub-link {{ font-size: 0.8em; color: #888; margin-left: 10px; }}
        </style>
    </head>
    <body>
        <h1>ğŸ“Š ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ (ì•„ì¹´ì´ë¸Œ)</h1>
        <p>í˜„ì¬ ì‹œê°: {DISPLAY_DATE} {DISPLAY_TIME} (KST)</p>
        <div class="list-container">
    """
    
    if not report_files:
        index_html += "<p>ì•„ì§ ìƒì„±ëœ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
    
    for file_path in report_files:
        filename = os.path.basename(file_path)
        # íŒŒì¼ëª…: report_20260129_143000.html
        # í‘œì‹œìš©: 2026-01-29 14:30:00
        timestamp_part = filename.replace("report_", "").replace(".html", "")
        try:
            dt_obj = datetime.strptime(timestamp_part, "%Y%m%d_%H%M%S")
            display_str = dt_obj.strftime("%Y-%m-%d %H:%M:%S")
        except:
            display_str = timestamp_part # í¬ë§· ì•ˆ ë§ìœ¼ë©´ ê·¸ëƒ¥ ì¶œë ¥
        
        list_filename = f"list_{timestamp_part}.html"
        
        # ì˜¤ëŠ˜ ë‚ ì§œì™€ ê°™ìœ¼ë©´ NEW ë±ƒì§€
        is_today = display_str.startswith(DISPLAY_DATE)
        badge = '<span class="badge">NEW</span>' if is_today else ''
        
        index_html += f"""
            <div class="card">
                <div>
                    <a href="reports/{filename}">ğŸ“„ {display_str} ë¦¬í¬íŠ¸</a>
                    <a href="reports/{list_filename}" class="sub-link" target="_blank">ğŸ—‚ï¸ ì „ì²´ ìˆ˜ì§‘ ëª©ë¡</a>
                    {badge}
                </div>
            </div>
        """
        
    index_html += "</div></body></html>"
    
    with open(os.path.join(DOCS_DIR, "index.html"), "w", encoding="utf-8") as f:
        f.write(index_html)
    print("âœ… [ëŒ€ì‹œë³´ë“œ] index.html ì—…ë°ì´íŠ¸ ì™„ë£Œ")

def main():
    driver = setup_driver()
    
    competitors = [
        {"name": "SKT ë‹¤ì´ë ‰íŠ¸", "url": "https://shop.tworld.co.kr/exhibition/submain", "param": None, "selector": None},
        {"name": "SKT Air", "url": "https://sktair-event.com/", "param": None, "selector": None},
        {"name": "KTM ëª¨ë°”ì¼", "url": "https://www.ktmmobile.com/event/eventBoardList.do", "param": None, "selector": None},
        {"name": "U+ ìœ ëª¨ë°”ì¼", "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", "param": None, "selector": "#wrap > main > div > section"},
        {"name": "í—¬ë¡œëª¨ë°”ì¼", "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli&category=USIM", "param": "pageIndex", "selector": None},
        {"name": "ìŠ¤ì¹´ì´ë¼ì´í”„", "url": "https://www.skylife.co.kr/event?category=mobile", "param": "p", "selector": None}
    ]
    
    today_results = {}
    for comp in competitors:
        try:
            today_results[comp['name']] = crawl_site_logic(driver, comp['name'], comp['url'], comp['param'], comp['selector'])
        except Exception as e:
            print(f"âŒ {comp['name']} ì‹¤íŒ¨: {e}")
    
    driver.quit()
    
    latest_file = os.path.join(DATA_DIR, "latest_data.json")
    yesterday_results = {}
    if os.path.exists(latest_file):
        with open(latest_file, "r", encoding="utf-8") as f:
            yesterday_results = json.load(f)
            
    report_body = ""
    total_change_count = 0
    company_summary = []
    
    for name, pages in today_results.items():
        site_changes = ""
        site_change_count = 0 
        old_pages = yesterday_results.get(name, {})
        
        all_urls = set(pages.keys()) | set(old_pages.keys())
        
        for url in all_urls:
            is_changed = False
            change_html = ""
            
            curr_data = pages.get(url, {"title": "Unknown", "content": ""})
            prev_data = old_pages.get(url, {"title": "Unknown", "content": ""})
            
            if isinstance(prev_data, str): prev_data = {"title": "Old Data", "content": prev_data}
            if isinstance(curr_data, str): curr_data = {"title": "New Data", "content": curr_data}

            title_display = curr_data['title'] if url in pages else prev_data['title']

            if url in pages and url not in old_pages:
                is_changed = True
                change_html = f"<h3 style='color:green'>[NEW] {title_display} <a href='{url}' target='_blank' style='font-size:0.7em'>ğŸ”—ë§í¬</a></h3>"
            
            elif url not in pages and url in old_pages:
                is_changed = True
                change_html = f"<h3 style='color:red'>[DELETED] {title_display} <a href='{url}' target='_blank' style='font-size:0.7em'>ğŸ”—ë§í¬</a></h3>"
            
            elif curr_data['content'].replace(" ","") != prev_data['content'].replace(" ",""):
                is_changed = True
                change_summary = analyze_changes(prev_data['content'], curr_data['content'])
                diff = difflib.HtmlDiff().make_table(prev_data['content'].splitlines(), curr_data['content'].splitlines(), context=True, numlines=3)
                
                change_html = f"""
                <div style="border:1px solid #ddd; padding:10px; border-radius:8px; margin-bottom:10px; background-color:#fff;">
                    <h3 style='margin:0 0 5px 0; color:orange;'>[UPDATED] {title_display}</h3>
                    <div style="font-size:0.9em; color:#666; margin-bottom:10px;">
                        {change_summary} <a href='{url}' target='_blank' style='text-decoration:none;'>ğŸ”— ë°”ë¡œê°€ê¸°</a>
                    </div>
                    <details>
                        <summary style="cursor:pointer; color:#0056b3; font-weight:bold; padding:8px; background:#f8f9fa; border-radius:5px;">ğŸ‘‰ ë³€ê²½ëœ ì½”ë“œ ìƒì„¸ ë³´ê¸° (í´ë¦­)</summary>
                        <div style="margin-top:10px; overflow-x:auto; font-size:0.85em;">{diff}</div>
                    </details>
                </div>
                """
            
            if is_changed:
                site_changes += change_html
                site_change_count += 1
        
        if site_changes:
            report_body += f"<h2>{name} ({site_change_count}ê±´)</h2>{site_changes}<hr>"
            total_change_count += site_change_count
            company_summary.append(f"{name}({site_change_count})")

    # [ìˆ˜ì •] íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì ìš© (ë®ì–´ì“°ê¸° ë°©ì§€)
    full_list_html = f"<h1>ğŸ“‚ {DISPLAY_DATE} ì „ì²´ ìˆ˜ì§‘ ëª©ë¡ ({DISPLAY_TIME} KST)</h1><hr>"
    for name, pages in today_results.items():
        full_list_html += f"<h3>{name} (ì´ {len(pages)}ê°œ)</h3><ul>"
        for url, data in pages.items():
            full_list_html += f"<li><a href='{url}' target='_blank'>{data['title']}</a></li>"
        full_list_html += "</ul><hr>"
    
    list_filename = f"list_{FILE_TIMESTAMP}.html"
    with open(os.path.join(REPORT_DIR, list_filename), "w", encoding="utf-8") as f:
        f.write(full_list_html)

    summary_text = f"ì´ {total_change_count}ê±´ ì—…ë°ì´íŠ¸ ({', '.join(company_summary)})" if total_change_count > 0 else "íŠ¹ì´ì‚¬í•­ ì—†ìŒ"
    
    report_header = f"""
    <h1>ğŸ“… {DISPLAY_DATE} ë¦¬í¬íŠ¸ <span style="font-size:0.6em; color:#888;">({DISPLAY_TIME} KST)</span></h1>
    <div style='background-color:#f4f4f4; padding:15px; border-radius:10px; border:1px solid #ddd;'>
        <h3>ğŸ“Š {summary_text}</h3>
        <p>
            <a href="../index.html">ğŸ”™ ëŒ€ì‹œë³´ë“œ</a> | 
            <a href="{list_filename}" target="_blank">ğŸ—‚ï¸ ì „ì²´ ìˆ˜ì§‘ ëª©ë¡ ë³´ê¸°</a>
        </p>
    </div>
    <hr>
    """
    full_report = report_header + (report_body if total_change_count > 0 else "<p>âœ… ê¸ˆì¼ ë³€ë™ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.</p>")
    
    # [ìˆ˜ì •] íŒŒì¼ëª… íƒ€ì„ìŠ¤íƒ¬í”„ ì ìš©
    filename = f"report_{FILE_TIMESTAMP}.html"
    with open(os.path.join(REPORT_DIR, filename), "w", encoding="utf-8") as f:
        f.write(full_report)
        
    with open(latest_file, "w", encoding="utf-8") as f:
        json.dump(today_results, f, ensure_ascii=False)

    update_index_page()

    dashboard_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/"
    report_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{filename}"
    list_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{list_filename}"
    
    # [ìˆ˜ì •] ìŠ¬ë™ ì•Œë¦¼ì— KST ì‹œê°„ ê°•ì¡°
    if total_change_count > 0:
        payload = {
            "text": f"ğŸ“¢ *[KST {DISPLAY_TIME}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… *ìš”ì•½:* {summary_text}\n\nğŸ‘‰ *ë³€ê²½ ë¦¬í¬íŠ¸:* {report_url}\nğŸ—‚ï¸ *ì „ì²´ ëª©ë¡:* {list_url}\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ (ì•„ì¹´ì´ë¸Œ):* {dashboard_url}"
        }
    else:
        payload = {
            "text": f"ğŸ“‹ *[KST {DISPLAY_TIME}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… íŠ¹ì´ì‚¬í•­ ì—†ìŒ\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ (ì•„ì¹´ì´ë¸Œ):* {dashboard_url}"
        }
        
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json=payload)
        print("âœ… ìŠ¬ë™ ì•Œë¦¼ ì™„ë£Œ")
    else:
        print("âš ï¸ ìŠ¬ë™ URL ì—†ìŒ")

if __name__ == "__main__":
    main()
