"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ (V25)
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-01-30 (ìœ ëª¨ë°”ì¼ ë“± 4ëŒ€ì¥ ë¡œì§ ì›ìƒë³µêµ¬ / ë¬¸ì œì•„ 3ì¸ë°© ë³„ë„ ê²©ë¦¬)
"""

import os
import json
import time
import glob
import re
import traceback
from datetime import datetime, timedelta, timezone
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"

KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
FILE_TIMESTAMP = NOW.strftime("%Y%m%d_%H%M%S")
DISPLAY_DATE = NOW.strftime("%Y-%m-%d")
DISPLAY_TIME = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    print("ğŸš— ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup, .close');
            popups.forEach(function(element) { element.remove(); });
        """)
    except: pass

def scroll_to_bottom(driver):
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(3): 
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height: break
            last_height = new_height
    except: pass

def clean_html(html_source):
    if not html_source: return ""
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside']):
        tag.decompose()
    for hidden in soup.find_all(attrs={"style": True}):
        if "display:none" in hidden["style"].replace(" ", "").lower():
            hidden.decompose()
    return body.prettify() if (body := soup.find('body')) else "No Content"

def load_previous_data():
    json_files = glob.glob(os.path.join(DATA_DIR, "data_*.json"))
    if not json_files: return {}
    json_files.sort()
    latest_file = json_files[-1]
    print(f"ğŸ“‚ ì´ì „ ë°ì´í„° ë¡œë“œ: {os.path.basename(latest_file)}")
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return {}

def analyze_content_changes(old_html, new_html):
    soup_old = BeautifulSoup(old_html, 'html.parser')
    soup_new = BeautifulSoup(new_html, 'html.parser')
    summary = []
    if soup_old.get_text().strip() != soup_new.get_text().strip():
        summary.append("âœï¸ ìƒì„¸ë‚´ìš©(í…ìŠ¤íŠ¸) ìˆ˜ì •")
    imgs_old = set([i['src'] for i in soup_old.find_all('img') if i.get('src')])
    imgs_new = set([i['src'] for i in soup_new.find_all('img') if i.get('src')])
    if imgs_old != imgs_new:
        summary.append("ğŸ–¼ï¸ ìƒì„¸ì´ë¯¸ì§€ êµì²´")
    return " / ".join(summary) if summary else "ğŸ¨ ë””ìì¸/ë ˆì´ì•„ì›ƒ ë³€ê²½"

# =========================================================
# [ê·¸ë£¹ A] ê¸°ì¡´ ì„±ê³µ ë©¤ë²„ ì „ìš© (V16 ë¡œì§ ë³µì›)
# ëŒ€ìƒ: SKT ë‹¤ì´ë ‰íŠ¸, ìœ ëª¨ë°”ì¼, ìŠ¤ì¹´ì´ë¼ì´í”„
# =========================================================
def extract_legacy_simple(driver, container_selector, site_name):
    cards_data = {} 
    try:
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, container_selector))
        )
        
        items = []
        # [ìœ ëª¨ë°”ì¼ V16 ì˜¤ë¦¬ì§€ë„ ë¡œì§]
        if "ìœ ëª¨ë°”ì¼" in site_name:
            items = container.find_elements(By.XPATH, ".//li | .//div[contains(@class, 'card')]")
            if not items: items = container.find_elements(By.TAG_NAME, "li")
        
        # [SKT ë‹¤ì´ë ‰íŠ¸ V16 ì˜¤ë¦¬ì§€ë„ ë¡œì§]
        elif "SKT ë‹¤ì´ë ‰íŠ¸" in site_name:
            items = container.find_elements(By.TAG_NAME, "li")
            
        # [ìŠ¤ì¹´ì´ë¼ì´í”„ V16 ì˜¤ë¦¬ì§€ë„ ë¡œì§]
        elif "ìŠ¤ì¹´ì´ë¼ì´í”„" in site_name:
            items = container.find_elements(By.XPATH, "./div")

        print(f"    [Legacy] Found {len(items)} items in {site_name}")

        for item in items:
            try:
                link_el = item.find_element(By.TAG_NAME, "a")
                href = link_el.get_attribute('href')
                if not href or "javascript" in href: continue

                title = item.text.strip().split("\n")[0]
                if not title:
                    try: title = item.find_element(By.TAG_NAME, "img").get_attribute("alt")
                    except: title = "ì œëª© ì—†ìŒ"
                
                img_src = ""
                try:
                    img = item.find_element(By.TAG_NAME, "img")
                    img_src = img.get_attribute("src")
                except: pass

                cards_data[href] = {"title": title, "img": img_src}
            except: continue
            
        return cards_data
    except Exception as e:
        print(f"    âš ï¸ [Legacy] ì¶”ì¶œ ì‹¤íŒ¨ ({site_name}): {e}")
        return {}

# =========================================================
# [ê·¸ë£¹ B] ë¬¸ì œì•„ 3ì¸ë°© ì „ìš© (JS í•´ë… + ì‹ ê·œ ë¡œì§)
# ëŒ€ìƒ: í—¬ë¡œëª¨ë°”ì¼, SK 7ëª¨ë°”ì¼, KTM
# =========================================================
def extract_special_js(driver, container_selector, site_name):
    cards_data = {} 
    try:
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, container_selector))
        )
        
        items = []
        if "í—¬
