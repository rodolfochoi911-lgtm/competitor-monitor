"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ (V24)
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-01-30 (ë¬´ì œí•œ ì „ìˆ˜ì¡°ì‚¬ / í˜ì´ì§€ë„¤ì´ì…˜ ë¶€í™œ / ì—ëŸ¬ ë¬´ì‹œ ëª¨ë“œ)
"""

import os
import json
import time
import glob
import re
import traceback
from datetime import datetime, timedelta, timezone
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"

# [ìˆ˜ì •] ì œí•œ í•´ì œ (Unlimited)
# MAX_ITEMS_PER_SITE = 9999 

KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
FILE_TIMESTAMP = NOW.strftime("%Y%m%d_%H%M%S")
DISPLAY_DATE = NOW.strftime("%Y-%m-%d")
DISPLAY_TIME = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    print("ğŸš— ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    print("âœ… ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup, .close');
            popups.forEach(function(element) { element.remove(); });
        """)
    except: pass

def scroll_to_bottom(driver):
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(5): # ì¶©ë¶„íˆ ìŠ¤í¬ë¡¤
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height: break
            last_height = new_height
    except: pass

def clean_html(html_source):
    if not html_source: return ""
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside']):
        tag.decompose()
    for hidden in soup.find_all(attrs={"style": True}):
        if "display:none" in hidden["style"].replace(" ", "").lower():
            hidden.decompose()
    return body.prettify() if (body := soup.find('body')) else "No Content"

def load_previous_data():
    json_files = glob.glob(os.path.join(DATA_DIR, "data_*.json"))
    if not json_files: return {}
    json_files.sort()
    latest_file = json_files[-1]
    print(f"ğŸ“‚ ì´ì „ ë°ì´í„° ë¡œë“œ: {os.path.basename(latest_file)}")
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return {}

# [í•µì‹¬] V24 ì¹´ë“œ ì¶”ì¶œê¸° (ë¬´ì œí•œ + JSí•´ë…)
def extract_cards_smartly(driver, container_selector, site_name):
    cards_data = {} 
    try:
        print(f"    ğŸ” [{site_name}] ì¹´ë“œ ìš”ì†Œ ì°¾ëŠ” ì¤‘...")
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, container_selector))
        )
        
        items = []
        
        # 1. SKT ë‹¤ì´ë ‰íŠ¸
        if "SKT ë‹¤ì´ë ‰íŠ¸" in site_name:
            items = container.find_elements(By.TAG_NAME, "li")

        # 2. ìœ ëª¨ë°”ì¼
        elif "ìœ ëª¨ë°”ì¼" in site_name:
            items = container.find_elements(By.XPATH, ".//li | .//div[contains(@class, 'card')]")
            if not items: items = container.find_elements(By.TAG_NAME, "li")

        # 3. ìŠ¤ì¹´ì´ë¼ì´í”„
        elif "ìŠ¤ì¹´ì´ë¼ì´í”„" in site_name:
            items = container.find_elements(By.XPATH, "./div")

        # 4. [HELLOMOBILE]
        elif "í—¬ë¡œëª¨ë°”ì¼" in site_name:
            try:
                list_ul = container.find_element(By.CSS_SELECTOR, ".event-list")
                items = list_ul.find_elements(By.TAG_NAME, "li")
            except: items = container.find_elements(By.TAG_NAME, "li")

        # 5. [SK 7MOBILE]
        elif "SK 7ì„¸ë¸ëª¨ë°”ì¼" in site_name:
            try:
                groups = container.find_elements(By.CSS_SELECTOR, ".event-group")
                for g in groups: items.extend(g.find_elements(By.TAG_NAME, "li"))
            except: items = container.find_elements(By.TAG_NAME, "li")

        # 6. KTM / ê·¸ ì™¸
        else:
            items = container.find_elements(By.TAG_NAME, "li")
            if not items: items = container.find_elements(By.TAG_NAME, "a")

        if not items:
            print("    âš ï¸ ì•„ì´í…œ ì—†ìŒ -> a íƒœê·¸ ë¹„ìƒ ìˆ˜ì§‘")
            items = container.find_elements(By.TAG_NAME, "a")

        print(f"    âœ¨ {len(items)}ê°œ í•­ëª© ë°œê²¬ (ì „ìˆ˜ì¡°ì‚¬)")

        for item in items:
            try:
                link_el = item if item.tag_name == 'a' else None
                if not link_el:
                    try: link_el = item.find_element(By.TAG_NAME, "a")
                    except: continue
                
                href = link_el.get_attribute('href')
                onclick = str(link_el.get_attribute('onclick')) 
                
                final_url = ""

                # [í•´ë… 1] í—¬ë¡œëª¨ë°”ì¼
                if "í—¬ë¡œëª¨ë°”ì¼" in site_name and "fncEventDetail" in onclick:
                    match = re.search(r"fncEventDetail\((\d+)", onclick)
                    if match: final_url = f"https://direct.lghellovision.net/event/viewEventDetail.do?idxOfEvent={match.group(1)}"
                
                # [í•´ë… 2] SK 7ëª¨ë°”ì¼
                elif "SK 7ì„¸ë¸ëª¨ë°”ì¼" in site_name and "fnSearchView" in onclick:
                    match = re.search(r"fnSearchView\('([^']+)'", onclick)
                    if match: final_url = f"https://www.sk7mobile.com/bnef/event/eventIngView.do?cntId={match.group(1)}"
                
                # [ì¼ë°˜]
                elif href and "javascript" not in href:
                    final_url = href
                elif href:
                    final_url = href

                if not final_url: continue

                title = item.text.strip().split("\n")[0]
                if not title:
                    try: title = item.find_element(By.TAG_NAME, "img").get_attribute("alt")
                    except: title = "ì œëª© ì—†ìŒ"
                
                img_src = ""
                try:
                    img = item.find_element(By.TAG_NAME, "img")
                    img_src = img.get_attribute("src")
                except: pass

                cards_data[final_url] = {"title": title, "img": img_src}
                
            except: continue
            
        return cards_data
    except Exception as e:
        print(f"    âŒ ì¹´ë“œ ì¶”ì¶œ ì¤‘ ì—ëŸ¬: {e}")
        return {}

def extract_single_page_content(driver, selector):
    try:
        container = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
        return {driver.current_url: {"title": "SKT Air ë©”ì¸", "img": "", "content": clean_html(container.get_attribute('outerHTML'))}}
    except: return {}

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    collected_items = {} 
    last_page_urls = []
    page = 1
    
    try:
        if site_name == "SKT Air":
            driver.get(base_url)
            time.sleep(3)
            return extract_single_page_content(driver, target_selector)

        # [ë¶€í™œ] í˜ì´ì§€ë„¤ì´ì…˜ Loop (ëê¹Œì§€ ê°)
        while True:
            target_url = base_url
            if pagination_param:
                if pagination_param == "#": target_url = f"{base_url}#{page}"
                else: 
                    conn = '&' if '?' in base_url else '?'
                    target_url = f"{base_url}{conn}{pagination_param}={page}"
                
            driver.get(target_url)
            # í•´ì‹œ ë°©ì‹ì€ ìƒˆë¡œê³ ì¹¨ í•„ìš”
            if pagination_param == "#": 
                driver.refresh()
                time.sleep(2)
            
            time.sleep(3)
            remove_popups(driver)
            scroll_to_bottom(driver)
            
            page_data = extract_cards_smartly(driver, target_selector, site_name)
            
            if not page_data: break # ë°ì´í„° ì—†ìœ¼ë©´ ì¢…ë£Œ
            
            # ì ˆëŒ€ê²½ë¡œ ë³€í™˜ & ì €ì¥
            current_page_urls = []
            for href, info in page_data.items():
                if href.startswith('/'):
                    from urllib.parse import urljoin
                    href = urljoin(base_url, href)
                
                # ì´ë¯¸ ìˆ˜ì§‘ëœ URLì´ë©´ ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë°©ì§€)
                if href in collected_items: continue
                
                collected_items[href] = info
                current_page_urls.append(href)

            print(f"  - Page {page}: {len(current_page_urls)}ê°œ ì‹ ê·œ ìˆ˜ì§‘")

            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if not current_page_urls: break # ì‹ ê·œ ë°ì´í„° ì—†ìœ¼ë©´ ì¢…ë£Œ
            if not pagination_param: break # ë‹¨ì¼ í˜ì´ì§€ë©´ ì¢…ë£Œ
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€ìš© (ì•ˆì „ì¥ì¹˜ê°€ ì•„ë‹ˆë¼ ì‚¬ê³  ë°©ì§€ìš©)
            if sorted(current_page_urls) == sorted(last_page_urls): break
            last_page_urls = current_page_urls
            
            page += 1
            if page > 10: break # ê·¸ë˜ë„ 10í˜ì´ì§€ ë„˜ì–´ê°€ë©´ ë„ˆë¬´ ë§ìœ¼ë‹ˆ ëŠìŒ

    except Exception as e:
        print(f"  âŒ [{site_name}] ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    print(f"  ğŸ” ìƒì„¸ ë¶„ì„ ì‹œì‘ ({len(collected_items)}ê±´) - ì „ìˆ˜ì¡°ì‚¬ ì§„í–‰ ì¤‘...")
    
    # [ì „ìˆ˜ì¡°ì‚¬] ëª¨ë“  ìƒì„¸ í˜ì´ì§€ ì ‘ì†
    for url, info in collected_items.items():
        try:
            if "javascript" not in url:
                driver.get(url)
                time.sleep(1) # ì ‘ì† ëŒ€ê¸°
                remove_popups(driver)
                collected_items[url]['content'] = clean_html(driver.page_source)
            else:
                collected_items[url]['content'] = "JS Link Only"
        except:
            collected_items[url]['content'] = "" 
            
    return collected_items

def main():
    try:
        driver = setup_driver()
        
        competitors = [
            {"name": "SKT ë‹¤ì´ë ‰íŠ¸", "url": "https://shop.tworld.co.kr/exhibition/submain", "param": None, "selector": "#wrap > div.container > div > div.event-list-wrap > div > ul"},
            {"name": "SKT Air", "url": "https://sktair-event.com/", "param": None, "selector": "#app > div > section.content"},
            {"name": "U+ ìœ ëª¨ë°”ì¼", "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", "param": None, "selector": "#wrap > main > div > section"},
            {"name": "ìŠ¤ì¹´ì´ë¼ì´í”„", "url": "https://www.skylife.co.kr/event?category=mobile", "param": "p", "selector": "body > div.pb-50.min-w-\[1248px\] > div.m-auto.max-w-\[1248px\].pt-20 > div > div > div.pt-14 > div > div.grid.grid-cols-3.gap-6.pt-4"},
            {"name": "í—¬ë¡œëª¨ë°”ì¼", "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli", "param": "#", "selector": ".event-list-wrap"},
            {"name": "SK 7ì„¸ë¸ëª¨ë°”ì¼", "url": "https://www.sk7mobile.com/bnef/event/eventIngList.do", "param": None, "selector": ".tb-list.bbs-card"},
            {"name": "KTM ëª¨ë°”ì¼", "url": "https://www.ktmmobile.com/event/eventBoardList.do", "param": None, "selector": "#listArea1"}
        ]
        
        today_results = {}
        for comp in competitors:
            try:
                today_results[comp['name']] = crawl_site_logic(driver, comp['name'], comp['url'], comp['param'], comp['selector'])
            except Exception as e:
                print(f"âŒ {comp['name']} ìŠ¤í‚µë¨ (ì—ëŸ¬): {e}")
        
        driver.quit()
        
        # ì €ì¥ (ê²°ê³¼ê°€ ìˆë“  ì—†ë“  ë¬´ì¡°ê±´ ì €ì¥)
        data_filename = f"data_{FILE_TIMESTAMP}.json"
        with open(os.path.join(DATA_DIR, data_filename), "w", encoding="utf-8") as f:
            json.dump(today_results, f, ensure_ascii=False)
            
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
        
        # ë¦¬í¬íŠ¸ ìƒì„± ë¡œì§ ë³µì› (V19 ë²„ì „ ë¡œì§ ì‚¬ìš©)
        generate_report(today_results)

    except Exception as e:
        print("â˜ ï¸ í”„ë¡œê·¸ë¨ ì „ì²´ í¬ë˜ì‹œ ë°œìƒ!")
        print(traceback.format_exc())

# [ë³µì›] ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜ (V19 ë¡œì§ ê·¸ëŒ€ë¡œ)
def generate_report(today_results):
    yesterday_results = load_previous_data()
    report_body = ""
    total_change_count = 0
    company_summary = []
    
    for name, pages in today_results.items():
        site_changes = ""
        site_change_count = 0 
        old_pages = yesterday_results.get(name, {})
        all_urls = set(pages.keys()) | set(old_pages.keys())
        
        for url in all_urls:
            is_changed = False
            change_type = ""
            reason = ""
            curr = pages.get(url, {"title": "?", "img": "", "content": ""})
            prev = old_pages.get(url, {"title": "?", "img": "", "content": ""})
            
            if url in pages and url not in old_pages:
                is_changed = True; change_type = "NEW"; reason = "ì‹ ê·œ ì´ë²¤íŠ¸"
            elif url not in pages and url in old_pages:
                is_changed = True; change_type = "DELETED"; reason = "ì¢…ë£Œëœ ì´ë²¤íŠ¸"
            elif curr['content'].replace(" ","") != prev['content'].replace(" ",""):
                is_changed = True; change_type = "UPDATED"; reason = analyze_content_changes(prev['content'], curr['content'])

            if is_changed:
                color = "green" if change_type == "NEW" else "red" if change_type == "DELETED" else "orange"
                img_html = f"<img src='{curr['img']}' style='height:50px; vertical-align:middle; margin-right:10px;'>" if curr['img'] else ""
                site_changes += f"""<div style="border-left: 5px solid {color}; padding: 10px; margin-bottom: 10px; background: #fff;">
                    <h3 style="margin: 0 0 5px 0;"><span style="color:{color};">[{change_type}]</span> {curr['title']}</h3>
                    <div style="display:flex; align-items:center;">{img_html}<div style="font-size: 0.9em; color: #555;"><b>ë³€ê²½ ì‚¬ìœ :</b> {reason}<br><a href="{url}" target="_blank">ğŸ”— ë°”ë¡œê°€ê¸°</a></div></div></div>"""
                site_change_count += 1
        
        if site_changes:
            report_body += f"<h2>{name} ({site_change_count}ê±´)</h2>{site_changes}<hr>"
            total_change_count += site_change_count
            company_summary.append(f"{name}({site_change_count})")

    # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥ ë° ìŠ¬ë™ ì „ì†¡ (ìƒëµëœ ë¶€ë¶„ ë³µì›)
    summary_text = f"ì´ {total_change_count}ê±´ ì—…ë°ì´íŠ¸ ({', '.join(company_summary)})" if total_change_count > 0 else "íŠ¹ì´ì‚¬í•­ ì—†ìŒ"
    report_header = f"<h1>ğŸ“… {DISPLAY_DATE} ë¦¬í¬íŠ¸ <span style='font-size:0.6em; color:#888;'>({DISPLAY_TIME} KST)</span></h1><div style='background-color:#f4f4f4; padding:15px;'><h3>ğŸ“Š {summary_text}</h3></div><hr>"
    full_report = report_header + (report_body if total_change_count > 0 else "<p>âœ… ê¸ˆì¼ ë³€ë™ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.</p>")
    
    filename = f"report_{FILE_TIMESTAMP}.html"
    with open(os.path.join(REPORT_DIR, filename), "w", encoding="utf-8") as f: f.write(full_report)
    update_index_page()
    
    dashboard_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/"
    report_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{filename}"
    
    payload = {"text": f"ğŸ“¢ *[KST {DISPLAY_TIME}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… *ìš”ì•½:* {summary_text}\n\nğŸ‘‰ *ë³€ê²½ ë¦¬í¬íŠ¸:* {report_url}\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ:* {dashboard_url}"}
    if SLACK_WEBHOOK_URL: requests.post(SLACK_WEBHOOK_URL, json=payload)

if __name__ == "__main__":
    main()
