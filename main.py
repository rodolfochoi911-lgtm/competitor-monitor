"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ (V52)
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-02-02 (ëª¨ë“  ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì§„ì…í•˜ì—¬ ë³¸ë¬¸ ìˆ˜ì§‘ + ë³€ê²½ ê°ì§€ ê³ ë„í™”)
"""

import os
import json
import time
import glob
import random
import re
import traceback
from datetime import datetime, timedelta, timezone
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup

# [í•µì‹¬] ì–¸ë””í…í‹°ë“œ í¬ë¡¬
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"

KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
FILE_TIMESTAMP = NOW.strftime("%Y%m%d_%H%M%S")
DISPLAY_DATE = NOW.strftime("%Y-%m-%d")
DISPLAY_TIME = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    print("ğŸš— [V52] ë“œë¼ì´ë²„ ì„¤ì • (ìƒì„¸ ìˆ˜ì§‘ ëª¨ë“œ)...")
    options = uc.ChromeOptions()
    options.add_argument("--headless=new") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ko_KR")
    
    # [ë²„ì „ ê³ ì •] 144
    driver = uc.Chrome(options=options, version_main=144)
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup, .close');
            popups.forEach(function(element) { element.remove(); });
        """)
    except: pass

def scroll_to_bottom(driver):
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(3): 
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(1.0, 2.0))
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height: break
            last_height = new_height
    except: pass

def clean_html(html_source):
    """HTMLì—ì„œ ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ë“± ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"""
    if not html_source: return ""
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside', 'link']):
        tag.decompose()
    # ê³µë°± ì œê±°í•œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë°˜í™˜ (ë¹„êµ ì •í™•ë„ í–¥ìƒ)
    return soup.get_text(separator=' ', strip=True)

def load_previous_data():
    json_files = glob.glob(os.path.join(DATA_DIR, "data_*.json"))
    if not json_files: return {}
    json_files.sort()
    latest_file = json_files[-1]
    print(f"ğŸ“‚ ì–´ì œ ë°ì´í„° ë¡œë“œ: {latest_file}")
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return {}

# [ë¹„êµ ë¡œì§] ì´ì œ ë³¸ë¬¸(Content)ê¹Œì§€ ê¼¼ê¼¼íˆ ë¹„êµí•¨
def analyze_content_changes(prev, curr):
    # 1. ì œëª© ë¹„êµ
    if prev.get('title', '').strip() != curr.get('title', '').strip():
        return f"âœï¸ ì œëª© ë³€ê²½: {prev.get('title')} -> {curr.get('title')}"
    
    # 2. ë³¸ë¬¸ ë¹„êµ (ìƒì„¸ í˜ì´ì§€ í…ìŠ¤íŠ¸)
    # ê³µë°±/ì¤„ë°”ê¿ˆ ë‹¤ ì—†ì• ê³  ì•Œë§¹ì´ ê¸€ìë§Œ ë¹„êµ
    prev_txt = prev.get('content', '').replace(" ", "").replace("\n", "")
    curr_txt = curr.get('content', '').replace(" ", "").replace("\n", "")
    
    if prev_txt and curr_txt and prev_txt != curr_txt:
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì°¨ì´ëŠ” ê·¸ëƒ¥ 'ë³¸ë¬¸ ìˆ˜ì •'ìœ¼ë¡œ í‰ì¹¨
        return "ğŸ“ ìƒì„¸ ë³¸ë¬¸ ë‚´ìš© ìˆ˜ì •ë¨"
            
    return None 

# =========================================================
# [í•µì‹¬ í•¨ìˆ˜] ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸ ìˆ˜ì§‘ê¸° (Deep Crawler)
# =========================================================
def extract_deep_events(driver, site_name, keyword_list, onclick_pattern=None, base_url=""):
    collected_data = {}
    
    try:
        # [Step 1] ëª©ë¡ í˜ì´ì§€ ë¡œë”©
        time.sleep(5)
        scroll_to_bottom(driver)
        
        # ìŠ¤ì¹´ì´ë¼ì´í”„ ì°¨ë‹¨ ì²´í¬
        if site_name == "ìŠ¤ì¹´ì´ë¼ì´í”„":
            if "ì ‘ì†ì´ ì›í™œí•˜ì§€" in driver.page_source:
                print("    ğŸš¨ [Skylife] ì°¨ë‹¨ë¨ (ëª©ë¡ ì§„ì… ë¶ˆê°€).")
                return {}

        # [Step 2] ë§í¬ ìˆ˜ì§‘ (ëª©ë¡)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        all_links = soup.find_all('a')
        
        target_urls = set()
        
        # ë§í¬ ì¶”ì¶œ ë° ì •ì œ
        for link in all_links:
            href = link.get('href', '')
            onclick = link.get('onclick', '')
            final_url = ""

            if href and "javascript" not in href and "#" != href:
                for key in keyword_list:
                    if key in href:
                        final_url = urljoin(base_url, href)
                        break
            elif onclick and onclick_pattern:
                match = re.search(onclick_pattern, onclick)
                if match:
                    if site_name == "í—¬ë¡œëª¨ë°”ì¼":
                        final_url = f"https://direct.lghellovision.net/event/viewEventDetail.do?idxOfEvent={match.group(1)}"
                    elif site_name == "SK 7ì„¸ë¸ëª¨ë°”ì¼":
                        final_url = f"https://www.sk7mobile.com/bnef/event/eventIngView.do?cntId={match.group(1)}"

            if site_name == "KTM ëª¨ë°”ì¼" and not final_url:
                seq = link.get('ntcartseq')
                if seq: final_url = f"https://www.ktmmobile.com/event/eventDetail.do?ntcartSeq={seq}"

            if final_url:
                if any(x in final_url for x in ["login", "my", "faq", "support", "logout"]): continue
                target_urls.add(final_url)
        
        print(f"    [{site_name}] ë°œê²¬ëœ ìƒì„¸ URL: {len(target_urls)}ê°œ -> ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘")

        # [Step 3] ìƒì„¸ í˜ì´ì§€ í•˜ë‚˜ì”© ë°©ë¬¸ (Deep Crawling)
        count = 0
        for url in target_urls:
            try:
                # ìƒì„¸ í˜ì´ì§€ ì´ë™
                driver.get(url)
                time.sleep(random.uniform(2.0, 3.5)) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content_text = clean_html(driver.page_source)
                
                # ì œëª© ì¶”ì¶œ (title íƒœê·¸ í™œìš©)
                page_title = driver.title
                if not page_title or site_name in page_title: # ì‚¬ì´íŠ¸ ì´ë¦„ë§Œ ìˆìœ¼ë©´ ë³¸ë¬¸ì—ì„œ ì°¾ê¸°
                    try: page_title = driver.find_element(By.TAG_NAME, "h1").text
                    except: 
                        try: page_title = driver.find_element(By.CSS_SELECTOR, "h2").text
                        except: page_title = "ì œëª© ì—†ìŒ"

                # ì¸ë„¤ì¼ (ëŒ€í‘œ ì´ë¯¸ì§€) - ë©”íƒ€íƒœê·¸ í™œìš©
                img_src = ""
                try:
                    meta_img = driver.find_element(By.CSS_SELECTOR, "meta[property='og:image']")
                    img_src = meta_img.get_attribute("content")
                except: pass
                
                collected_data[url] = {
                    "title": page_title.strip(),
                    "img": img_src,
                    "content": content_text[:3000] # ë„ˆë¬´ ê¸¸ë©´ ìë¦„ (ë¹„êµìš©)
                }
                count += 1
                print(f"      - [{count}/{len(target_urls)}] ìˆ˜ì§‘ ì™„ë£Œ: {page_title[:20]}...")
                
                # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ë„ˆë¬´ ë§ì´ëŠ” ìˆ˜ì§‘ ì•ˆ í•¨ (ìµœëŒ€ 20ê°œ ì œí•œ)
                if count >= 20: break
                
            except Exception as e:
                print(f"      âŒ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
                continue
                
    except Exception as e:
        print(f"    âš ï¸ {site_name} ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
    return collected_data


def extract_single_page_content(driver, selector):
    try:
        container = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
        return {driver.current_url: {"title": "SKT Air ë©”ì¸", "img": "", "content": clean_html(container.get_attribute('outerHTML'))}}
    except: return {}

# =========================================================
# í†µí•© í¬ë¡¤ë§ ë¡œì§
# =========================================================
def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] ì‹œì‘...")
    
    if site_name == "SKT Air":
        driver.get(base_url); time.sleep(3)
        return extract_single_page_content(driver, target_selector)
    
    # ì„¤ì •
    keywords = []
    onclick = None
    base = ""
    
    if site_name == "U+ ìœ ëª¨ë°”ì¼": keywords = ["event", "benefit"]; base = "https://www.uplusumobile.com"
    elif site_name == "KTM ëª¨ë°”ì¼": keywords = ["eventDetail"]; base = "https://www.ktmmobile.com"
    elif site_name == "ìŠ¤ì¹´ì´ë¼ì´í”„": keywords = ["/event/"]; base = "https://www.skylife.co.kr"
    elif site_name == "í—¬ë¡œëª¨ë°”ì¼": keywords = ["event"]; onclick = r"(\d+)"; base = "https://direct.lghellovision.net"
    elif site_name == "SK 7ì„¸ë¸ëª¨ë°”ì¼": keywords = ["event"]; onclick = r"['\"]([^'\"]+)['\"]"; base = "https://www.sk7mobile.com"
    elif site_name == "SKT ë‹¤ì´ë ‰íŠ¸": keywords = ["event", "plan"]; base = "https://shop.tworld.co.kr"
    
    # [Step 1] ëª©ë¡ í˜ì´ì§€ ì§„ì…
    driver.get(base_url)
    time.sleep(3)
    remove_popups(driver)
    
    # [Step 2] Deep Crawler ì‹¤í–‰ (ëª©ë¡ -> ìƒì„¸ ë°©ë¬¸ -> ìˆ˜ì§‘)
    return extract_deep_events(driver, site_name, keywords, onclick, base)

def update_index_page():
    report_files = glob.glob(os.path.join(REPORT_DIR, "report_*.html"))
    report_files.sort(reverse=True)
    index_html = f"<html><body><h1>ëª¨ë‹ˆí„°ë§ ì•„ì¹´ì´ë¸Œ</h1><p>Update: {DISPLAY_DATE} {DISPLAY_TIME}</p>"
    for f in report_files:
        name = os.path.basename(f)
        index_html += f"<div><a href='reports/{name}'>{name}</a></div>"
    index_html += "</body></html>"
    with open(os.path.join(DOCS_DIR, "index.html"), "w", encoding="utf-8") as f: f.write(index_html)

def main():
    try:
        driver = setup_driver()
        
        competitors = [
            {"name": "SKT ë‹¤ì´ë ‰íŠ¸", "url": "https://shop.tworld.co.kr/exhibition/submain", "param": None, "selector": ""},
            {"name": "SKT Air", "url": "https://sktair-event.com/", "param": None, "selector": "#app > div > section.content"},
            {"name": "U+ ìœ ëª¨ë°”ì¼", "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", "param": None, "selector": ""},
            {"name": "KTM ëª¨ë°”ì¼", "url": "https://www.ktmmobile.com/event/eventBoardList.do", "param": None, "selector": ""},
            {"name": "ìŠ¤ì¹´ì´ë¼ì´í”„", "url": "https://www.skylife.co.kr/event?category=mobile", "param": "p", "selector": ""},
            {"name": "í—¬ë¡œëª¨ë°”ì¼", "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli", "param": "#", "selector": ""},
            {"name": "SK 7ì„¸ë¸ëª¨ë°”ì¼", "url": "https://www.sk7mobile.com/bnef/event/eventIngList.do", "param": None, "selector": ""}
        ]
        
        yesterday_results = load_previous_data()
        today_results = {}
        
        for comp in competitors:
            try:
                data = crawl_site_logic(driver, comp['name'], comp['url'], comp['param'], comp['selector'])
                
                # [ì•ˆì „ì¥ì¹˜] 0ê°œë©´ ê¸°ì¡´ ë°ì´í„° ìœ ì§€ (ì°¨ë‹¨/ì˜¤ë¥˜ ë°©ì–´)
                if len(data) == 0:
                    print(f"    ğŸ›‘ {comp['name']} ìˆ˜ì§‘ ì‹¤íŒ¨(0ê±´). ê¸°ì¡´ ë°ì´í„° ìœ ì§€.")
                    today_results[comp['name']] = yesterday_results.get(comp['name'], {})
                else:
                    today_results[comp['name']] = data
                    
            except Exception as e:
                print(f"âŒ {comp['name']} Error: {e}")
                today_results[comp['name']] = yesterday_results.get(comp['name'], {})
        
        driver.quit()
        
        data_filename = f"data_{FILE_TIMESTAMP}.json"
        with open(os.path.join(DATA_DIR, data_filename), "w", encoding="utf-8") as f:
            json.dump(today_results, f, ensure_ascii=False)
            
        print("âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report_body = ""
        total_change_count = 0
        company_summary = []
        
        for name, pages in today_results.items():
            site_change_count = 0 
            old_pages = yesterday_results.get(name, {})
            all_urls = set(pages.keys()) | set(old_pages.keys())
            site_changes = ""
            
            for url in all_urls:
                change_type = ""; reason = ""
                curr = pages.get(url)
                prev = old_pages.get(url)

                if curr and not prev:
                    change_type = "NEW"; reason = "ì‹ ê·œ ì´ë²¤íŠ¸"
                elif not curr and prev:
                    change_type = "DELETED"; reason = "ì¢…ë£Œëœ ì´ë²¤íŠ¸"
                elif curr and prev:
                    diff_reason = analyze_content_changes(prev, curr)
                    if diff_reason:
                        change_type = "UPDATED"; reason = diff_reason

                if change_type:
                    color = "green" if change_type == "NEW" else "red" if change_type == "DELETED" else "orange"
                    img_src = curr.get('img') if curr else prev.get('img')
                    img_html = f"<img src='{img_src}' style='height:50px; margin-right:10px;'>" if img_src else ""
                    title = curr.get('title') if curr else prev.get('title')
                    
                    site_changes += f"""
                    <div style="border-left: 5px solid {color}; padding: 10px; margin-bottom: 10px; background: #fff;">
                        <h3 style="margin: 0 0 5px 0;"><span style="color:{color};">[{change_type}]</span> {title}</h3>
                        <div style="display:flex; align-items:center;">
                            {img_html}
                            <div style="font-size: 0.9em; color: #555;"><b>ì‚¬ìœ :</b> {reason}<br><a href="{url}" target="_blank">ğŸ”— ë§í¬</a></div>
                        </div>
                    </div>
                    """
                    site_change_count += 1
            
            if site_changes:
                report_body += f"<h2>{name} ({site_change_count}ê±´)</h2>{site_changes}<hr>"
                total_change_count += site_change_count
                company_summary.append(f"{name}({site_change_count})")

        summary_text = f"ì´ {total_change_count}ê±´ ë³€ë™ ({', '.join(company_summary)})" if total_change_count > 0 else "íŠ¹ì´ì‚¬í•­ ì—†ìŒ"
        report_header = f"<h1>ğŸ“… {DISPLAY_DATE} ë¦¬í¬íŠ¸</h1><div><h3>ğŸ“Š {summary_text}</h3></div><hr>"
        
        filename = f"report_{FILE_TIMESTAMP}.html"
        with open(os.path.join(REPORT_DIR, filename), "w", encoding="utf-8") as f: f.write(report_header + report_body)
        update_index_page()
        
        # ì „ì²´ ëª©ë¡ íŒŒì¼ ìƒì„± (ë””ë²„ê¹…ìš©)
        full_list_html = f"<h1>ğŸ“‚ {DISPLAY_DATE} ì „ì²´ ëª©ë¡</h1><hr>"
        for name, pages in today_results.items():
            full_list_html += f"<h3>{name} ({len(pages)}ê°œ)</h3><ul>"
            for url, data in pages.items():
                full_list_html += f"<li><a href='{url}'>{data.get('title')}</a></li>"
            full_list_html += "</ul><hr>"
        
        list_filename = f"list_{FILE_TIMESTAMP}.html"
        with open(os.path.join(REPORT_DIR, list_filename), "w", encoding="utf-8") as f: f.write(full_list_html)

        dashboard_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/"
        report_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{filename}"
        list_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{list_filename}"
        
        payload = {"text": f"ğŸ“¢ *[KST {DISPLAY_TIME}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… *ìš”ì•½:* {summary_text}\n\nğŸ‘‰ *ë³€ê²½ ë¦¬í¬íŠ¸:* {report_url}\nğŸ—‚ï¸ *ì „ì²´ ëª©ë¡:* {list_url}\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ:* {dashboard_url}"}
        
        if SLACK_WEBHOOK_URL:
            requests.post(SLACK_WEBHOOK_URL, json=payload)
            print("âœ… ìŠ¬ë™ ì „ì†¡ ì™„ë£Œ")

    except Exception as e:
        print(f"ğŸ”¥ Critical Error: {traceback.format_exc()}")

if __name__ == "__main__":
    main()
