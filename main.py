"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ (V64)
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-02-05 (V64: ë³¸ë¬¸ ë‚´ ë§í¬ ê°ì§€ ì œê±° + ìˆœìˆ˜ í…ìŠ¤íŠ¸ í˜•ê´‘íœ ë¹„êµ + ëª©ë¡ ì¸ë„¤ì¼ ìµœì í™”)
"""

import os
import json
import time
import glob
import random
import re
import traceback
import html
import difflib
import requests
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm"
REPO_NAME = "competitor-monitor"
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"
SIMILARITY_THRESHOLD = 0.8

KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
FILE_TIMESTAMP = NOW.strftime("%Y%m%d_%H%M%S")
DISPLAY_DATE = NOW.strftime("%Y-%m-%d")
DISPLAY_TIME = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

# =========================================================
# [ìœ í‹¸ë¦¬í‹°] ìŠ¬ë™ ì•ŒëŒ ì „ì†¡
# =========================================================
def send_slack_alert(webhook_url, payload):
    if not webhook_url: return
    try:
        requests.post(webhook_url, json=payload, headers={"Content-Type": "application/json"}, timeout=10)
    except: pass

# =========================================================
# [í•µì‹¬] ë…¸ì´ì¦ˆ ì œê±° (íƒ€ì´ë¨¸/ì¹´ìš´íŠ¸ë‹¤ìš´ ì°¨ë‹¨)
# =========================================================
def clean_noise(text):
    if not text: return ""
    # 1. ì¡°íšŒìˆ˜ ì œê±°
    text = re.sub(r'(ì¡°íšŒ|view|ì½ìŒ)(ìˆ˜)?[\s:.]*[\d,]+', '', text, flags=re.IGNORECASE)
    
    # 2. íƒ€ì´ë¨¸ íŒ¨í„´ ì œê±° (D-Day, ì‹œê°„, ë‚¨ì€ ê¸°ê°„)
    # 13 : 15 / 13:15 / 12ì‹œ 30ë¶„ ë“±
    text = re.sub(r'\d{1,2}\s*[:ì‹œ]\s*\d{1,2}(\s*[:ë¶„]\s*\d{1,2})?', '', text)
    text = re.sub(r'D-[\dDay]+', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\d+(ì¼|ì‹œê°„|ë¶„|ì´ˆ)\s*(ë‚¨ìŒ|ë‚¨ì•˜|ì „|í›„)', '', text)
    text = re.sub(r'(ë§ˆê°|ì¢…ë£Œ|ì´ë²¤íŠ¸)\s*(ê¹Œì§€)?', '', text)
    
    # 3. ê³µë°± ë° íŠ¹ìˆ˜ ë…¸ì´ì¦ˆ ì •ë¦¬
    text = re.sub(r'Loading.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_html(html_source):
    if not html_source: return ""
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside', 'link', 'form']):
        tag.decompose()
    return soup.body.prettify() if soup.body else soup.prettify()

def get_clean_text(html_content):
    """
    [V64 ë³µêµ¬] ë³¸ë¬¸ ë‚´ ë§í¬(B) ì¶”ì¶œ ë¡œì§ì„ ì œê±°í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•¨.
    """
    if not html_content: return ""
    soup = BeautifulSoup(html_content, "html.parser")
    return soup.get_text(separator=" ", strip=True)

def calculate_similarity(text1, text2):
    if not text1 or not text2: return 0.0
    return difflib.SequenceMatcher(None, text1, text2).ratio()

# =========================================================
# [ì‹œê°í™”] ë³€ê²½ì‚¬í•­ í˜•ê´‘íœ í•˜ì´ë¼ì´íŒ… ìƒì„±ê¸°
# =========================================================
def generate_diff_html(old_text, new_text):
    matcher = difflib.SequenceMatcher(None, old_text, new_text)
    result_html = []
    has_change = False
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            content = old_text[i1:i2]
            if len(content) > 60:
                result_html.append(content[:30] + " ... " + content[-30:])
            else:
                result_html.append(content)
        elif tag in ('replace', 'delete', 'insert'):
            has_change = True
            old_part = html.escape(old_text[i1:i2]) if i1 != i2 else ""
            new_part = html.escape(new_text[j1:j2]) if j1 != j2 else ""
            
            if tag == 'replace':
                result_html.append(f'<span style="background:#ffeef0; text-decoration:line-through; color:#999;">{old_part}</span> â†’ <span style="background:#e6fffa; color:#006600; font-weight:bold; padding:0 4px;">{new_part}</span>')
            elif tag == 'delete':
                result_html.append(f'<span style="background:#ffeef0; text-decoration:line-through; color:#999;">{old_part}</span>')
            elif tag == 'insert':
                result_html.append(f'<span style="background:#e6fffa; color:#006600; font-weight:bold; padding:0 4px;">{new_part}</span>')

    if not has_change: return ""
    return f'<div style="font-size:13px; line-height:1.6; color:#444; background:#fafafa; padding:12px; border-radius:8px; border-left:4px solid #3498db; margin-top:10px;">{" ".join(result_html)}</div>'

def check_update_same_url(prev, curr):
    reasons = []
    diff_html = ""
    
    # ì œëª© ë¹„êµ
    if prev.get('title', '').strip() != curr.get('title', '').strip():
        reasons.append("ì œëª© ë³€ê²½")
        diff_html += f"<div style='margin-bottom:8px;'><b>ì œëª©:</b> {prev.get('title')} <span style='color:blue;'>â–¶</span> <b>{curr.get('title')}</b></div>"
    
    # ë³¸ë¬¸ ë¹„êµ (ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ)
    prev_raw = get_clean_text(prev.get('content', ''))
    curr_raw = get_clean_text(curr.get('content', ''))
    prev_clean = clean_noise(prev_raw)
    curr_clean = clean_noise(curr_raw)
    
    if prev_clean and curr_clean:
        if calculate_similarity(prev_clean, curr_clean) < 1.0: 
            reasons.append("ë³¸ë¬¸ ìˆ˜ì •")
            diff_html += generate_diff_html(prev_clean, curr_clean)

    # ì¸ë„¤ì¼ ë¹„êµ
    if prev.get('img', '').strip() != curr.get('img', '').strip():
        reasons.append("ì¸ë„¤ì¼ ë³€ê²½")
            
    if reasons: 
        return {"msg": f"{', '.join(reasons)}", "html": diff_html}
    return None

# =========================================================
# [í¬ë¡¤ëŸ¬] ëª©ë¡ ê¸°ë°˜ ìˆ˜ì§‘ ë¡œì§ (ì¸ë„¤ì¼ ì„ ì )
# =========================================================
def setup_driver():
    options = uc.ChromeOptions()
    options.add_argument("--headless=new") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    driver = uc.Chrome(options=options, version_main=144)
    return driver

def extract_list_with_thumbnails(driver, site_name, keyword_list, onclick_pattern=None, base_url="", target_selector=None):
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ ì´ë²¤íŠ¸ URLê³¼ ì¸ë„¤ì¼ì„ ì„¸íŠ¸ë¡œ ìˆ˜ì§‘í•¨.
    """
    targets = {} # { url: thumbnail_url }
    try:
        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        for link in soup.find_all('a'):
            href = link.get('href', '')
            onclick = link.get('onclick', '')
            final_url = ""

            if href and "javascript" not in href and "#" != href:
                for key in keyword_list:
                    if key in href:
                        final_url = urljoin(base_url, href)
                        break
            elif onclick and onclick_pattern:
                match = re.search(onclick_pattern, onclick)
                if match:
                    if site_name == "í—¬ë¡œëª¨ë°”ì¼": final_url = f"https://direct.lghellovision.net/event/viewEventDetail.do?idxOfEvent={match.group(1)}"
                    elif site_name == "SK 7ì„¸ë¸ëª¨ë°”ì¼": final_url = f"https://www.sk7mobile.com/bnef/event/eventIngView.do?cntId={match.group(1)}"
            
            if site_name == "KTM ëª¨ë°”ì¼" and not final_url:
                seq = link.get('ntcartseq')
                if seq: final_url = f"https://www.ktmmobile.com/event/eventDetail.do?ntcartSeq={seq}"

            if final_url:
                if any(x in final_url for x in ["login", "my", "faq", "logout"]): continue
                img_tag = link.find('img')
                # ëª©ë¡ ì´ë¯¸ì§€ ì£¼ì†Œ ì¶”ì¶œ
                thumb = urljoin(base_url, img_tag.get('src') or img_tag.get('data-src')) if img_tag else ""
                if final_url not in targets or (thumb and not targets[final_url]):
                    targets[final_url] = thumb
    except: pass

    final_data = {}
    for url, thumb in targets.items():
        try:
            driver.get(url)
            time.sleep(2)
            content = ""
            if target_selector:
                try: content = clean_html(driver.find_element(By.CSS_SELECTOR, target_selector).get_attribute('outerHTML'))
                except: content = clean_html(driver.page_source)
            else: content = clean_html(driver.page_source)
            
            title = ""
            try: title = driver.find_element(By.TAG_NAME, "h1").text.strip()
            except: pass
            if not title: title = driver.title.strip()

            final_data[url] = {"title": title, "img": thumb, "content": content[:15000]}
        except: continue
    return final_data

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] í¬ë¡¤ë§ ì‹œì‘...")
    if site_name == "SKT Air":
        driver.get(base_url); time.sleep(3)
        try:
            cont = driver.find_element(By.CSS_SELECTOR, target_selector)
            return {driver.current_url: {"title": "SKT Air ë©”ì¸", "img": "", "content": clean_html(cont.get_attribute('outerHTML'))}}
        except: return {}
    
    keywords = []
    onclick = None
    base = ""
    if site_name == "U+ ìœ ëª¨ë°”ì¼": keywords = ["event", "benefit"]; base = "https://www.uplusumobile.com"
    elif site_name == "KTM ëª¨ë°”ì¼": keywords = ["eventDetail"]; base = "https://www.ktmmobile.com"
    elif site_name == "ìŠ¤ì¹´ì´ë¼ì´í”„": keywords = ["/event/"]; base = "https://www.skylife.co.kr"
    elif site_name == "í—¬ë¡œëª¨ë°”ì¼": keywords = ["event"]; onclick = r"(\d+)"; base = "https://direct.lghellovision.net"
    elif site_name == "SK 7ì„¸ë¸ëª¨ë°”ì¼": keywords = ["event"]; onclick = r"['\"]([^'\"]+)['\"]"; base = "https://www.sk7mobile.com"
    elif site_name == "SKT ë‹¤ì´ë ‰íŠ¸": keywords = ["event", "plan"]; base = "https://shop.tworld.co.kr"
    
    collected = {}
    # ëª©ë¡ í˜ì´ì§€ ìˆœíšŒ
    for page in range(1, 4):
        t_url = f"{base_url}{('&' if '?' in base_url else '?')}{pagination_param}={page}" if pagination_param and pagination_param != "#" else base_url
        driver.get(t_url); time.sleep(2)
        data = extract_list_with_thumbnails(driver, site_name, keywords, onclick, base, target_selector)
        if not data: break
        collected.update(data)
        if not pagination_param: break
    return collected

# =========================================================
# [ëŒ€ì‹œë³´ë“œ] ë©”ì¸ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
# =========================================================
def update_index_page(change_stats):
    report_files = glob.glob(os.path.join(REPORT_DIR, "report_*.html"))
    report_files.sort(reverse=True)
    
    index_html = f"""
    <html><head><meta charset="utf-8"><title>Competitor Monitor Dashboard</title>
    <style>body{{font-family:sans-serif; padding:40px; background:#f5f7fa; color:#333;}} 
    .card{{background:white; padding:25px; border-radius:15px; box-shadow:0 5px 15px rgba(0,0,0,0.05); margin-bottom:30px;}}
    a{{color:#3498db; text-decoration:none;}} a:hover{{text-decoration:underline;}}
    </style></head><body>
    <div class="card"><h1>ğŸ“Š ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ê´€ì œíƒ‘ ({DISPLAY_DATE})</h1>
    <p>Last Crawl: {DISPLAY_TIME}</p></div>
    <div class="card"><h3>ğŸ“‘ ìµœì‹  ë¦¬í¬íŠ¸ íˆìŠ¤í† ë¦¬</h3>
    {''.join([f"<div style='margin-bottom:10px;'>ğŸ“… <a href='reports/{os.path.basename(f)}'>{os.path.basename(f)}</a></div>" for f in report_files[:15]])}
    </div></body></html>
    """
    with open(os.path.join(DOCS_DIR, "index.html"), "w", encoding="utf-8") as f: f.write(index_html)
    with open(os.path.join(DOCS_DIR, ".nojekyll"), "w") as f: f.write("")

# =========================================================
# [ë©”ì¸] ì‹¤í–‰ ì—”ì§„
# =========================================================
def main():
    try:
        driver = setup_driver()
        competitors = [
            {"name": "SKT ë‹¤ì´ë ‰íŠ¸", "url": "https://shop.tworld.co.kr/exhibition/submain", "param": None, "selector": "#contents"},
            {"name": "SKT Air", "url": "https://sktair-event.com/", "param": None, "selector": "#app > div > section.content"},
            {"name": "U+ ìœ ëª¨ë°”ì¼", "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", "param": None, "selector": ""},
            {"name": "KTM ëª¨ë°”ì¼", "url": "https://www.ktmmobile.com/event/eventBoardList.do", "param": None, "selector": ""},
            {"name": "ìŠ¤ì¹´ì´ë¼ì´í”„", "url": "https://www.skylife.co.kr/event?category=mobile", "param": "p", "selector": ""},
            {"name": "í—¬ë¡œëª¨ë°”ì¼", "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli", "param": "#", "selector": ""},
            {"name": "SK 7ì„¸ë¸ëª¨ë°”ì¼", "url": "https://www.sk7mobile.com/bnef/event/eventIngList.do", "param": None, "selector": ""}
        ]
        
        yesterday_data = load_previous_data()
        today_data = {}
        change_stats = {c['name']: {'new': 0, 'updated': 0, 'deleted': 0} for c in competitors}
        
        for c in competitors:
            try:
                result = crawl_site_logic(driver, c['name'], c['url'], c['param'], c['selector'])
                today_data[c['name']] = result if result else yesterday_data.get(c['name'], {})
            except: today_data[c['name']] = yesterday_data.get(c['name'], {})
        
        driver.quit()
        
        # ì˜¤ëŠ˜ì˜ ë°ì´í„° ì €ì¥
        with open(os.path.join(DATA_DIR, f"data_{FILE_TIMESTAMP}.json"), "w", encoding="utf-8") as f:
            json.dump(today_data, f, ensure_ascii=False)
            
        report_body = ""
        total_changes = 0
        company_summary = []
        
        for name, pages in today_data.items():
            old = yesterday_data.get(name, {})
            list_new = [{"url": u, "data": pages[u]} for u in (set(pages.keys()) - set(old.keys()))]
            list_del = [{"url": u, "data": old[u]} for u in (set(old.keys()) - set(pages.keys()))]
            list_upd = []
            
            for url in (set(pages.keys()) & set(old.keys())):
                diff = check_update_same_url(old[url], pages[url])
                if diff: list_upd.append({"url": url, "reason": diff['msg'], "data": pages[url], "diff_html": diff['html']})
            
            change_stats[name].update({'new': len(list_new), 'updated': len(list_upd), 'deleted': len(list_del)})
            cnt = len(list_new) + len(list_upd) + len(list_del)
            
            if cnt > 0:
                s_html = f"<div style='margin-bottom:50px;'><h2>ğŸ¢ {name} ({cnt}ê±´ ë³€ë™)</h2>"
                for i in list_new:
                    img = f"<img src='{i['data']['img']}' style='height:80px; margin-right:15px; vertical-align:middle;'>" if i['data']['img'] else ""
                    s_html += f"<div style='background:#f9fff9; padding:15px; border:1px solid #cfc; border-radius:8px; margin-bottom:10px;'>{img}<b>[ì‹ ê·œ] {i['data']['title']}</b><br><a href='{i['url']}' target='_blank'>ìƒì„¸ë³´ê¸°</a></div>"
                for i in list_upd:
                    s_html += f"<div style='background:#fffcf5; padding:15px; border:1px solid #fc9; border-radius:8px; margin-bottom:10px;'><b>[ë³€ê²½] {i['data']['title']}</b><br>{i['diff_html']}<br><a href='{i['url']}' target='_blank'>ìƒì„¸ë³´ê¸°</a></div>"
                for i in list_del:
                    s_html += f"<div style='background:#fff5f5; padding:15px; border:1px solid #fcc; border-radius:8px; margin-bottom:10px; color:#999;'><strike>{i['data']['title']}</strike> (ì¢…ë£Œë¨)</div>"
                report_body += s_html + "</div><hr>"
                total_changes += cnt
                company_summary.append(f"{name}({cnt})")

        # ë¦¬í¬íŠ¸ íŒŒì¼ ìƒì„±
        report_file = f"report_{FILE_TIMESTAMP}.html"
        with open(os.path.join(REPORT_DIR, report_file), "w", encoding="utf-8") as f:
            f.write(f"<html><head><meta charset='utf-8'><style>body{{font-family:sans-serif; line-height:1.6; padding:30px; color:#333;}} b{{color:#e67e22;}}</style></head><body><h1>ğŸ“… {DISPLAY_DATE} ê²½ìŸì‚¬ ë™í–¥ ë¦¬í¬íŠ¸</h1><p>ì´ {total_changes}ê±´ì˜ ë³€ë™ì‚¬í•­ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.</p><hr>{report_body}</body></html>")
        
        update_index_page(change_stats)
        
        # ìŠ¬ë™ ì•Œë¦¼ ë°œì†¡
        report_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{report_file}"
        summary_txt = f"ì´ {total_changes}ê±´ ë³€ë™ ({', '.join(company_summary)})" if total_changes > 0 else "ë³€ë™ì‚¬í•­ ì—†ìŒ"
        payload = {"text": f"ğŸ“¢ *[ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§]* {summary_txt}\nğŸ‘‰ <{report_url}|ìƒì„¸ ë¦¬í¬íŠ¸ í™•ì¸í•˜ê¸°>"}
        send_slack_alert(SLACK_WEBHOOK_URL, payload)
        print("âœ… ì‹œìŠ¤í…œ ì •ìƒ ì¢…ë£Œ")

    except Exception as e:
        print(f"ğŸ”¥ Error: {traceback.format_exc()}")
        send_slack_alert(SLACK_WEBHOOK_URL, {"text": f"ğŸš¨ í¬ë¡¤ë§ ì—”ì§„ ì—ëŸ¬: {str(e)}"})

if __name__ == "__main__": main()
