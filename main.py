"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ (V22)
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-01-30 (ì „ìˆ˜ì¡°ì‚¬ ëª¨ë“œ / í—¬ë¡œëª¨ë°”ì¼ & 7ëª¨ë°”ì¼ JS í•´ë… / ì•ˆì „ì¥ì¹˜ í•´ì œ)
"""

import os
import json
import time
import glob
import re
from datetime import datetime, timedelta, timezone
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜ ë° ì‹œê°„ (KST)
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"

KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
FILE_TIMESTAMP = NOW.strftime("%Y%m%d_%H%M%S")
DISPLAY_DATE = NOW.strftime("%Y-%m-%d")
DISPLAY_TIME = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup, .close');
            popups.forEach(function(element) { element.remove(); });
        """)
    except:
        pass

def scroll_to_bottom(driver):
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        # ì „ìˆ˜ì¡°ì‚¬ë¥¼ ìœ„í•´ ìŠ¤í¬ë¡¤ ì¶©ë¶„íˆ (5íšŒ)
        for _ in range(5): 
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
    except:
        pass

def clean_html(html_source):
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside']):
        tag.decompose()
    for hidden in soup.find_all(attrs={"style": True}):
        if "display:none" in hidden["style"].replace(" ", "").lower():
            hidden.decompose()
    trash_ids = ['across_adn_container', 'criteo-tags-div', 'kakao-pixel-id', 'facebook-pixel-id']
    for t_id in trash_ids:
        tag = soup.find(id=t_id)
        if tag: tag.decompose()
    body = soup.find('body')
    return body.prettify() if body else "No Content"

def load_previous_data():
    json_files = glob.glob(os.path.join(DATA_DIR, "data_*.json"))
    if not json_files: return {}
    json_files.sort()
    latest_file = json_files[-1]
    print(f"ğŸ“‚ ì´ì „ ë°ì´í„° ë¡œë“œ: {os.path.basename(latest_file)}")
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return {}

def analyze_content_changes(old_html, new_html):
    soup_old = BeautifulSoup(old_html, 'html.parser')
    soup_new = BeautifulSoup(new_html, 'html.parser')
    summary = []
    if soup_old.get_text().strip() != soup_new.get_text().strip():
        summary.append("âœï¸ ìƒì„¸ë‚´ìš©(í…ìŠ¤íŠ¸) ìˆ˜ì •")
    imgs_old = set([i['src'] for i in soup_old.find_all('img') if i.get('src')])
    imgs_new = set([i['src'] for i in soup_new.find_all('img') if i.get('src')])
    if imgs_old != imgs_new:
        summary.append("ğŸ–¼ï¸ ìƒì„¸ì´ë¯¸ì§€ êµì²´")
    return " / ".join(summary) if summary else "ğŸ¨ ë””ìì¸/ë ˆì´ì•„ì›ƒ ë³€ê²½"

# [í•µì‹¬] V22 ìŠ¤ë§ˆíŠ¸ ì¹´ë“œ ì¶”ì¶œê¸° (JS í•´ë… + ì „ìˆ˜ì¡°ì‚¬)
def extract_cards_smartly(driver, container_selector, site_name):
    cards_data = {} 
    try:
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, container_selector))
        )
        
        items = []
        
        # 1. SKT ë‹¤ì´ë ‰íŠ¸ (ê³ ì •)
        if "SKT ë‹¤ì´ë ‰íŠ¸" in site_name:
            items = container.find_elements(By.TAG_NAME, "li")

        # 2. ìœ ëª¨ë°”ì¼ (ê³ ì •)
        elif "ìœ ëª¨ë°”ì¼" in site_name:
            items = container.find_elements(By.XPATH, ".//li | .//div[contains(@class, 'card')]")
            if not items: items = container.find_elements(By.TAG_NAME, "li")

        # 3. ìŠ¤ì¹´ì´ë¼ì´í”„ (ê³ ì •)
        elif "ìŠ¤ì¹´ì´ë¼ì´í”„" in site_name:
            items = container.find_elements(By.XPATH, "./div")

        # 4. [HELLOMOBILE] JS ID ì¶”ì¶œ
        elif "í—¬ë¡œëª¨ë°”ì¼" in site_name:
            print("    âš¡ í—¬ë¡œëª¨ë°”ì¼: JS ID í•´ë… ëª¨ë“œ")
            # HTML êµ¬ì¡°ìƒ .event-list ì•ˆì— lië“¤ì´ ìˆìŒ
            try:
                list_ul = container.find_element(By.CSS_SELECTOR, ".event-list")
                items = list_ul.find_elements(By.TAG_NAME, "li")
            except:
                items = container.find_elements(By.TAG_NAME, "li")

        # 5. [SK 7MOBILE] JS ID ì¶”ì¶œ
        elif "SK 7ì„¸ë¸ëª¨ë°”ì¼" in site_name:
            print("    âš¡ SK 7ëª¨ë°”ì¼: JS ID í•´ë… ëª¨ë“œ")
            # HTML êµ¬ì¡°ìƒ .event-group ì•ˆì— lië“¤ì´ ìˆìŒ
            try:
                # event-groupì´ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª¨ë‘ ì°¾ìŒ
                groups = container.find_elements(By.CSS_SELECTOR, ".event-group")
                for g in groups:
                    items.extend(g.find_elements(By.TAG_NAME, "li"))
            except:
                items = container.find_elements(By.TAG_NAME, "li")

        # 6. KTM / ê·¸ ì™¸: ì „ìˆ˜ì¡°ì‚¬
        else:
            print(f"    âš¡ {site_name}: ë§í¬ ì „ìˆ˜ì¡°ì‚¬")
            items = container.find_elements(By.TAG_NAME, "li")
            if not items: items = container.find_elements(By.TAG_NAME, "a")

        if not items:
            print("    âš ï¸ ì•„ì´í…œ ì—†ìŒ -> a íƒœê·¸ ê°•ì œ ìˆ˜ì§‘")
            items = container.find_elements(By.TAG_NAME, "a")

        print(f"    found {len(items)} items in {site_name}")

        for item in items:
            try:
                link_el = item if item.tag_name == 'a' else None
                if not link_el:
                    try: link_el = item.find_element(By.TAG_NAME, "a")
                    except: continue
                
                href = link_el.get_attribute('href')
                onclick = link_el.get_attribute('onclick')
                
                final_url = ""

                # [í•´ë… 1] í—¬ë¡œëª¨ë°”ì¼: fncEventDetail(753, ...)
                if "í—¬ë¡œëª¨ë°”ì¼" in site_name and onclick:
                    match = re.search(r"fncEventDetail\((\d+)", onclick)
                    if match:
                        event_id = match.group(1)
                        final_url = f"https://direct.lghellovision.net/event/viewEventDetail.do?idxOfEvent={event_id}"
                
                # [í•´ë… 2] SK 7ëª¨ë°”ì¼: fnSearchView('code', ...)
                elif "SK 7ì„¸ë¸ëª¨ë°”ì¼" in site_name and onclick:
                    match = re.search(r"fnSearchView\('([^']+)'", onclick)
                    if match:
                        content_id = match.group(1)
                        final_url = f"https://www.sk7mobile.com/bnef/event/eventIngView.do?cntId={content_id}"
                
                # [ì¼ë°˜] href ì‚¬ìš©
                elif href and "javascript" not in href:
                    final_url = href
                
                # [Fallback] JS ë§í¬ì§€ë§Œ ì¼ë‹¨ ê°€ì ¸ì˜´ (ìœ ë‹ˆí¬ í‚¤ ìš©ë„)
                elif href:
                    final_url = href

                if not final_url: continue

                # ì œëª© ì¶”ì¶œ
                title = item.text.strip().split("\n")[0]
                if not title:
                    try: title = item.find_element(By.TAG_NAME, "img").get_attribute("alt")
                    except: title = "ì œëª© ì—†ìŒ"
                
                # ì´ë¯¸ì§€ ì¶”ì¶œ
                img_src = ""
                try:
                    img = item.find_element(By.TAG_NAME, "img")
                    src = img.get_attribute("src")
                    if src and "icon" not in src and "logo" not in src: img_src = src
                except: pass

                cards_data[final_url] = {"title": title, "img": img_src}
            except: continue
            
        return cards_data
    except Exception as e:
        print(f"    âš ï¸ ì¹´ë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return {}

def extract_single_page_content(driver, selector):
    print("    ğŸ“¸ ë‹¨ì¼ í˜ì´ì§€ ìŠ¤ëƒ…ìƒ· (SKT Air)")
    try:
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
        )
        html_content = clean_html(container.get_attribute('outerHTML'))
        return {driver.current_url: {"title": "SKT Air ë©”ì¸ í”„ë¡œëª¨ì…˜", "img": "", "content": html_content}}
    except: return {}

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    collected_items = {} 
    
    if site_name == "SKT Air":
        driver.get(base_url)
        time.sleep(3)
        remove_popups(driver)
        return extract_single_page_content(driver, target_selector)

    # í˜ì´ì§€ë„¤ì´ì…˜ ì—†ì´ 1í˜ì´ì§€ë§Œ ì „ìˆ˜ì¡°ì‚¬ (ì¼ë°˜ì  ìƒí™©)
    # í•„ìš”í•˜ë©´ while ë£¨í”„ ì‚´ë¦´ ìˆ˜ ìˆì§€ë§Œ, í˜„ì¬ ì´ìŠˆ í•´ê²°ì´ ìš°ì„ ì´ë¼ 1í˜ì´ì§€ ì§‘ì¤‘
    target_url = base_url
    if pagination_param == "#": target_url = f"{base_url}#1"
        
    try:
        driver.get(target_url)
        time.sleep(3)
        remove_popups(driver)
        scroll_to_bottom(driver)
        
        # ì¹´ë“œ ìˆ˜ì§‘
        page_data = extract_cards_smartly(driver, target_selector, site_name)
        
        # ì ˆëŒ€ê²½ë¡œ ë³€í™˜
        for href, info in page_data.items():
            if href.startswith('/'):
                from urllib.parse import urljoin
                href = urljoin(base_url, href)
            collected_items[href] = info

    except Exception as e:
        print(f"  âš ï¸ ì˜¤ë¥˜: {e}")

    # [ì „ìˆ˜ì¡°ì‚¬] ëª¨ë“  ìƒì„¸ í˜ì´ì§€ ì ‘ì† (ì‹œê°„ ê±¸ë¦¼)
    print(f"  ğŸ” ìƒì„¸ ë¶„ì„ ì¤‘ ({len(collected_items)}ê±´) - ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    
    for url, info in collected_items.items():
        try:
            # í•´ë…ëœ URLì€ ì ‘ì† ê°€ëŠ¥
            if "javascript" not in url:
                driver.get(url)
                time.sleep(1) # ì•ˆì •ì„±ì„ ìœ„í•´ 1ì´ˆ ëŒ€ê¸°
                remove_popups(driver)
                collected_items[url]['content'] = clean_html(driver.page_source)
            else:
                collected_items[url]['content'] = "JS Link (No Content)"
        except:
            collected_items[url]['content'] = "" 
            
    return collected_items

def update_index_page():
    report_files = glob.glob(os.path.join(REPORT_DIR, "report_*.html"))
    report_files.sort(reverse=True)
    
    index_html = f"""
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§</title>
        <style>
            body {{ font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background: #f9f9f9; }}
            h1 {{ border-bottom: 2px solid #0056b3; padding-bottom: 10px; }}
            .card {{ background: white; padding: 15px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }}
            a {{ text-decoration: none; color: #0056b3; font-weight: bold; }}
            .badge {{ background: #28a745; color: white; padding: 3px 8px; border-radius: 10px; font-size: 0.8em; }}
        </style>
    </head>
    <body>
        <h1>ğŸ“Š ëª¨ë‹ˆí„°ë§ ì•„ì¹´ì´ë¸Œ</h1>
        <p>í˜„ì¬ ì‹œê°: {DISPLAY_DATE} {DISPLAY_TIME} (KST)</p>
    """
    if not report_files: index_html += "<p>ë°ì´í„° ì—†ìŒ</p>"
    for f in report_files:
        name = os.path.basename(f)
        ts = name.replace("report_", "").replace(".html", "")
        try:
            dt = datetime.strptime(ts, "%Y%m%d_%H%M%S")
            disp = dt.strftime("%Y-%m-%d %H:%M:%S")
        except: disp = ts
        badge = '<span class="badge">NEW</span>' if disp.startswith(DISPLAY_DATE) else ''
        index_html += f"<div class='card'><a href='reports/{name}'>ğŸ“„ {disp} ë¦¬í¬íŠ¸</a> {badge}</div>"
    index_html += "</body></html>"
    with open(os.path.join(DOCS_DIR, "index.html"), "w", encoding="utf-8") as f:
        f.write(index_html)

def main():
    driver = setup_driver()
    
    competitors = [
        # [ê³ ì •] ì„±ê³µí•œ 4ê°œ
        {"name": "SKT ë‹¤ì´ë ‰íŠ¸", "url": "https://shop.tworld.co.kr/exhibition/submain", "param": None, "selector": "#wrap > div.container > div > div.event-list-wrap > div > ul"},
        {"name": "SKT Air", "url": "https://sktair-event.com/", "param": None, "selector": "#app > div > section.content"},
        {"name": "U+ ìœ ëª¨ë°”ì¼", "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", "param": None, "selector": "#wrap > main > div > section"},
        {"name": "ìŠ¤ì¹´ì´ë¼ì´í”„", "url": "https://www.skylife.co.kr/event?category=mobile", "param": "p", "selector": "body > div.pb-50.min-w-\[1248px\] > div.m-auto.max-w-\[1248px\].pt-20 > div > div > div.pt-14 > div > div.grid.grid-cols-3.gap-6.pt-4"},
        
        # [JS í•´ë… + ìš¸íƒ€ë¦¬ íƒ€ê²ŸíŒ…]
        {"name": "í—¬ë¡œëª¨ë°”ì¼", "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli", "param": "#", "selector": ".event-list-wrap"},
        {"name": "SK 7ì„¸ë¸ëª¨ë°”ì¼", "url": "https://www.sk7mobile.com/bnef/event/eventIngList.do", "param": None, "selector": ".tb-list.bbs-card"},
        
        # [KTM: ë¦¬ìŠ¤íŠ¸ ì˜ì—­ ê³ ì •]
        {"name": "KTM ëª¨ë°”ì¼", "url": "https://www.ktmmobile.com/event/eventBoardList.do", "param": None, "selector": "#listArea1"}
    ]
    
    today_results = {}
    for comp in competitors:
        try:
            today_results[comp['name']] = crawl_site_logic(driver, comp['name'], comp['url'], comp['param'], comp['selector'])
        except Exception as e:
            print(f"âŒ {comp['name']} ì‹¤íŒ¨: {e}")
    
    driver.quit()
    
    yesterday_results = load_previous_data()
            
    report_body = ""
    total_change_count = 0
    company_summary = []
    
    for name, pages in today_results.items():
        site_changes = ""
        site_change_count = 0 
        old_pages = yesterday_results.get(name, {})
        all_urls = set(pages.keys()) | set(old_pages.keys())
        
        for url in all_urls:
            is_changed = False
            change_type = ""
            reason = ""
            
            curr = pages.get(url, {"title": "?", "img": "", "content": ""})
            prev = old_pages.get(url, {"title": "?", "img": "", "content": ""})
            if isinstance(prev, str): prev = {"title": "Old", "img": "", "content": prev}

            if url in pages and url not in old_pages:
                is_changed = True
                change_type = "NEW"
                reason = "ì‹ ê·œ ì´ë²¤íŠ¸ ë“±ë¡"
            elif url not in pages and url in old_pages:
                is_changed = True
                change_type = "DELETED"
                reason = "ì´ë²¤íŠ¸ ì¢…ë£Œ/ì‚­ì œ"
            else:
                if curr['title'] != prev['title']:
                    is_changed = True
                    change_type = "UPDATED"
                    reason = f"ì œëª© ë³€ê²½: {prev['title']} -> {curr['title']}"
                elif curr['img'] != prev['img']:
                    is_changed = True
                    change_type = "UPDATED"
                    reason = "ì¸ë„¤ì¼/ë°°ë„ˆ ì´ë¯¸ì§€ ë³€ê²½"
                elif curr['content'].replace(" ","") != prev['content'].replace(" ",""):
                    is_changed = True
                    change_type = "UPDATED"
                    reason = analyze_content_changes(prev['content'], curr['content'])

            if is_changed:
                color = "green" if change_type == "NEW" else "red" if change_type == "DELETED" else "orange"
                img_html = f"<img src='{curr['img']}' style='height:50px; vertical-align:middle; margin-right:10px;'>" if curr['img'] else ""
                site_changes += f"""
                <div style="border-left: 5px solid {color}; padding: 10px; margin-bottom: 10px; background: #fff; box-shadow: 0 1px 2px rgba(0,0,0,0.1);">
                    <h3 style="margin: 0 0 5px 0;">
                        <span style="color:{color}; font-weight:bold;">[{change_type}]</span> {curr['title']}
