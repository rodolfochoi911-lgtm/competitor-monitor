"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ (V58)
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-02-03 (ëŒ€ì‹œë³´ë“œ ê³ ë„í™”: 'ì˜¤ëŠ˜ì˜ ë³€ë™(ìŠ¤íƒë°”)' + '7ì¼ê°„ ì¶”ì´(ë¼ì¸ì°¨íŠ¸)' ì‹œê°í™” ì ìš©)
"""

import os
import json
import time
import glob
import random
import re
import traceback
import html
import difflib
import requests
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"
SIMILARITY_THRESHOLD = 0.8

KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
FILE_TIMESTAMP = NOW.strftime("%Y%m%d_%H%M%S")
DISPLAY_DATE = NOW.strftime("%Y-%m-%d")
DISPLAY_TIME = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    print("ğŸš— [V58] ë“œë¼ì´ë²„ ì„¤ì • (ë²„ì „ 144)...")
    options = uc.ChromeOptions()
    options.add_argument("--headless=new") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ko_KR")
    driver = uc.Chrome(options=options, version_main=144)
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup, .close');
            popups.forEach(function(element) { element.remove(); });
        """)
    except: pass

def scroll_to_bottom(driver):
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(3): 
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(1.0, 2.0))
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height: break
            last_height = new_height
    except: pass

def clean_html(html_source):
    if not html_source: return ""
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside', 'link', 'form']):
        tag.decompose()
    return soup.body.prettify() if soup.body else soup.prettify()

def get_clean_text(html_content):
    if not html_content: return ""
    return BeautifulSoup(html_content, "html.parser").get_text(separator=" ", strip=True)

def load_previous_data():
    json_files = glob.glob(os.path.join(DATA_DIR, "data_*.json"))
    if not json_files: return {}
    json_files.sort()
    latest_file = json_files[-1]
    print(f"ğŸ“‚ ì–´ì œ ë°ì´í„° ë¡œë“œ: {latest_file}")
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return {}

def calculate_similarity(text1, text2):
    if not text1 or not text2: return 0.0
    return difflib.SequenceMatcher(None, text1, text2).ratio()

# Side-by-Side Diff ìƒì„±
def check_update_same_url(prev, curr):
    reasons = []
    diff_html = ""
    
    if prev.get('title', '').strip() != curr.get('title', '').strip():
        reasons.append("ì œëª© ë³€ê²½")
    
    prev_txt = get_clean_text(prev.get('content', ''))
    curr_txt = get_clean_text(curr.get('content', ''))
    
    if prev_txt and curr_txt:
        sim = calculate_similarity(prev_txt, curr_txt)
        if sim < 1.0: 
            reasons.append("ë³¸ë¬¸ ìˆ˜ì •")
            diff_html = f"""
            <div style="display:flex; gap:10px; margin-top:10px;">
                <div style="flex:1; background:#ffeef0; padding:10px; border-radius:5px;">
                    <strong style="color:red;">[ì´ì „]</strong>
                    <div style="font-size:13px; line-height:1.4; max-height:200px; overflow-y:auto;">{html.escape(prev_txt[:500])}...</div>
                </div>
                <div style="flex:1; background:#e6fffa; padding:10px; border-radius:5px;">
                    <strong style="color:green;">[í˜„ì¬]</strong>
                    <div style="font-size:13px; line-height:1.4; max-height:200px; overflow-y:auto;">{html.escape(curr_txt[:500])}...</div>
                </div>
            </div>
            """

    if prev.get('img', '').strip() != curr.get('img', '').strip():
        reasons.append("ì¸ë„¤ì¼ ë³€ê²½")
            
    if reasons: 
        return {"msg": f"{', '.join(reasons)}", "html": diff_html}
    return None

# =========================================================
# [Deep Crawler] ìƒì„¸ ìˆ˜ì§‘
# =========================================================
def extract_deep_events(driver, site_name, keyword_list, onclick_pattern=None, base_url=""):
    collected_data = {}
    try:
        time.sleep(4)
        scroll_to_bottom(driver)
        
        if "ì ‘ì†ì´ ì›í™œí•˜ì§€" in driver.page_source or "Access Denied" in driver.page_source:
            print(f"    ğŸš¨ [{site_name}] ì°¨ë‹¨ë¨.")
            return {}

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        all_links = soup.find_all('a')
        target_urls = set()
        exclude_keywords = ["winner", "notice", "dangcheom", "end", "fin", "past", "ë‹¹ì²¨", "ì¢…ë£Œ", "ë°œí‘œ", "ê³µì§€"]

        for link in all_links:
            href = link.get('href', '')
            onclick = link.get('onclick', '')
            final_url = ""

            if href and "javascript" not in href and "#" != href:
                for key in keyword_list:
                    if key in href:
                        final_url = urljoin(base_url, href)
                        break
            elif onclick and onclick_pattern:
                match = re.search(onclick_pattern, onclick)
                if match:
                    if site_name == "í—¬ë¡œëª¨ë°”ì¼":
                        final_url = f"https://direct.lghellovision.net/event/viewEventDetail.do?idxOfEvent={match.group(1)}"
                    elif site_name == "SK 7ì„¸ë¸ëª¨ë°”ì¼":
                        final_url = f"https://www.sk7mobile.com/bnef/event/eventIngView.do?cntId={match.group(1)}"

            if site_name == "KTM ëª¨ë°”ì¼" and not final_url:
                seq = link.get('ntcartseq')
                if seq: final_url = f"https://www.ktmmobile.com/event/eventDetail.do?ntcartSeq={seq}"

            if final_url:
                if any(x in final_url for x in ["login", "my", "faq", "support", "logout"]): continue
                if any(ex in final_url.lower() for ex in exclude_keywords): continue
                if site_name == "U+ ìœ ëª¨ë°”ì¼" and "ì¢…ë£Œ" in link.get_text(): continue
                target_urls.add(final_url)
        
        print(f"    [{site_name}] ìƒì„¸ URL: {len(target_urls)}ê°œ -> ìˆ˜ì§‘ ì‹œì‘")

        count = 0
        for url in target_urls:
            try:
                driver.get(url)
                time.sleep(random.uniform(1.5, 3.0))
                
                if "404" in driver.title or "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜" in driver.page_source: continue

                content_html = clean_html(driver.page_source)
                page_title = ""
                
                try: page_title = driver.find_element(By.TAG_NAME, "h1").text.strip()
                except: pass
                
                if not page_title or site_name in page_title:
                    try: page_title = driver.find_element(By.CSS_SELECTOR, ".view-tit, .event-view-title, .board-view-title, h2").text.strip()
                    except: pass
                if not page_title: page_title = driver.title.strip()
                
                img_src = ""
                try:
                    meta_img = driver.find_element(By.CSS_SELECTOR, "meta[property='og:image']")
                    img_src = meta_img.get_attribute("content")
                except:
                    try:
                        imgs = driver.find_elements(By.CSS_SELECTOR, "div.content img, div.view_content img")
                        for i in imgs:
                            if i.size['width'] > 200:
                                img_src = i.get_attribute("src")
                                break
                    except: pass

                if "ë‹¹ì²¨" in page_title or "ë°œí‘œ" in page_title: continue

                collected_data[url] = {
                    "title": page_title,
                    "img": img_src,
                    "content": content_html[:15000] 
                }
                count += 1
                if count >= 60: break
            except: continue
    except: pass
    return collected_data

def extract_single_page_content(driver, selector):
    try:
        container = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
        return {driver.current_url: {"title": "SKT Air ë©”ì¸", "img": "", "content": clean_html(container.get_attribute('outerHTML'))}}
    except: return {}

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] ì‹œì‘...")
    if site_name == "SKT Air":
        driver.get(base_url); time.sleep(3)
        return extract_single_page_content(driver, target_selector)
    
    keywords = []
    onclick = None
    base = ""
    if site_name == "U+ ìœ ëª¨ë°”ì¼": keywords = ["event", "benefit"]; base = "https://www.uplusumobile.com"
    elif site_name == "KTM ëª¨ë°”ì¼": keywords = ["eventDetail"]; base = "https://www.ktmmobile.com"
    elif site_name == "ìŠ¤ì¹´ì´ë¼ì´í”„": keywords = ["/event/"]; base = "https://www.skylife.co.kr"
    elif site_name == "í—¬ë¡œëª¨ë°”ì¼": keywords = ["event"]; onclick = r"(\d+)"; base = "https://direct.lghellovision.net"
    elif site_name == "SK 7ì„¸ë¸ëª¨ë°”ì¼": keywords = ["event"]; onclick = r"['\"]([^'\"]+)['\"]"; base = "https://www.sk7mobile.com"
    elif site_name == "SKT ë‹¤ì´ë ‰íŠ¸": keywords = ["event", "plan"]; base = "https://shop.tworld.co.kr"
    
    collected_items = {}
    page = 1
    max_page = 5 if site_name != "SKT ë‹¤ì´ë ‰íŠ¸" else 10
    
    while True:
        if pagination_param:
            if pagination_param == "#": target_url = f"{base_url}#{page}"
            else:
                separator = "&" if "?" in base_url else "?"
                target_url = f"{base_url}{separator}{pagination_param}={page}"
        else: target_url = base_url

        driver.get(target_url)
        if pagination_param == "#": driver.refresh(); time.sleep(2)
        time.sleep(3)
        remove_popups(driver)
        
        page_data = extract_deep_events(driver, site_name, keywords, onclick, base)
        
        new_cnt = 0
        for href, info in page_data.items():
            if href not in collected_items:
                collected_items[href] = info
                new_cnt += 1
        
        if new_cnt == 0: break
        if not pagination_param: break
        page += 1
        if page > max_page: break

    return collected_items

# =========================================================
# [ëŒ€ì‹œë³´ë“œ] ì°¨íŠ¸ ì‹œê°í™” (ë³€ë™ í˜„í™© + ì‹œê³„ì—´ ì¶”ì´)
# =========================================================
def update_index_page(change_stats):
    print("ğŸ“Š ëŒ€ì‹œë³´ë“œ(ì°¨íŠ¸) ì—…ë°ì´íŠ¸ ì¤‘...")
    report_files = glob.glob(os.path.join(REPORT_DIR, "report_*.html"))
    report_files.sort(reverse=True)
    
    # 1. ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„: ì˜¤ëŠ˜ ë³€ë™ í˜„í™© (Stacked Bar)
    labels = list(change_stats.keys())
    new_counts = [v['new'] for v in change_stats.values()]
    updated_counts = [v['updated'] for v in change_stats.values()]
    deleted_counts = [v['deleted'] for v in change_stats.values()]

    # 2. ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„: 7ì¼ê°„ ì¶”ì´ (Line Chart)
    # ê³¼ê±° 7ì¼ ë°ì´í„° ë¡œë“œ
    history_files = sorted(glob.glob(os.path.join(DATA_DIR, "data_*.json")))[-7:]
    history_dates = []
    history_series = {name: [] for name in labels}
    
    for h_file in history_files:
        try:
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (data_20240203_120000.json -> 02/03)
            date_str = os.path.basename(h_file).split('_')[1]
            formatted_date = f"{date_str[4:6]}/{date_str[6:8]}"
            history_dates.append(formatted_date)
            
            with open(h_file, 'r', encoding='utf-8') as f:
                d = json.load(f)
                for name in labels:
                    history_series[name].append(len(d.get(name, {})))
        except: pass

    # ì‹œê³„ì—´ ë°ì´í„°ì…‹ ìƒì„± (ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì‚¬ìš©)
    colors = ['#FF6384', '#36A2EB', '#FFCE56', '#4BC0C0', '#9966FF', '#FF9F40', '#E7E9ED']
    line_datasets = []
    for idx, (name, data) in enumerate(history_series.items()):
        line_datasets.append({
            "label": name,
            "data": data,
            "borderColor": colors[idx % len(colors)],
            "fill": False,
            "tension": 0.1
        })

    chart_script = f"""
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script>
        // Chart 1: ì˜¤ëŠ˜ì˜ ë³€ë™ í˜„í™© (Stacked Bar)
        const ctx1 = document.getElementById('changeChart');
        new Chart(ctx1, {{
            type: 'bar',
            data: {{
                labels: {json.dumps(labels)},
                datasets: [
                    {{ label: 'ğŸŸ¢ ì‹ ê·œ', data: {json.dumps(new_counts)}, backgroundColor: 'rgba(75, 192, 192, 0.7)' }},
                    {{ label: 'ğŸŸ  ë³€ê²½', data: {json.dumps(updated_counts)}, backgroundColor: 'rgba(255, 159, 64, 0.7)' }},
                    {{ label: 'ğŸ”´ ì¢…ë£Œ', data: {json.dumps(deleted_counts)}, backgroundColor: 'rgba(255, 99, 132, 0.7)' }}
                ]
            }},
            options: {{
                responsive: true,
                scales: {{ x: {{ stacked: true }}, y: {{ stacked: true }} }},
                plugins: {{ legend: {{ position: 'top' }} }}
            }}
        }});

        // Chart 2: 7ì¼ê°„ ì´ë²¤íŠ¸ ì´ëŸ‰ ì¶”ì´ (Line)
        const ctx2 = document.getElementById('trendChart');
        new Chart(ctx2, {{
            type: 'line',
            data: {{
                labels: {json.dumps(history_dates)},
                datasets: {json.dumps(line_datasets)}
            }},
            options: {{
                responsive: true,
                plugins: {{ legend: {{ position: 'bottom' }} }}
            }}
        }});
    </script>
    """

    index_html = f"""
    <html>
    <head>
        <meta charset="utf-8">
        <title>Competitor Promo Monitor</title>
        <style>
            body {{ font-family: 'Segoe UI', sans-serif; max-width: 1000px; margin: 0 auto; padding: 20px; background-color:#f5f7fa; }}
            .card {{ background:white; padding:25px; border-radius:12px; box-shadow:0 4px 6px rgba(0,0,0,0.05); margin-bottom:25px; }}
            h1 {{ color: #2c3e50; margin-bottom: 5px; }}
            h3 {{ color: #34495e; border-bottom: 2px solid #eee; padding-bottom: 10px; margin-top: 0; }}
            .report-link {{ display: block; padding: 12px; border-bottom: 1px solid #f0f0f0; text-decoration: none; color: #3498db; font-weight:500; transition:0.2s; }}
            .report-link:hover {{ background-color: #f0f8ff; padding-left: 15px; }}
            .grid-container {{ display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }}
            @media (max-width: 768px) {{ .grid-container {{ grid-template-columns: 1fr; }} }}
        </style>
    </head>
    <body>
        <div style="text-align:center; margin-bottom:30px;">
            <h1>ğŸ“± ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ê´€ì œíƒ‘</h1>
            <p style="color:#7f8c8d;">Last Update: {DISPLAY_DATE} {DISPLAY_TIME} (KST)</p>
        </div>
        
        <div class="grid-container">
            <div class="card">
                <h3>ğŸ“Š ì˜¤ëŠ˜ ë³€ë™ í˜„í™© (New/Mod/Del)</h3>
                <canvas id="changeChart"></canvas>
            </div>
            <div class="card">
                <h3>ğŸ“ˆ 7ì¼ê°„ ì´ë²¤íŠ¸ ê·œëª¨ ì¶”ì´</h3>
                <canvas id="trendChart"></canvas>
            </div>
        </div>
        
        <div class="card">
            <h3>ğŸ—‚ï¸ ë¦¬í¬íŠ¸ ì•„ì¹´ì´ë¸Œ</h3>
            <div style="max-height: 400px; overflow-y: auto;">
                {''.join([f"<a href='reports/{os.path.basename(f)}' class='report-link'>{os.path.basename(f)}</a>" for f in report_files])}
            </div>
        </div>
        {chart_script}
    </body>
    </html>
    """
    with open(os.path.join(DOCS_DIR, "index.html"), "w", encoding="utf-8") as f: f.write(index_html)
    with open(os.path.join(DOCS_DIR, ".nojekyll"), "w") as f: f.write("")

def main():
    try:
        driver = setup_driver()
        competitors = [
            {"name": "SKT ë‹¤ì´ë ‰íŠ¸", "url": "https://shop.tworld.co.kr/exhibition/submain", "param": None, "selector": ""},
            {"name": "SKT Air", "url": "https://sktair-event.com/", "param": None, "selector": "#app > div > section.content"},
            {"name": "U+ ìœ ëª¨ë°”ì¼", "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", "param": None, "selector": ""},
            {"name": "KTM ëª¨ë°”ì¼", "url": "https://www.ktmmobile.com/event/eventBoardList.do", "param": None, "selector": ""},
            {"name": "ìŠ¤ì¹´ì´ë¼ì´í”„", "url": "https://www.skylife.co.kr/event?category=mobile", "param": "p", "selector": ""},
            {"name": "í—¬ë¡œëª¨ë°”ì¼", "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli", "param": "#", "selector": ""},
            {"name": "SK 7ì„¸ë¸ëª¨ë°”ì¼", "url": "https://www.sk7mobile.com/bnef/event/eventIngList.do", "param": None, "selector": ""}
        ]
        
        yesterday_results = load_previous_data()
        today_results = {}
        # ëŒ€ì‹œë³´ë“œìš© ë³€ë™ í†µê³„ (ì‹ ê·œ/ìˆ˜ì •/ì‚­ì œ ê±´ìˆ˜)
        change_stats = {comp['name']: {'new': 0, 'updated': 0, 'deleted': 0} for comp in competitors}
        
        for comp in competitors:
            try:
                data = crawl_site_logic(driver, comp['name'], comp['url'], comp['param'], comp['selector'])
                if len(data) == 0: today_results[comp['name']] = yesterday_results.get(comp['name'], {})
                else: today_results[comp['name']] = data
            except: today_results[comp['name']] = yesterday_results.get(comp['name'], {})
        
        driver.quit()
        
        with open(os.path.join(DATA_DIR, f"data_{FILE_TIMESTAMP}.json"), "w", encoding="utf-8") as f:
            json.dump(today_results, f, ensure_ascii=False)
            
        report_body = ""
        total_change_count = 0
        company_summary = []
        
        for name, pages in today_results.items():
            site_change_count = 0 
            old_pages = yesterday_results.get(name, {})
            
            common_urls = set(pages.keys()) & set(old_pages.keys())
            new_candidate_urls = set(pages.keys()) - set(old_pages.keys())
            deleted_candidate_urls = set(old_pages.keys()) - set(pages.keys())
            
            list_new = []
            list_updated = []
            list_deleted = []

            # 1. Update Check
            for url in common_urls:
                diff = check_update_same_url(old_pages[url], pages[url])
                if diff:
                    list_updated.append({"url": url, "reason": diff['msg'], "data": pages[url], "diff_html": diff['html']})

            # 2. Similarity Check
            real_new = []
            real_deleted = list(deleted_candidate_urls)

            for new_url in new_candidate_urls:
                is_moved = False
                new_item = pages[new_url]
                for old_url in list(real_deleted):
                    old_item = old_pages[old_url]
                    total_score = (calculate_similarity(new_item.get('title'), old_item.get('title')) * 0.4) + \
                                  (calculate_similarity(get_clean_text(new_item.get('content')), get_clean_text(old_item.get('content'))) * 0.6)
                    
                    if total_score >= SIMILARITY_THRESHOLD:
                        list_updated.append({"url": new_url, "reason": f"ğŸ”— ë§í¬ ë³€ê²½ (ìœ ì‚¬ë„ {int(total_score*100)}%)", "data": new_item, "diff_html": ""})
                        real_deleted.remove(old_url)
                        is_moved = True
                        break
                if not is_moved: real_new.append(new_url)

            # 3. Finalize Lists
            for url in real_new: list_new.append({"url": url, "data": pages[url]})
            for url in real_deleted: list_deleted.append({"url": url, "data": old_pages[url]})

            # 4. í†µê³„ ì§‘ê³„ (for Chart 1)
            change_stats[name]['new'] = len(list_new)
            change_stats[name]['updated'] = len(list_updated)
            change_stats[name]['deleted'] = len(list_deleted)

            # 5. HTML Generation
            site_html = ""
            if list_new:
                site_html += f"<h3 style='color:green;'>ğŸŸ¢ ì‹ ê·œ ({len(list_new)}ê±´)</h3>"
                for item in list_new:
                    data = item['data']
                    img = f"<img src='{data.get('img')}' style='height:80px; margin-right:15px; border-radius:4px;'>" if data.get('img') else ""
                    site_html += f"<div style='padding:15px; background:#f9fff9; border:1px solid #ccffcc; border-radius:5px; display:flex; margin-bottom:10px;'>{img}<div><b>{data.get('title')}</b><br><a href='{item['url']}' target='_blank'>ğŸ”— ë°”ë¡œê°€ê¸°</a></div></div>"

            if list_updated:
                site_html += f"<h3 style='color:orange;'>ğŸŸ  ë³€ê²½ ({len(list_updated)}ê±´)</h3>"
                for item in list_updated:
                    data = item['data']
                    diff_view = item.get('diff_html', '')
                    site_html += f"<div style='padding:15px; background:#fffcf5; border:1px solid #ffebcc; border-radius:5px; margin-bottom:10px;'><b>{data.get('title')}</b><br><span style='color:#666;'>{item['reason']}</span><br><a href='{item['url']}' target='_blank'>ğŸ”— ë°”ë¡œê°€ê¸°</a>{diff_view}</div>"

            if list_deleted:
                site_html += f"<h3 style='color:red;'>ğŸ”´ ì¢…ë£Œ ({len(list_deleted)}ê±´)</h3>"
                for item in list_deleted:
                    data = item['data']
                    site_html += f"<div style='padding:15px; background:#fff5f5; border:1px solid #ffcccc; border-radius:5px; margin-bottom:10px; opacity:0.7;'><b style='text-decoration:line-through;'>{data.get('title')}</b></div>"

            cnt = len(list_new) + len(list_updated) + len(list_deleted)
            if cnt > 0:
                report_body += f"<div style='margin-bottom:40px;'><h2>{name} ({cnt}ê±´)</h2>{site_html}</div><hr>"
                total_change_count += cnt
                company_summary.append(f"{name}({cnt})")

        # 6. Final Outputs
        summary_text = f"ì´ {total_change_count}ê±´ ë³€ë™ ({', '.join(company_summary)})" if total_change_count > 0 else "íŠ¹ì´ì‚¬í•­ ì—†ìŒ"
        report_header = f"<h1>ğŸ“… {DISPLAY_DATE} ë¦¬í¬íŠ¸</h1><div><h3>ğŸ“Š {summary_text}</h3></div><hr>"
        filename = f"report_{FILE_TIMESTAMP}.html"
        
        with open(os.path.join(REPORT_DIR, filename), "w", encoding="utf-8") as f: f.write(report_header + report_body)
        
        # [V58] ëŒ€ì‹œë³´ë“œ(ì°¨íŠ¸) ì—…ë°ì´íŠ¸ í˜¸ì¶œ (í†µê³„ ì „ë‹¬)
        update_index_page(change_stats)
        
        # ì „ì²´ ëª©ë¡ (ê·¸ë¦¬ë“œ)
        full_list_html = f"<h1>ğŸ“‚ {DISPLAY_DATE} ì „ì²´ ëª©ë¡</h1><hr>"
        for name, pages in today_results.items():
            full_list_html += f"<h3>{name} ({len(pages)}ê°œ)</h3><div style='display:grid; grid-template-columns: repeat(auto-fill, minmax(200px, 1fr)); gap:15px;'>"
            for url, data in pages.items():
                img = f"<img src='{data.get('img','')}' style='width:100%; height:120px; object-fit:cover; border-radius:5px; border:1px solid #eee;'>" if data.get('img') else "<div style='width:100%; height:120px; background:#f0f0f0; display:flex; align-items:center; justify-content:center; border-radius:5px;'>No Image</div>"
                full_list_html += f"<div style='border:1px solid #ddd; padding:10px; border-radius:8px;'><a href='{url}' target='_blank' style='text-decoration:none; color:#333;'>{img}<p style='margin:10px 0 0 0; font-weight:bold; overflow:hidden; text-overflow:ellipsis; white-space:nowrap;'>{data.get('title')}</p></a></div>"
            full_list_html += "</div><hr>"
        
        list_filename = f"list_{FILE_TIMESTAMP}.html"
        with open(os.path.join(REPORT_DIR, list_filename), "w", encoding="utf-8") as f: f.write(full_list_html)

        dashboard_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/"
        report_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{filename}"
        list_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{list_filename}"
        
        payload = {"text": f"ğŸ“¢ *[KST {DISPLAY_TIME}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… *ìš”ì•½:* {summary_text}\n\nğŸ‘‰ *ë³€ê²½ ë¦¬í¬íŠ¸:* {report_url}\nğŸ—‚ï¸ *ì „ì²´ ëª©ë¡:* {list_url}\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ:* {dashboard_url}"}
        if SLACK_WEBHOOK_URL: requests.post(SLACK_WEBHOOK_URL, json=payload)
        print("âœ… ì™„ë£Œ")

    except Exception as e: print(f"ğŸ”¥ Error: {traceback.format_exc()}")

if __name__ == "__main__":
    main()

