"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-01-29 (ìŠ¤ì¹´ì´ë¼ì´í”„ í˜ì´ì§€ íŒŒë¼ë¯¸í„° ìˆ˜ì • 'p' ë°˜ì˜)
"""

import os
import json
import time
import glob
from datetime import datetime
import difflib
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •]
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"

NOW = datetime.now()
TODAY_STR = NOW.strftime("%Y-%m-%d")
TIME_STR = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # ì°½ í¬ê¸°ë¥¼ í‚¤ì›Œì„œ PC ë²„ì „ ë¡œë”© ìœ ë„ (ëª¨ë°”ì¼ ë ˆì´ì•„ì›ƒ ë°©ì§€)
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup');
            popups.forEach(function(element) { element.remove(); });
        """)
    except:
        pass

def scroll_to_bottom(driver):
    """ì§€ì—° ë¡œë”© ì½˜í…ì¸  í™œì„±í™”ë¥¼ ìœ„í•œ ìŠ¤í¬ë¡¤"""
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
    except:
        pass

def clean_html(html_source):
    """HTML ì „ì²˜ë¦¬"""
    soup = BeautifulSoup(html_source, 'html.parser')
    
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside']):
        tag.decompose()

    for hidden in soup.find_all(attrs={"style": True}):
        if "display:none" in hidden["style"].replace(" ", "").lower():
            hidden.decompose()
            
    trash_ids = ['across_adn_container', 'criteo-tags-div', 'kakao-pixel-id', 'facebook-pixel-id']
    for t_id in trash_ids:
        tag = soup.find(id=t_id)
        if tag: tag.decompose()
        
    body = soup.find('body')
    return body.prettify() if body else "No Content"

def analyze_changes(old_html, new_html):
    """ë³€ê²½ ì‚¬í•­ ìš”ì•½"""
    soup_old = BeautifulSoup(old_html, 'html.parser')
    soup_new = BeautifulSoup(new_html, 'html.parser')
    
    summary_tags = []
    
    imgs_old = set([img.get('src') for img in soup_old.find_all('img') if img.get('src')])
    imgs_new = set([img.get('src') for img in soup_new.find_all('img') if img.get('src')])
    if imgs_old != imgs_new:
        summary_tags.append("ğŸ–¼ï¸ <b>ì´ë¯¸ì§€/ë°°ë„ˆ ë³€ê²½</b>")
        
    if soup_old.get_text().strip() != soup_new.get_text().strip():
        summary_tags.append("âœï¸ <b>í…ìŠ¤íŠ¸(ë‚´ìš©) ìˆ˜ì •</b>")
        
    links_old = set([a.get('href') for a in soup_old.find_all('a') if a.get('href')])
    links_new = set([a.get('href') for a in soup_new.find_all('a') if a.get('href')])
    if links_old != links_new:
        summary_tags.append("ğŸ”— <b>ì—°ê²° ë§í¬ ë³€ê²½</b>")

    if not summary_tags and old_html != new_html:
        summary_tags.append("ğŸ¨ <b>ë””ìì¸/ìŠ¤íƒ€ì¼ ë³€ê²½</b>")
        
    if not summary_tags:
        return "ğŸ” ë¯¸ì„¸í•œ ì½”ë“œ ë³€ê²½"
    
    return " / ".join(summary_tags)

def extract_links_safely(driver, base_url, target_selector):
    """ë§í¬ ì¶”ì¶œ (Selector ì‹¤íŒ¨ ì‹œ ì „ì²´ ê²€ìƒ‰ Fallback)"""
    links = []
    method = "Selector"
    
    if target_selector:
        try:
            container = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, target_selector))
            )
            found_links = container.find_elements(By.TAG_NAME, "a")
            if len(found_links) == 0:
                raise Exception("0 links found in selector")
            links = found_links
        except:
            print(f"    âš ï¸ ì„ íƒì({target_selector}) ì‹¤íŒ¨/0ê±´ -> ì „ì²´ í˜ì´ì§€ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜")
            links = driver.find_elements(By.TAG_NAME, "a")
            method = "Fallback (All)"
    else:
        links = driver.find_elements(By.TAG_NAME, "a")
        method = "Full Page"

    current_page_urls = []
    extracted_data = {} 

    for link in links:
        try:
            href = link.get_attribute('href')
            title = link.text.strip()
            if not title:
                img = link.find_element(By.TAG_NAME, "img")
                title = img.get_attribute("alt") if img else "ì œëª© ì—†ìŒ"
            
            if href and ('event' in href or 'view' in href or 'detail' in href or 'notice' in href) and not href.startswith('#') and 'javascript' not in href:
                if href.startswith('/'):
                    from urllib.parse import urljoin
                    href = urljoin(base_url, href)
                
                if href not in current_page_urls:
                    current_page_urls.append(href)
                    if href not in extracted_data:
                        extracted_data[href] = title
        except:
            continue
            
    return extracted_data, method

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    collected_links = {} 
    last_page_links = []
    page = 1
    
    while True:
        # URL ìƒì„±
        if pagination_param:
            connector = '&' if '?' in base_url else '?'
            target_url = f"{base_url}{connector}{pagination_param}={page}"
        else:
            target_url = base_url
            
        try:
            driver.get(target_url)
            time.sleep(5)
            remove_popups(driver)
            scroll_to_bottom(driver)
            
            page_data, method = extract_links_safely(driver, base_url, target_selector)
            
            if page == 1:
                print(f"  - [{method}] Page {page}: {len(page_data)}ê°œ ë°œê²¬")
            
            if not page_data: break
            
            current_urls = sorted(list(page_data.keys()))
            if current_urls == sorted(last_page_links): 
                break
            
            collected_links.update(page_data)
            
            if not pagination_param: break
            
            last_page_links = current_urls
            page += 1
            if page > 10: break

        except Exception as e:
            print(f"  âš ï¸ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            break

    print(f"  ğŸ” ìƒì„¸ ë¶„ì„ ì¤‘ ({len(collected_links)}ê±´)...")
    site_data = {}
    
    for link, title in collected_links.items():
        try:
            driver.get(link)
            time.sleep(1)
            remove_popups(driver)
            site_data[link] = {
                "title": title if title else "ì œëª© ì—†ìŒ",
                "content": clean_html(driver.page_source)
            }
        except:
            pass
            
    return site_data

def update_index_page():
    report_files = glob.glob(os.path.join(REPORT_DIR, "report_*.html"))
    report_files.sort(reverse=True)
    
    index_html = f"""
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ</title>
        <style>
            body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background-color: #f9f9f9; }}
            h1 {{ color: #333; border-bottom: 2px solid #0056b3; padding-bottom: 10px; }}
            .card {{ background: white; padding: 15px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); display: flex; justify-content: space-between; align-items: center; }}
            .card a {{ text-decoration: none; color: #0056b3; font-weight: bold; font-size: 1.1em; }}
            .card a:hover {{ text-decoration: underline; }}
            .date {{ color: #666; font-size: 0.9em; }}
            .badge {{ background-color: #28a745; color: white; padding: 3px 8px; border-radius: 12px; font-size: 0.8em; }}
            .sub-link {{ font-size: 0.8em; color: #888; margin-left: 10px; }}
        </style>
    </head>
    <body>
        <h1>ğŸ“Š ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ</h1>
        <p>ìµœì¢… ì—…ë°ì´íŠ¸: {TODAY_STR} {TIME_STR}</p>
        <div class="list-container">
    """
    
    if not report_files:
        index_html += "<p>ì•„ì§ ìƒì„±ëœ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
    
    for file_path in report_files:
        filename = os.path.basename(file_path)
        date_str = filename.replace("report_", "").replace(".html", "")
        list_filename = f"list_{date_str}.html"
        badge = '<span class="badge">NEW</span>' if date_str == TODAY_STR else ''
        
        index_html += f"""
            <div class="card">
                <div>
                    <a href="reports/{filename}">ğŸ“„ {date_str} ë¦¬í¬íŠ¸ (ë³€ê²½ì‚¬í•­)</a>
                    <a href="reports/{list_filename}" class="sub-link" target="_blank">ğŸ—‚ï¸ ì „ì²´ ìˆ˜ì§‘ ëª©ë¡</a>
                    {badge}
                </div>
                <span class="date">{date_str}</span>
            </div>
        """
        
    index_html += "</div></body></html>"
    
    with open(os.path.join(DOCS_DIR, "index.html"), "w", encoding="utf-8") as f:
        f.write(index_html)
    print("âœ… [ëŒ€ì‹œë³´ë“œ] index.html ì—…ë°ì´íŠ¸ ì™„ë£Œ")

def main():
    driver = setup_driver()
    
    # [ì„¤ì •] ìŠ¤ì¹´ì´ë¼ì´í”„ íŒŒë¼ë¯¸í„° 'p'ë¡œ ìˆ˜ì • ì™„ë£Œ
    competitors = [
        {"name": "SKT ë‹¤ì´ë ‰íŠ¸", "url": "https://shop.tworld.co.kr/exhibition/submain", "param": None, "selector": "#wrap > div.container > div > div.event-list-wrap > div > ul"},
        {"name": "SKT Air", "url": "https://sktair-event.com/", "param": None, "selector": "#app > div > section.content"},
        {"name": "KTM ëª¨ë°”ì¼", "url": "https://www.ktmmobile.com/event/eventBoardList.do", "param": None, "selector": "#listArea1"},
        {"name": "U+ ìœ ëª¨ë°”ì¼", "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", "param": None, "selector": "#wrap > main > div > section"},
        {"name": "í—¬ë¡œëª¨ë°”ì¼", "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli&category=USIM", "param": "pageIndex", "selector": "#contentWrap > div.event-list-wrap > section > div.list-wrap > ul"},
        {"name": "ìŠ¤ì¹´ì´ë¼ì´í”„", "url": "https://www.skylife.co.kr/event?category=mobile", "param": "p", "selector": "body > div.pb-50.min-w-\[1248px\] > div.m-auto.max-w-\[1248px\].pt-20 > div > div > div.pt-14 > div > div.grid.grid-cols-3.gap-6.pt-4"}
    ]
    
    today_results = {}
    for comp in competitors:
        try:
            today_results[comp['name']] = crawl_site_logic(driver, comp['name'], comp['url'], comp['param'], comp['selector'])
        except Exception as e:
            print(f"âŒ {comp['name']} ì‹¤íŒ¨: {e}")
    
    driver.quit()
    
    latest_file = os.path.join(DATA_DIR, "latest_data.json")
    yesterday_results = {}
    if os.path.exists(latest_file):
        with open(latest_file, "r", encoding="utf-8") as f:
            yesterday_results = json.load(f)
            
    report_body = ""
    total_change_count = 0
    company_summary = []
    
    for name, pages in today_results.items():
        site_changes = ""
        site_change_count = 0 
        old_pages = yesterday_results.get(name, {})
        
        all_urls = set(pages.keys()) | set(old_pages.keys())
        
        for url in all_urls:
            is_changed = False
            change_html = ""
            
            curr_data = pages.get(url, {"title": "Unknown", "content": ""})
            prev_data = old_pages.get(url, {"title": "Unknown", "content": ""})
            
            if isinstance(prev_data, str): prev_data = {"title": "Old Data", "content": prev_data}
            if isinstance(curr_data, str): curr_data = {"title": "New Data", "content": curr_data}

            title_display = curr_data['title'] if url in pages else prev_data['title']

            if url in pages and url not in old_pages:
                is_changed = True
                change_html = f"<h3 style='color:green'>[NEW] {title_display} <a href='{url}' target='_blank' style='font-size:0.7em'>ğŸ”—ë§í¬</a></h3>"
            
            elif url not in pages and url in old_pages:
                is_changed = True
                change_html = f"<h3 style='color:red'>[DELETED] {title_display} <a href='{url}' target='_blank' style='font-size:0.7em'>ğŸ”—ë§í¬</a></h3>"
            
            elif curr_data['content'].replace(" ","") != prev_data['content'].replace(" ",""):
                is_changed = True
                change_summary = analyze_changes(prev_data['content'], curr_data['content'])
                diff = difflib.HtmlDiff().make_table(prev_data['content'].splitlines(), curr_data['content'].splitlines(), context=True, numlines=3)
                
                change_html = f"""
                <div style="border:1px solid #ddd; padding:10px; border-radius:8px; margin-bottom:10px; background-color:#fff;">
                    <h3 style='margin:0 0 5px 0; color:orange;'>[UPDATED] {title_display}</h3>
                    <div style="font-size:0.9em; color:#666; margin-bottom:10px;">
                        {change_summary} <a href='{url}' target='_blank' style='text-decoration:none;'>ğŸ”— ë°”ë¡œê°€ê¸°</a>
                    </div>
                    <details>
                        <summary style="cursor:pointer; color:#0056b3; font-weight:bold; padding:8px; background:#f8f9fa; border-radius:5px;">ğŸ‘‰ ë³€ê²½ëœ ì½”ë“œ ìƒì„¸ ë³´ê¸° (í´ë¦­)</summary>
                        <div style="margin-top:10px; overflow-x:auto; font-size:0.85em;">{diff}</div>
                    </details>
                </div>
                """
            
            if is_changed:
                site_changes += change_html
                site_change_count += 1
        
        if site_changes:
            report_body += f"<h2>{name} ({site_change_count}ê±´)</h2>{site_changes}<hr>"
            total_change_count += site_change_count
            company_summary.append(f"{name}({site_change_count})")

    full_list_html = f"<h1>ğŸ“‚ {TODAY_STR} ì „ì²´ ìˆ˜ì§‘ ëª©ë¡ ({TIME_STR} ê¸°ì¤€)</h1><hr>"
    for name, pages in today_results.items():
        full_list_html += f"<h3>{name} (ì´ {len(pages)}ê°œ)</h3><ul>"
        for url, data in pages.items():
            full_list_html += f"<li><a href='{url}' target='_blank'>{data['title']}</a></li>"
        full_list_html += "</ul><hr>"
    
    list_filename = f"list_{TODAY_STR}.html"
    with open(os.path.join(REPORT_DIR, list_filename), "w", encoding="utf-8") as f:
        f.write(full_list_html)

    summary_text = f"ì´ {total_change_count}ê±´ ì—…ë°ì´íŠ¸ ({', '.join(company_summary)})" if total_change_count > 0 else "íŠ¹ì´ì‚¬í•­ ì—†ìŒ"
    
    report_header = f"""
    <h1>ğŸ“… {TODAY_STR} ë¦¬í¬íŠ¸ <span style="font-size:0.6em; color:#888;">({TIME_STR} ê¸°ì¤€)</span></h1>
    <div style='background-color:#f4f4f4; padding:15px; border-radius:10px; border:1px solid #ddd;'>
        <h3>ğŸ“Š {summary_text}</h3>
        <p>
            <a href="../index.html">ğŸ”™ ëŒ€ì‹œë³´ë“œ</a> | 
            <a href="{list_filename}" target="_blank">ğŸ—‚ï¸ ì „ì²´ ìˆ˜ì§‘ ëª©ë¡ ë³´ê¸°</a>
        </p>
    </div>
    <hr>
    """
    full_report = report_header + (report_body if total_change_count > 0 else "<p>âœ… ê¸ˆì¼ ë³€ë™ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.</p>")
    
    filename = f"report_{TODAY_STR}.html"
    with open(os.path.join(REPORT_DIR, filename), "w", encoding="utf-8") as f:
        f.write(full_report)
        
    with open(latest_file, "w", encoding="utf-8") as f:
        json.dump(today_results, f, ensure_ascii=False)

    update_index_page()

    dashboard_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/"
    report_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{filename}"
    list_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{list_filename}"
    
    if total_change_count > 0:
        payload = {
            "text": f"ğŸ“¢ *[{TODAY_STR} {TIME_STR}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… *ìš”ì•½:* {summary_text}\n\nğŸ‘‰ *ë³€ê²½ ë¦¬í¬íŠ¸:* {report_url}\nğŸ—‚ï¸ *ì „ì²´ ëª©ë¡:* {list_url}\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ (ì•„ì¹´ì´ë¸Œ):* {dashboard_url}"
        }
    else:
        payload = {
            "text": f"ğŸ“‹ *[{TODAY_STR} {TIME_STR}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… íŠ¹ì´ì‚¬í•­ ì—†ìŒ\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ (ì•„ì¹´ì´ë¸Œ):* {dashboard_url}"
        }
        
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json=payload)
        print("âœ… ìŠ¬ë™ ì•Œë¦¼ ì™„ë£Œ")
    else:
        print("âš ï¸ ìŠ¬ë™ URL ì—†ìŒ")

if __name__ == "__main__":
    main()
