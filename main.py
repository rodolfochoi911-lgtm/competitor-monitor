"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-01-29 (SKT Air í¬í•¨ 6ê°œ ì‚¬ì´íŠ¸ ì™„ì „ì²´ + ëª¨ë“  ê¸°ëŠ¥ í†µí•©)
"""

import os
import json
import time
import glob
from datetime import datetime
import difflib
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •]
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"

# ë‚ ì§œ ë° ì‹œê°„ ì„¤ì •
NOW = datetime.now()
TODAY_STR = NOW.strftime("%Y-%m-%d")
TIME_STR = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup');
            popups.forEach(function(element) { element.remove(); });
        """)
    except:
        pass

def clean_html(html_source):
    """HTML ì „ì²˜ë¦¬ (ê´‘ê³  íƒœê·¸ ë° ë…¸ì´ì¦ˆ ì œê±°)"""
    soup = BeautifulSoup(html_source, 'html.parser')
    
    # 1. ë¶ˆí•„ìš” íƒœê·¸ ì œê±°
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside']):
        tag.decompose()

    # 2. ìˆ¨ê²¨ì§„ ìš”ì†Œ(display:none) ì œê±°
    for hidden in soup.find_all(attrs={"style": True}):
        if "display:none" in hidden["style"].replace(" ", "").lower():
            hidden.decompose()
            
    # 3. ê´‘ê³ /ì¶”ì  ID ê°•ì œ ì œê±°
    trash_ids = ['across_adn_container', 'criteo-tags-div', 'kakao-pixel-id', 'facebook-pixel-id']
    for t_id in trash_ids:
        tag = soup.find(id=t_id)
        if tag: tag.decompose()
        
    body = soup.find('body')
    return body.prettify() if body else "No Content"

def analyze_changes(old_html, new_html):
    """ë³€ê²½ ì‚¬í•­ì˜ ì¢…ë¥˜ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ëŒ ë§ë¡œ ìš”ì•½"""
    soup_old = BeautifulSoup(old_html, 'html.parser')
    soup_new = BeautifulSoup(new_html, 'html.parser')
    
    summary_tags = []
    
    # ì´ë¯¸ì§€ ë³€ê²½ ê°ì§€
    imgs_old = set([img.get('src') for img in soup_old.find_all('img') if img.get('src')])
    imgs_new = set([img.get('src') for img in soup_new.find_all('img') if img.get('src')])
    if imgs_old != imgs_new:
        summary_tags.append("ğŸ–¼ï¸ <b>ì´ë¯¸ì§€/ë°°ë„ˆ ë³€ê²½</b>")
        
    # í…ìŠ¤íŠ¸ ë³€ê²½ ê°ì§€
    if soup_old.get_text().strip() != soup_new.get_text().strip():
        summary_tags.append("âœï¸ <b>í…ìŠ¤íŠ¸(ë‚´ìš©) ìˆ˜ì •</b>")
        
    # ë§í¬ ë³€ê²½ ê°ì§€
    links_old = set([a.get('href') for a in soup_old.find_all('a') if a.get('href')])
    links_new = set([a.get('href') for a in soup_new.find_all('a') if a.get('href')])
    if links_old != links_new:
        summary_tags.append("ğŸ”— <b>ì—°ê²° ë§í¬ ë³€ê²½</b>")

    # ë””ìì¸ ë³€ê²½ ê°ì§€
    if not summary_tags and old_html != new_html:
        summary_tags.append("ğŸ¨ <b>ë””ìì¸/ìŠ¤íƒ€ì¼ ë³€ê²½</b>")
        
    if not summary_tags:
        return "ğŸ” ë¯¸ì„¸í•œ ì½”ë“œ ë³€ê²½"
    
    return " / ".join(summary_tags)

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    collected_links = {} 
    last_page_links = []
    page = 1
    
    while True:
        if pagination_param:
            connector = '&' if '?' in base_url else '?'
            target_url = f"{base_url}{connector}{pagination_param}={page}"
        else:
            target_url = base_url
            
        try:
            driver.get(target_url)
            time.sleep(3)
            remove_popups(driver)
            
            links = []
            if target_selector:
                try:
                    container = WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, target_selector))
                    )
                    links = container.find_elements(By.TAG_NAME, "a")
                except:
                    links = driver.find_elements(By.TAG_NAME, "a")
            else:
                links = driver.find_elements(By.TAG_NAME, "a")

            current_page_urls = []
            
            for link in links:
                try:
                    href = link.get_attribute('href')
                    # ì œëª© ì¶”ì¶œ
                    title = link.text.strip()
                    if not title:
                        img = link.find_element(By.TAG_NAME, "img")
                        title = img.get_attribute("alt") if img else "ì œëª© ì—†ìŒ"
                    
                    if href and ('event' in href or 'view' in href or 'detail' in href or 'notice' in href) and not href.startswith('#') and 'javascript' not in href:
                        if href.startswith('/'):
                            from urllib.parse import urljoin
                            href = urljoin(base_url, href)
                        
                        if href not in current_page_urls:
                            current_page_urls.append(href)
                            if href not in collected_links:
                                collected_links[href] = title
                except:
                    continue
            
            if not current_page_urls: break
            if sorted(current_page_urls) == sorted(last_page_urls): break
            
            if not pagination_param: break
            
            last_page_links = current_page_urls
            page += 1
            if page > 10: break

        except Exception as e:
            print(f"  âš ï¸ ì˜¤ë¥˜: {e}")
            break

    print(f"  ğŸ” ìƒì„¸ ë¶„ì„ ì¤‘ ({len(collected_links)}ê±´)...")
    site_data = {}
    
    for link, title in collected_links.items():
        try:
            driver.get(link)
            time.sleep(1)
            remove_popups(driver)
            site_data[link] = {
                "title": title if title else "ì œëª© ì—†ìŒ",
                "content": clean_html(driver.page_source)
            }
        except:
            pass
            
    return site_data

def update_index_page():
    report_files = glob.glob(os.path.join(REPORT_DIR, "report_*.html"))
    report_files.sort(reverse=True)
    
    index_html = f"""
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ê²½ìŸì‚¬ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ</title>
        <style>
            body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background-color: #f9f9f9; }}
            h1 {{ color: #333; border-bottom: 2px solid #0056b3; padding-bottom: 10px; }}
            .card {{ background: white; padding: 15px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); display: flex; justify-content: space-between; align-items: center; }}
            .card a {{ text-decoration: none; color: #0056b3; font-weight: bold; font-size: 1.1em; }}
            .card a:hover {{ text-decoration: underline; }}
            .date {{ color: #666; font-size: 0.9em; }}
            .badge {{ background-color: #28a745; color: white; padding: 3px 8px; border-radius: 12px; font-size: 0.8em; }}
            .sub-link {{ font-size: 0.8em; color: #888; margin-left: 10px; }}
        </style>
    </head>
    <body>
        <h1>ğŸ“Š ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ</h1>
        <p>ìµœì¢… ì—…ë°ì´íŠ¸: {TODAY_STR} {TIME_STR}</p>
        <div class="list-container">
    """
    
    if not report_files:
        index_html += "<p>ì•„ì§ ìƒì„±ëœ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
    
    for file_path in report_files:
        filename = os.path.basename(file_path)
        date_str = filename.replace("report_", "").replace(".html", "")
        list_filename = f"list_{date_str}.html"
        badge = '<span class="badge">NEW</span>' if date_str == TODAY_STR else ''
        
        index_html += f"""
            <div class="card">
                <div>
                    <a href="reports/{filename}">ğŸ“„ {date_str} ë¦¬í¬íŠ¸ (ë³€ê²½ì‚¬í•­)</a>
                    <a href="reports/{list_filename}" class="sub-link" target="_blank">ğŸ—‚ï¸ ì „ì²´ ìˆ˜ì§‘ ëª©ë¡</a>
                    {badge}
                </div>
                <span class="date">{date_str}</span>
            </div>
        """
        
    index_html += "</div></body></html>"
    
    with open(os.path.join(DOCS_DIR, "index.html"), "w", encoding="utf-8") as f:
        f.write(index_html)
    print("âœ… [ëŒ€ì‹œë³´ë“œ] index.html ì—…ë°ì´íŠ¸ ì™„ë£Œ")

def main():
    driver = setup_driver()
    
    # [ì„¤ì •] 6ê°œ ì‚¬ì´íŠ¸ ì „ì²´ ë¦¬ìŠ¤íŠ¸ (SKT Air í¬í•¨)
    competitors = [
        {"name": "SKT ë‹¤ì´ë ‰íŠ¸", "url": "https://shop.tworld.co.kr/exhibition/submain", "param": None, "selector": "#wrap > div.container > div > div.event-list-wrap > div > ul"},
        {"name": "SKT Air", "url": "https://sktair-event.com/", "param": None, "selector": "#app > div > section.content"},
        {"name": "KTM ëª¨ë°”ì¼", "url": "https://www.ktmmobile.com/event/eventBoardList.do", "param": None, "selector": "#listArea1"},
        {"name": "U+ ìœ ëª¨ë°”ì¼", "url": "https://www.uplusumobile.com/event-benefit/event/ongoing", "param": None, "selector": "#wrap > main > div > section"},
        {"name": "í—¬ë¡œëª¨ë°”ì¼", "url": "https://direct.lghellovision.net/event/viewEventList.do?returnTab=allli&category=USIM", "param": "pageIndex", "selector": "#contentWrap > div.event-list-wrap > section > div.list-wrap > ul"},
        {"name": "ìŠ¤ì¹´ì´ë¼ì´í”„", "url": "https://www.skylife.co.kr/event?category=mobile", "param": "page", "selector": "body > div.pb-50.min-w-\[1248px\] > div.m-auto.max-w-\[1248px\].pt-20 > div > div > div.pt-14 > div > div.grid.grid-cols-3.gap-6.pt-4"}
    ]
    
    today_results = {}
    for comp in competitors:
        try:
            today_results[comp['name']] = crawl_site_logic(driver, comp['name'], comp['url'], comp['param'], comp['selector'])
        except Exception as e:
            print(f"âŒ {comp['name']} ì‹¤íŒ¨: {e}")
    
    driver.quit()
    
    latest_file = os.path.join(DATA_DIR, "latest_data.json")
    yesterday_results = {}
    if os.path.exists(latest_file):
        with open(latest_file, "r", encoding="utf-8") as f:
            yesterday_results = json.load(f)
            
    report_body = ""
    total_change_count = 0
    company_summary = []
    
    for name, pages in today_results.items():
        site_changes = ""
        site_change_count = 0 
        old_pages = yesterday_results.get(name, {})
        
        all_urls = set(pages.keys()) | set(old_pages.keys())
        
        for url in all_urls:
            is_changed = False
            change_html = ""
            
            curr_data = pages.get(url, {"title": "Unknown", "content": ""})
            prev_data = old_pages.get(url, {"title": "Unknown", "content": ""})
            
            # êµ¬ë²„ì „ í˜¸í™˜ì„± ì²´í¬
            if isinstance(prev_data, str): prev_data = {"title": "Old Data", "content": prev_data}
            if isinstance(curr_data, str): curr_data = {"title": "New Data", "content": curr_data}

            title_display = curr_data['title'] if url in pages else prev_data['title']

            # Case 1: NEW
            if url in pages and url not in old_pages:
                is_changed = True
                change_html = f"<h3 style='color:green'>[NEW] {title_display} <a href='{url}' target='_blank' style='font-size:0.7em'>ğŸ”—ë§í¬</a></h3>"
            
            # Case 2: DELETED
            elif url not in pages and url in old_pages:
                is_changed = True
                change_html = f"<h3 style='color:red'>[DELETED] {title_display} <a href='{url}' target='_blank' style='font-size:0.7em'>ğŸ”—ë§í¬</a></h3>"
            
            # Case 3: UPDATED
            elif curr_data['content'].replace(" ","") != prev_data['content'].replace(" ",""):
                is_changed = True
                change_summary = analyze_changes(prev_data['content'], curr_data['content'])
                diff = difflib.HtmlDiff().make_table(prev_data['content'].splitlines(), curr_data['content'].splitlines(), context=True, numlines=3)
                
                change_html = f"""
                <div style="border:1px solid #ddd; padding:10px; border-radius:8px; margin-bottom:10px; background-color:#fff;">
                    <h3 style='margin:0 0 5px 0; color:orange;'>[UPDATED] {title_display}</h3>
                    <div style="font-size:0.9em; color:#666; margin-bottom:10px;">
                        {change_summary} <a href='{url}' target='_blank' style='text-decoration:none;'>ğŸ”— ë°”ë¡œê°€ê¸°</a>
                    </div>
                    <details>
                        <summary style="cursor:pointer; color:#0056b3; font-weight:bold; padding:8px; background:#f8f9fa; border-radius:5px;">ğŸ‘‰ ë³€ê²½ëœ ì½”ë“œ ìƒì„¸ ë³´ê¸° (í´ë¦­)</summary>
                        <div style="margin-top:10px; overflow-x:auto; font-size:0.85em;">{diff}</div>
                    </details>
                </div>
                """
            
            if is_changed:
                site_changes += change_html
                site_change_count += 1
        
        if site_changes:
            report_body += f"<h2>{name} ({site_change_count}ê±´)</h2>{site_changes}<hr>"
            total_change_count += site_change_count
            company_summary.append(f"{name}({site_change_count})")

    # ì „ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    full_list_html = f"<h1>ğŸ“‚ {TODAY_STR} ì „ì²´ ìˆ˜ì§‘ ëª©ë¡ ({TIME_STR} ê¸°ì¤€)</h1><hr>"
    for name, pages in today_results.items():
        full_list_html += f"<h3>{name} (ì´ {len(pages)}ê°œ)</h3><ul>"
        for url, data in pages.items():
            full_list_html += f"<li><a href='{url}' target='_blank'>{data['title']}</a></li>"
        full_list_html += "</ul><hr>"
    
    list_filename = f"list_{TODAY_STR}.html"
    with open(os.path.join(REPORT_DIR, list_filename), "w", encoding="utf-8") as f:
        f.write(full_list_html)

    # ë¦¬í¬íŠ¸ ìƒì„±
    summary_text = f"ì´ {total_change_count}ê±´ ì—…ë°ì´íŠ¸ ({', '.join(company_summary)})" if total_change_count > 0 else "íŠ¹ì´ì‚¬í•­ ì—†ìŒ"
    
    report_header = f"""
    <h1>ğŸ“… {TODAY_STR} ë¦¬í¬íŠ¸ <span style="font-size:0.6em; color:#888;">({TIME_STR} ê¸°ì¤€)</span></h1>
    <div style='background-color:#f4f4f4; padding:15px; border-radius:10px; border:1px solid #ddd;'>
        <h3>ğŸ“Š {summary_text}</h3>
        <p>
            <a href="../index.html">ğŸ”™ ëŒ€ì‹œë³´ë“œ</a> | 
            <a href="{list_filename}" target="_blank">ğŸ—‚ï¸ ì „ì²´ ìˆ˜ì§‘ ëª©ë¡ ë³´ê¸°</a>
        </p>
    </div>
    <hr>
    """
    full_report = report_header + (report_body if total_change_count > 0 else "<p>âœ… ê¸ˆì¼ ë³€ë™ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.</p>")
    
    filename = f"report_{TODAY_STR}.html"
    with open(os.path.join(REPORT_DIR, filename), "w", encoding="utf-8") as f:
        f.write(full_report)
        
    with open(latest_file, "w", encoding="utf-8") as f:
        json.dump(today_results, f, ensure_ascii=False)

    update_index_page()

    dashboard_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/"
    report_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{filename}"
    list_url = f"https://{GITHUB_USER}.github.io/{REPO_NAME}/reports/{list_filename}"
    
    if total_change_count > 0:
        payload = {
            "text": f"ğŸ“¢ *[{TODAY_STR} {TIME_STR}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… *ìš”ì•½:* {summary_text}\n\nğŸ‘‰ *ë³€ê²½ ë¦¬í¬íŠ¸:* {report_url}\nğŸ—‚ï¸ *ì „ì²´ ëª©ë¡:* {list_url}\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ (ì•„ì¹´ì´ë¸Œ):* {dashboard_url}"
        }
    else:
        payload = {
            "text": f"ğŸ“‹ *[{TODAY_STR} {TIME_STR}] ê²½ìŸì‚¬ ë™í–¥ ë³´ê³ * \n\nâœ… íŠ¹ì´ì‚¬í•­ ì—†ìŒ\nğŸ“‚ *ëŒ€ì‹œë³´ë“œ (ì•„ì¹´ì´ë¸Œ):* {dashboard_url}"
        }
        
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json=payload)
        print("âœ… ìŠ¬ë™ ì•Œë¦¼ ì™„ë£Œ")
    else:
        print("âš ï¸ ìŠ¬ë™ URL ì—†ìŒ")

if __name__ == "__main__":
    main()
