"""
[í”„ë¡œì íŠ¸] ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ ëª¨ë‹ˆí„°ë§ ìë™í™” ì‹œìŠ¤í…œ (V39)
[ì‘ì„±ì] ìµœì§€ì› (GTM Strategy)
[ì—…ë°ì´íŠ¸] 2026-02-01 (U+, KTM, Skylife HTML ë¶„ì„ ê¸°ë°˜ ì „ìš© ë¡œì§ íƒ‘ì¬ / ìŠ¬ë™ ì „ì²´ëª©ë¡ ë³µêµ¬)
"""

import os
import json
import time
import glob
import re
import traceback
from datetime import datetime, timedelta, timezone
import requests
from urllib.parse import urljoin # ìƒëŒ€ê²½ë¡œ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# =========================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜
# =========================================================
GITHUB_USER = "rodolfochoi911-lgtm" 
REPO_NAME = "competitor-monitor" 
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL") 

DATA_DIR = "data"
DOCS_DIR = "docs"
REPORT_DIR = "docs/reports"

KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
FILE_TIMESTAMP = NOW.strftime("%Y%m%d_%H%M%S")
DISPLAY_DATE = NOW.strftime("%Y-%m-%d")
DISPLAY_TIME = NOW.strftime("%H:%M:%S")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def setup_driver():
    print("ğŸš— ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def remove_popups(driver):
    try:
        driver.execute_script("""
            var popups = document.querySelectorAll('.popup, .modal, .layer, .dimmed, .overlay, .toast, .banner, #popup, .close');
            popups.forEach(function(element) { element.remove(); });
        """)
    except: pass

def scroll_to_bottom(driver):
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(5): 
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height: break
            last_height = new_height
    except: pass

def clean_html(html_source):
    if not html_source: return ""
    soup = BeautifulSoup(html_source, 'html.parser')
    for tag in soup(['script', 'style', 'meta', 'noscript', 'header', 'footer', 'iframe', 'button', 'input', 'nav', 'aside']):
        tag.decompose()
    return body.prettify() if (body := soup.find('body')) else "No Content"

def load_previous_data():
    json_files = glob.glob(os.path.join(DATA_DIR, "data_*.json"))
    if not json_files: return {}
    json_files.sort()
    latest_file = json_files[-1]
    try:
        with open(latest_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return {}

def analyze_content_changes(old_html, new_html):
    soup_old = BeautifulSoup(old_html, 'html.parser')
    soup_new = BeautifulSoup(new_html, 'html.parser')
    if soup_old.get_text().strip() != soup_new.get_text().strip(): return "âœï¸ í…ìŠ¤íŠ¸ ìˆ˜ì •"
    imgs_old = set([i['src'] for i in soup_old.find_all('img') if i.get('src')])
    imgs_new = set([i['src'] for i in soup_new.find_all('img') if i.get('src')])
    if imgs_old != imgs_new: return "ğŸ–¼ï¸ ì´ë¯¸ì§€ êµì²´"
    return "ğŸ¨ ë ˆì´ì•„ì›ƒ ë³€ê²½"

# =========================================================
# [ì „ìš© ì¶”ì¶œê¸° 1] U+ ìœ ëª¨ë°”ì¼ (HTML ë¶„ì„ ê¸°ë°˜)
# =========================================================
def extract_uplus_mobile(driver):
    cards_data = {}
    try:
        # ì»¨í…Œì´ë„ˆ: .going-list-wrap
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".going-list-wrap"))
        )
        # ì•„ì´í…œ: a.cardList-wrap (li ì•„ë‹˜, í†µì§œ aíƒœê·¸)
        items = container.find_elements(By.CSS_SELECTOR, "a.cardList-wrap")
        print(f"    [U+ Mobile] Found {len(items)} items")
        
        for item in items:
            try:
                href = item.get_attribute('href')
                if not href or "javascript" in href: continue
                
                # ìƒëŒ€ê²½ë¡œ ì²˜ë¦¬
                final_url = urljoin("https://www.uplusumobile.com", href)

                # ì œëª©: .main-title (ì—†ìœ¼ë©´ .cardList-desc)
                try: title = item.find_element(By.CSS_SELECTOR, ".main-title").text.strip()
                except: title = "ì œëª© ì—†ìŒ"
                
                # ì´ë¯¸ì§€: .cardList-img img
                img_src = ""
                try:
                    img = item.find_element(By.CSS_SELECTOR, ".cardList-img img")
                    img_src = img.get_attribute("src")
                except: pass
                
                cards_data[final_url] = {"title": title, "img": img_src}
            except: continue
    except Exception as e:
        print(f"    âš ï¸ U+ ìœ ëª¨ë°”ì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return cards_data

# =========================================================
# [ì „ìš© ì¶”ì¶œê¸° 2] KTM ëª¨ë°”ì¼ (ntcartseq ì†ì„± ë¶„ì„)
# =========================================================
def extract_ktm_mobile(driver):
    cards_data = {}
    try:
        # ì»¨í…Œì´ë„ˆ: .event-list
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".event-list"))
        )
        items = container.find_elements(By.TAG_NAME, "li")
        print(f"    [KTM Mobile] Found {len(items)} items")
        
        for item in items:
            try:
                # a íƒœê·¸ ì°¾ê¸°
                link_el = item.find_element(By.TAG_NAME, "a")
                
                # í•µì‹¬: hrefê°€ ì•„ë‹ˆë¼ 'ntcartseq' ì†ì„± ê°’ì„ ê°€ì ¸ì™€ì•¼ í•¨
                seq = link_el.get_attribute("ntcartseq")
                
                if seq:
                    # URL ì§ì ‘ ì¡°ë¦½
                    final_url = f"https://www.ktmmobile.com/event/eventDetail.do?ntcartSeq={seq}"
                else:
                    # í˜¹ì‹œ hrefê°€ ìˆëŠ” ê²½ìš° ëŒ€ë¹„
                    href = link_el.get_attribute('href')
                    if href and "javascript" not in href: final_url = href
                    else: continue

                # ì œëª©
                try: title = item.find_element(By.CSS_SELECTOR, ".event-list__title__sub").text.strip()
                except: title = "ì œëª© ì—†ìŒ"
                
                # ì´ë¯¸ì§€
                img_src = ""
                try:
                    img = item.find_element(By.TAG_NAME, "img")
                    img_src = img.get_attribute("src")
                except: pass

                cards_data[final_url] = {"title": title, "img": img_src}
            except: continue
    except Exception as e:
        print(f"    âš ï¸ KTM ëª¨ë°”ì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return cards_data

# =========================================================
# [ì „ìš© ì¶”ì¶œê¸° 3] ìŠ¤ì¹´ì´ë¼ì´í”„ (Grid êµ¬ì¡° ë¶„ì„)
# =========================================================
def extract_skylife(driver):
    cards_data = {}
    try:
        # ì»¨í…Œì´ë„ˆ: div.grid (Tailwind í´ë˜ìŠ¤ í™œìš©)
        # body > div... ë“± ë³µì¡í•œ ê²½ë¡œ ëŒ€ì‹  í•µì‹¬ í´ë˜ìŠ¤ë¡œ ì°¾ìŒ
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.grid.grid-cols-3"))
        )
        items = container.find_elements(By.TAG_NAME, "a")
        print(f"    [Skylife] Found {len(items)} items")
        
        for item in items:
            try:
                href = item.get_attribute('href')
                if not href or "javascript" in href: continue
                
                final_url = urljoin("https://www.skylife.co.kr", href)

                # ì œëª© (p íƒœê·¸ ì¤‘ í°íŠ¸ êµµì€ ê²ƒ)
                try: title = item.find_element(By.CSS_SELECTOR, "p.font-semibold").text.strip()
                except: title = "ì œëª© ì—†ìŒ"
                
                # ì´ë¯¸ì§€
                img_src = ""
                try:
                    img = item.find_element(By.TAG_NAME, "img")
                    img_src = img.get_attribute("srcset").split(" ")[0] # srcset ì²˜ë¦¬
                    if not img_src: img_src = img.get_attribute("src")
                except: pass

                cards_data[final_url] = {"title": title, "img": img_src}
            except: continue
    except Exception as e:
        print(f"    âš ï¸ ìŠ¤ì¹´ì´ë¼ì´í”„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return cards_data

# [ê¸°ì¡´] Legacy Simple (SKT ë‹¤ì´ë ‰íŠ¸ìš©)
def extract_legacy_simple(driver, container_selector, site_name):
    cards_data = {} 
    try:
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, container_selector))
        )
        items = container.find_elements(By.TAG_NAME, "li")
        print(f"    [Legacy] Found {len(items)} items in {site_name}")

        for item in items:
            try:
                try: link_el = item.find_element(By.TAG_NAME, "a")
                except: 
                    if item.tag_name == 'a': link_el = item
                    else: continue

                href = link_el.get_attribute('href')
                if not href: continue

                title = item.text.strip().split("\n")[0]
                if not title:
                    try: title = item.find_element(By.TAG_NAME, "img").get_attribute("alt")
                    except: title = "ì œëª© ì—†ìŒ"
                
                img_src = ""
                try:
                    img = item.find_element(By.TAG_NAME, "img")
                    img_src = img.get_attribute("src")
                except: pass

                cards_data[href] = {"title": title, "img": img_src}
            except: continue
        return cards_data
    except Exception as e:
        print(f"    âš ï¸ [Legacy] ì¶”ì¶œ ì‹¤íŒ¨ ({site_name}): {e}")
        return {}

# [ê¸°ì¡´] JS í•´ë… ë¡œì§ (í—¬ë¡œëª¨ë°”ì¼, 7ëª¨ë°”ì¼ìš©)
def extract_special_js(driver, container_selector, site_name):
    cards_data = {} 
    try:
        container = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, container_selector))
        )
        
        items = []
        if "í—¬ë¡œëª¨ë°”ì¼" in site_name:
            try: items = container.find_element(By.CSS_SELECTOR, ".event-list").find_elements(By.TAG_NAME, "li")
            except: items = container.find_elements(By.TAG_NAME, "li")
        elif "SK 7ì„¸ë¸ëª¨ë°”ì¼" in site_name:
            try: 
                groups = container.find_elements(By.CSS_SELECTOR, ".event-group")
                for g in groups: items.extend(g.find_elements(By.TAG_NAME, "li"))
            except: items = container.find_elements(By.TAG_NAME, "li")
        
        print(f"    [Special] Found {len(items)} items in {site_name}")

        for item in items:
            try:
                link_el = item if item.tag_name == 'a' else None
                if not link_el:
                    try: link_el = item.find_element(By.TAG_NAME, "a")
                    except: continue
                
                href = link_el.get_attribute('href')
                onclick = str(link_el.get_attribute('onclick'))
                
                final_url = ""
                
                if "í—¬ë¡œëª¨ë°”ì¼" in site_name and "fncEventDetail" in onclick:
                    if m := re.search(r"(\d+)", onclick):
                        final_url = f"https://direct.lghellovision.net/event/viewEventDetail.do?idxOfEvent={m.group(1)}"
                
                elif "SK 7ì„¸ë¸ëª¨ë°”ì¼" in site_name and "fnSearchView" in onclick:
                    if m := re.search(r"['\"]([^'\"]+)['\"]", onclick):
                        final_url = f"https://www.sk7mobile.com/bnef/event/eventIngView.do?cntId={m.group(1)}"
                
                if not final_url:
                    if href and "javascript" not in href: final_url = href
                    elif href: final_url = href

                if not final_url: continue

                title = item.text.strip().split("\n")[0]
                if not title:
                    try: title = item.find_element(By.TAG_NAME, "img").get_attribute("alt")
                    except: title = "ì œëª© ì—†ìŒ"
                
                img_src = ""
                try:
                    img = item.find_element(By.TAG_NAME, "img")
                    img_src = img.get_attribute("src")
                except: pass

                cards_data[final_url] = {"title": title, "img": img_src}
            except: continue
        return cards_data
    except Exception as e:
        print(f"    âš ï¸ [Special] ì¶”ì¶œ ì‹¤íŒ¨ ({site_name}): {e}")
        return {}

def extract_single_page_content(driver, selector):
    try:
        container = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
        return {driver.current_url: {"title": "SKT Air ë©”ì¸", "img": "", "content": clean_html(container.get_attribute('outerHTML'))}}
    except: return {}

def crawl_site_logic(driver, site_name, base_url, pagination_param=None, target_selector=None):
    print(f"ğŸš€ [{site_name}] ì‹œì‘...")
    collected_items = {} 
    
    if site_name == "SKT Air":
        driver.get(base_url); time.sleep(3)
        return extract_single_page_content(driver, target_selector)

    page = 1
    while True:
        target_url = base_url
        if pagination_param == "#": target_url = f"{base_url}#{page}"
        elif pagination_param == "p": target_url = f"{base_url}?{pagination_param}={page}"
            
        driver.get(target_url)
        if pagination_param == "#": driver.refresh(); time.sleep(2)
        time.sleep(3)
        remove_popups(driver)
        scroll_to_bottom(driver)
        
        # [í•µì‹¬] ì‚¬ì´íŠ¸ë³„ ë¡œì§ ë¶„ê¸° (3ëŒ€ì¥ ë³µêµ¬)
        if site_name == "U+ ìœ ëª¨ë°”ì¼":
            page_data = extract_uplus_mobile(driver)
        elif site_name == "KTM ëª¨ë°”ì¼":
            page_data = extract_ktm_mobile(driver)
        elif site_name == "ìŠ¤ì¹´ì´ë¼ì´í”„":
            page_data = extract_skylife(driver)
        elif site_name == "SKT ë‹¤ì´ë ‰íŠ¸":
            page_data = extract_legacy_simple(driver, target_selector, site_name)
        else: # í—¬ë¡œ, 7ëª¨ë°”ì¼
            page_data = extract_special_js(driver, target_selector, site_name)
        
        if not page_data: break
        
        new_cnt = 0
        for href, info in page_data.items():
            # ì´ë¯¸ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜ëœ hrefê°€ ë“¤ì–´ì˜´
            if href not in collected_items:
                collected_items[href] = info
                new_cnt += 1
        
        if new_cnt == 0: break
        if not pagination_param: break
        
        page += 1
        if page > 10: break

    print(f"  ğŸ” [{site_name}] ìƒì„¸ ë¶„ì„ ({len(collected_items)}ê±´)...")
