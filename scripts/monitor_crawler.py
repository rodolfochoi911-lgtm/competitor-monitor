import os
import json
import time
import random
import datetime
import pytz
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# --- [ì„¤ì •] ---
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

# í•œêµ­ ì‹œê°„ ê¸°ì¤€ 'ì–´ì œ' ë‚ ì§œ ê³„ì‚°
TZ_KST = pytz.timezone('Asia/Seoul')
NOW = datetime.datetime.now(TZ_KST)
YESTERDAY = NOW - datetime.timedelta(days=1)

# í¬ë§· ì •ì˜
YESTERDAY_FULL = YESTERDAY.strftime('%Y-%m-%d') # 2026-02-01
YESTERDAY_DOT = YESTERDAY.strftime('%y.%m.%d')   # 25.02.01

print(f"ğŸ“… íƒ€ê²Ÿ ë‚ ì§œ: {YESTERDAY_FULL}")

# --- [1. ë¸Œë¼ìš°ì € ì„¤ì •] ---
def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--lang=ko_KR")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

# --- [2. í¬ë¡¤ëŸ¬: ë½ë¿Œ] ---
def get_ppomppu_posts(driver):
    print("running ppomppu crawler...")
    posts = []
    base_url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=phone&page={}"
    
    for page in range(1, 6):
        try:
            driver.get(base_url.format(page))
            time.sleep(random.uniform(2, 4))
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.select('tr.baseList')
            
            for row in rows:
                time_span = row.select_one('.baseList-time')
                if not time_span: continue
                date_td = time_span.find_parent('td')
                if not date_td or not date_td.get('title'): continue
                
                raw_date = date_td['title'].split(' ')[0]
                post_date = "20" + raw_date.replace('.', '-')
                
                if post_date == YESTERDAY_FULL:
                    title_tag = row.select_one('.baseList-title')
                    if not title_tag: continue
                    title = title_tag.text.strip()
                    link = "https://www.ppomppu.co.kr/zboard/" + title_tag['href']
                    views = int(row.select_one('.baseList-views').text.strip() or 0)
                    comments = int(row.select_one('.baseList-c').text.strip() or 0)
                    
                    posts.append({'source': 'ppomppu', 'title': title, 'link': link, 'views': views, 'comments': comments})
        except Exception as e:
            print(f"Err Ppomppu p{page}: {e}")
    return posts

# --- [3. í¬ë¡¤ëŸ¬: ë””ì‹œ] ---
def get_dc_posts(driver):
    print("running dc crawler...")
    posts = []
    base_url = "https://gall.dcinside.com/mgallery/board/lists/?id=mvnogallery&page={}"
    
    for page in range(1, 10):
        try:
            driver.get(base_url.format(page))
            time.sleep(random.uniform(2, 4))
            
            if "ë””ì‹œì¸ì‚¬ì´ë“œì…ë‹ˆë‹¤" in driver.title and "ì•Œëœ°í°" not in driver.title:
                break

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.select('tr.ub-content.us-post')
            
            for row in rows:
                if row.get('data-type') == 'icon_notice': continue
                date_tag = row.select_one('.gall_date')
                if not date_tag or not date_tag.get('title'): continue
                
                post_date = date_tag['title'].split(' ')[0]
                
                if post_date == YESTERDAY_FULL:
                    title_tag = row.select_one('.gall_tit > a')
                    if not title_tag: continue
                    title = title_tag.text.strip()
                    link = "https://gall.dcinside.com" + title_tag['href']
                    views_tag = row.select_one('.gall_count')
                    views = int(views_tag.text.strip().replace(',', '')) if views_tag and views_tag.text.strip().isdigit() else 0
                    reply_tag = row.select_one('.reply_num')
                    comments = int(reply_tag.text.strip('[]')) if reply_tag else 0
                    
                    posts.append({'source': 'dc', 'title': title, 'link': link, 'views': views, 'comments': comments})
        except Exception as e:
            print(f"Err DC p{page}: {e}")
    return posts

# --- [4. ë¶„ì„ ë° ì•Œë¦¼] ---
def analyze_and_notify(p_posts, d_posts):
    total_posts = p_posts + d_posts
    
    if not total_posts:
        print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„° 0ê±´")
        os.makedirs('data/monitoring', exist_ok=True)
        with open(f'data/monitoring/data_{YESTERDAY_FULL}.json', 'w', encoding='utf-8') as f:
            json.dump([], f)
        return

    df = pd.DataFrame(total_posts)

    # 1. ì‹œì¥ ì§€ì§„ê³„
    p_cnt = len(p_posts)
    d_cnt = len(d_posts)
    p_status = "ğŸ”´ ê³¼ì—´" if p_cnt >= 100 else ("ğŸŸ¢ í‰ì˜¨" if p_cnt < 50 else "ğŸŸ¡ í™œë°œ")
    d_status = "ğŸ”´ ê³¼ì—´" if d_cnt >= 80 else ("ğŸŸ¢ í‰ì˜¨" if d_cnt < 30 else "ğŸŸ¡ í™œë°œ")

    # 2. ë¸Œëœë“œ ì ìœ ìœ¨ (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
    brands = {
        'ì„¸ë¸ëª¨ë°”ì¼': ['ì„¸ë¸','ì„¸ë¸ëª¨ë°”ì¼', '7ëª¨', 'sk7', 'skí…”ë§í¬'],
        'ë¦¬ë¸Œì— ': ['ë¦¬ë¸Œì— ', 'ë¦¬ë¸Œëª¨ë°”ì¼', 'kb','ë¦¬ë¸Œ'],
        'ì´ì•¼ê¸°': ['ì´ì•¼ê¸°', 'ì´ì•¼ê¸°ëª¨ë°”ì¼'],
        'í—¬ë¡œëª¨ë°”ì¼': ['í—¬ë¡œëª¨ë°”ì¼', 'í—¬ëª¨','í—¬ë¡œ'],
        'í”„ë¦¬í‹°': ['í”„ë¦¬í‹°'],
        'í‹°í”ŒëŸ¬ìŠ¤': ['í‹°í”ŒëŸ¬ìŠ¤', 'í‹°í”Œ'],
        # [ì‹ ê·œ ì¶”ê°€]
        'í‹°ë‹¤ì´ë ‰íŠ¸': ['í‹°ë‹¤ì´ë ‰íŠ¸', 'í‹°ë‹¤', 'të‹¤ì´ë ‰íŠ¸', 'të‹¤'],
        'KTì— ëª¨ë°”ì¼': ['ktì— ëª¨ë°”ì¼', 'ì— ëª¨ë°”ì¼', 'ì— ëª¨', 'ktm'],
        'ìŠ¤ì¹´ì´ë¼ì´í”„': ['ìŠ¤ì¹´ì´ë¼ì´í”„', 'ìŠ¤ì¹´ì´', 'skylife'], 
        'ìœ ëª¨ë°”ì¼': ['ìœ ëª¨ë°”ì¼', 'ìœ ëª¨', 'uëª¨ë°”ì¼', 'ìœ ì•Œëª¨'],
        'SKT_Air': ['sktì—ì–´', 'skt air','ì—ì–´'] 
    }
    
    sov_lines = []
    # ì¹´ìš´íŠ¸ê°€ 0ì´ì–´ë„ ëª¨ë‘ í‘œì‹œ (ì ìœ ìœ¨ 0ë„ ì •ë³´ì´ë¯€ë¡œ)
    for b_name, keywords in brands.items():
        cnt = df[df['title'].apply(lambda x: any(k in x.lower() for k in keywords))].shape[0]
        sov_lines.append(f"â€¢ {b_name}: {cnt}ê±´")
    sov_msg = "\n".join(sov_lines)

    # 3. Top 5 í¬ë§·íŒ…
    def format_list(sub_df):
        if sub_df.empty: return "ì—†ìŒ"
        top5 = sub_df.sort_values(by='views', ascending=False).head(5)
        lines = []
        for idx, row in top5.iterrows():
            title = row['title']
            icon = ""
            if any(k in title for k in ['0ì›', 'ë¬´ì œí•œ', 'í‰ìƒ', 'ëŒ€ë€', 'ê³µì§œ']): icon = " ğŸ’°"
            lines.append(f"{idx+1}. <{row['link']}|{title}>{icon} (ğŸ‘ï¸ {row['views']:,} / ğŸ’¬ {row['comments']})")
        return "\n".join(lines)

    slack_text = f"""
*[ğŸ“Š {YESTERDAY_FULL} ì•Œëœ°í° ì‹œì¥ ëª¨ë‹ˆí„°ë§]*

*ğŸŒ¡ï¸ ì‹œì¥ í™œì„±ë„ (ì–´ì œ ê²Œì‹œê¸€ ìˆ˜)*
â€¢ ë½ë¿Œ: {p_status} ({p_cnt}ê°œ)
â€¢ ë””ì‹œ: {d_status} ({d_cnt}ê°œ)

*ğŸ“ˆ ë¸Œëœë“œ ì–¸ê¸‰ëŸ‰ (SOV)*
{sov_msg}

*1ï¸âƒ£ ë½ë¿Œ íœ´ëŒ€í°í¬ëŸ¼ (Top 5)*
{format_list(pd.DataFrame(p_posts))}

*2ï¸âƒ£ ë””ì‹œ ì•Œëœ°í° ê°¤ëŸ¬ë¦¬ (Top 5)*
{format_list(pd.DataFrame(d_posts))}

ğŸ‘‰ <https://github.com/rodolfochoi911-lgtm/competitor-monitor|ì›¹ ëŒ€ì‹œë³´ë“œ ë°”ë¡œê°€ê¸°>
    """
    
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json={"text": slack_text})

    os.makedirs('data/monitoring', exist_ok=True)
    with open(f'data/monitoring/data_{YESTERDAY_FULL}.json', 'w', encoding='utf-8') as f:
        json.dump(total_posts, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    driver = get_driver()
    try:
        p_data = get_ppomppu_posts(driver)
        d_data = get_dc_posts(driver)
        analyze_and_notify(p_data, d_data)
        print("âœ… ì‘ì—… ì™„ë£Œ")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        driver.quit()
