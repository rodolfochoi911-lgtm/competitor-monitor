import os
import json
import time
import random
import datetime
import pytz
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# --- [ì„¤ì •] ---
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

# í•œêµ­ ì‹œê°„ ê¸°ì¤€ 'ì–´ì œ' ë‚ ì§œ ê³„ì‚°
TZ_KST = pytz.timezone('Asia/Seoul')
NOW = datetime.datetime.now(TZ_KST)
YESTERDAY = NOW - datetime.timedelta(days=1)

# í¬ë§· ì •ì˜
YESTERDAY_FULL = YESTERDAY.strftime('%Y-%m-%d') # 2026-02-01
YESTERDAY_DOT = YESTERDAY.strftime('%y.%m.%d')   # 25.02.01

print(f"ğŸ“… íƒ€ê²Ÿ ë‚ ì§œ: {YESTERDAY_FULL}")

# --- [1. ë¸Œë¼ìš°ì € ì„¤ì •] ---
def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--lang=ko_KR")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

# --- [2. í¬ë¡¤ëŸ¬: ë½ë¿Œ (ëˆ„ë½ ë°©ì§€ ê°•í™”)] ---
def get_ppomppu_posts(driver):
    print("running ppomppu crawler...")
    posts = []
    base_url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=phone&page={}"
    
    # ë½ë¿ŒëŠ” ì¤‘ê°„ì— ê³µì§€ ë“±ìœ¼ë¡œ ë‚ ì§œê°€ ì„ì¼ ìˆ˜ ìˆì–´, 
    # ë‚ ì§œê°€ ì§€ë‚¬ë‹¤ê³  ë°”ë¡œ ëŠì§€ ì•Šê³  10í˜ì´ì§€ê¹Œì§€ëŠ” ë¬´ì¡°ê±´ í›‘ìŠµë‹ˆë‹¤.
    for page in range(1, 11): 
        try:
            print(f"  - Ppomppu page {page} scanning...")
            driver.get(base_url.format(page))
            time.sleep(random.uniform(1.0, 2.0))
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.select('tr.baseList')
            
            if not rows: break
            
            page_match_count = 0
            
            for row in rows:
                time_span = row.select_one('.baseList-time')
                if not time_span: continue
                date_td = time_span.find_parent('td')
                if not date_td or not date_td.get('title'): continue
                
                raw_date = date_td['title'].split(' ')[0] # 26.02.01
                post_date = "20" + raw_date.replace('.', '-') # 2026-02-01
                
                # ì •í™•íˆ ì–´ì œ ë‚ ì§œì¸ ê²ƒë§Œ ìˆ˜ì§‘
                if post_date == YESTERDAY_FULL:
                    title_tag = row.select_one('.baseList-title')
                    if not title_tag: continue
                    
                    title = title_tag.text.strip()
                    link = "https://www.ppomppu.co.kr/zboard/" + title_tag['href']
                    views = int(row.select_one('.baseList-views').text.strip() or 0)
                    comments = int(row.select_one('.baseList-c').text.strip() or 0)
                    
                    posts.append({'source': 'ppomppu', 'title': title, 'link': link, 'views': views, 'comments': comments})
                    page_match_count += 1
            
            # í•œ í˜ì´ì§€ë¥¼ ë‹¤ í„¸ì—ˆëŠ”ë° ì–´ì œ ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ê³ , 
            # í˜ì´ì§€ë„ 5í˜ì´ì§€ê°€ ë„˜ì–´ê°€ë©´ ê·¸ë•Œ ê·¸ë§Œë‘  (ì•ˆì „ì¥ì¹˜)
            if page_match_count == 0 and page > 5:
                # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë§ˆì§€ë§‰ìœ¼ë¡œ ì²´í¬: í˜„ì¬ í˜ì´ì§€ì˜ ë‚ ì§œë“¤ì´ ì „ë¶€ ê³¼ê±°ì¸ê°€?
                # (ì´ ë¡œì§ì€ ë³µì¡í•˜ë‹ˆ ì¼ë‹¨ 10í˜ì´ì§€ ê°•ì œ ìŠ¤ìº”ìœ¼ë¡œ ìœ ì§€)
                pass

        except Exception as e:
            print(f"Err Ppomppu p{page}: {e}")
            
    return posts

# --- [3. í¬ë¡¤ëŸ¬: ë””ì‹œ (ê¸°ì¡´ ìœ ì§€)] ---
def get_dc_posts(driver):
    print("running dc crawler...")
    posts = []
    base_url = "https://gall.dcinside.com/mgallery/board/lists/?id=mvnogallery&page={}"
    page = 1
    
    while True:
        try:
            print(f"  - DC page {page} scanning...")
            driver.get(base_url.format(page))
            time.sleep(random.uniform(1.0, 2.0))
            
            if "ë””ì‹œì¸ì‚¬ì´ë“œì…ë‹ˆë‹¤" in driver.title and "ì•Œëœ°í°" not in driver.title:
                print("  - Blocked by DC.")
                break

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.select('tr.ub-content.us-post')
            
            if not rows: break
                
            stop_crawling = False
            
            for row in rows:
                if row.get('data-type') == 'icon_notice': continue
                date_tag = row.select_one('.gall_date')
                if not date_tag or not date_tag.get('title'): continue
                
                post_date = date_tag['title'].split(' ')[0] # 2026-02-01
                
                if post_date == YESTERDAY_FULL:
                    title_tag = row.select_one('.gall_tit > a')
                    if not title_tag: continue
                    title = title_tag.text.strip()
                    link = "https://gall.dcinside.com" + title_tag['href']
                    views_tag = row.select_one('.gall_count')
                    views = int(views_tag.text.strip().replace(',', '')) if views_tag and views_tag.text.strip().isdigit() else 0
                    reply_tag = row.select_one('.reply_num')
                    comments = int(reply_tag.text.strip('[]')) if reply_tag else 0
                    
                    posts.append({'source': 'dc', 'title': title, 'link': link, 'views': views, 'comments': comments})
                
                elif post_date < YESTERDAY_FULL:
                    stop_crawling = True
            
            if stop_crawling:
                break
            
            page += 1
            if page > 50: break 
            
        except Exception as e:
            print(f"Err DC p{page}: {e}")
            break
            
    return posts

# --- [4. ë¶„ì„, ì €ì¥, ì•Œë¦¼] ---
def analyze_and_notify(p_posts, d_posts):
    total_posts = p_posts + d_posts
    
    df = pd.DataFrame(total_posts)
    p_cnt = len(p_posts)
    d_cnt = len(d_posts)
    
    # [ìˆ˜ì •] ì»¤ë®¤ë‹ˆí‹° í™œì„±ë„ ê¸°ì¤€ (í•˜ë“œì½”ë”© - ì¶”í›„ í‰ê· ìœ¼ë¡œ ë³€ê²½ ì¶”ì²œ)
    p_status = "ğŸ”´ ê³¼ì—´" if p_cnt >= 180 else ("ğŸŸ¢ í‰ì˜¨" if p_cnt < 80 else "ğŸŸ¡ í™œë°œ")
    d_status = "ğŸ”´ ê³¼ì—´" if d_cnt >= 600 else ("ğŸŸ¢ í‰ì˜¨" if d_cnt < 300 else "ğŸŸ¡ í™œë°œ")

    # ë¸Œëœë“œ í‚¤ì›Œë“œ
    brands = {
        'ì„¸ë¸ëª¨ë°”ì¼': ['ì„¸ë¸ëª¨ë°”ì¼', '7ëª¨', 'sk7', 'skí…”ë§í¬'],
        'ëª¨ë¹™': ['ëª¨ë¹™'],
        'ë¦¬ë¸Œì— ': ['ë¦¬ë¸Œì— ', 'ë¦¬ë¸Œëª¨ë°”ì¼', 'kb'],
        'ì´ì•¼ê¸°': ['ì´ì•¼ê¸°', 'ì´ì•¼ê¸°ëª¨ë°”ì¼'],
        'í—¬ë¡œëª¨ë°”ì¼': ['í—¬ë¡œëª¨ë°”ì¼', 'í—¬ëª¨'],
        'í”„ë¦¬í‹°': ['í”„ë¦¬í‹°'],
        'í‹°í”ŒëŸ¬ìŠ¤': ['í‹°í”ŒëŸ¬ìŠ¤', 'í‹°í”Œ'],
        'í‹°ë‹¤ì´ë ‰íŠ¸': ['í‹°ë‹¤ì´ë ‰íŠ¸', 'í‹°ë‹¤', 'të‹¤ì´ë ‰íŠ¸', 'të‹¤'],
        'KTì— ëª¨ë°”ì¼': ['ktì— ëª¨ë°”ì¼', 'ì— ëª¨ë°”ì¼', 'ì— ëª¨', 'ktm'],
        'ìŠ¤ì¹´ì´ë¼ì´í”„': ['ìŠ¤ì¹´ì´ë¼ì´í”„', 'ìŠ¤ì¹´ë¼', 'skylife'],
        'ìœ ëª¨ë°”ì¼': ['ìœ ëª¨ë°”ì¼', 'ìœ ëª¨', 'uëª¨ë°”ì¼', 'ìœ ì•Œëª¨'],
        'SKT_Air': ['sktì—ì–´', 'skt air']
    }
    
    brand_counts = {}
    sov_lines = []
    
    # [ìˆ˜ì •] ì„¸ë¸ëª¨ë°”ì¼ ë§í¬ ìˆ˜ì§‘ìš© ë¦¬ìŠ¤íŠ¸
    seven_mobile_links = []

    if not df.empty:
        for b_name, keywords in brands.items():
            # í•´ë‹¹ ë¸Œëœë“œê°€ í¬í•¨ëœ í–‰ë§Œ í•„í„°ë§
            filtered_df = df[df['title'].apply(lambda x: any(k in x.lower() for k in keywords))]
            cnt = len(filtered_df)
            brand_counts[b_name] = int(cnt)
            
            # 0ê±´ì´ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ X (ìˆ¨ê¹€ ì²˜ë¦¬)
            if cnt > 0:
                sov_lines.append(f"â€¢ {b_name}: {cnt}ê±´")
                
                # [ìˆ˜ì •] ì„¸ë¸ëª¨ë°”ì¼ì´ë©´ ë§í¬ ìˆ˜ì§‘
                if b_name == 'ì„¸ë¸ëª¨ë°”ì¼':
                    for _, row in filtered_df.iterrows():
                        seven_mobile_links.append(f"  â”” <{row['link']}|{row['title']}>")
    else:
        for b_name in brands: brand_counts[b_name] = 0
        sov_lines = ["ë°ì´í„° ì—†ìŒ"]

    sov_msg = "\n".join(sov_lines)
    
    # ì„¸ë¸ëª¨ë°”ì¼ ë§í¬ê°€ ìˆë‹¤ë©´ ë©”ì‹œì§€ì— ì¶”ê°€
    if seven_mobile_links:
        sov_msg += "\n\n*ğŸ“Œ ì„¸ë¸ëª¨ë°”ì¼ ì–¸ê¸‰ ê²Œì‹œê¸€:*\n" + "\n".join(seven_mobile_links)

    # Top 5 í¬ë§·íŒ…
    def format_list(sub_df):
        if sub_df.empty: return "ì—†ìŒ"
        top5 = sub_df.sort_values(by='views', ascending=False).head(5)
        lines = []
        for idx, row in top5.iterrows():
            title = row['title']
            icon = ""
            if any(k in title for k in ['0ì›', 'ë¬´ì œí•œ', 'í‰ìƒ', 'ëŒ€ë€', 'ê³µì§œ']): icon = " ğŸ’°"
            lines.append(f"â€¢ <{row['link']}|{title}>{icon} (ğŸ‘ï¸ {row['views']:,} / ğŸ’¬ {row['comments']})")
        
        top5_data = top5[['title', 'link', 'views', 'comments']].to_dict('records')
        return "\n".join(lines), top5_data

    p_msg, p_top5 = format_list(pd.DataFrame(p_posts))
    d_msg, d_top5 = format_list(pd.DataFrame(d_posts))

    # --- [ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì €ì¥] ---
    history_file = 'data/dashboard_history.json'
    history_data = []
    
    if os.path.exists(history_file):
        with open(history_file, 'r', encoding='utf-8') as f:
            try: history_data = json.load(f)
            except: pass
    
    today_entry = {
        "date": YESTERDAY_FULL,
        "total_volume": { "ppomppu": p_cnt, "dc": d_cnt },
        "brand_sov": brand_counts,
        "top_posts": { "ppomppu": p_top5, "dc": d_top5 }
    }
    
    history_data = [d for d in history_data if d['date'] != YESTERDAY_FULL]
    history_data.append(today_entry)
    history_data.sort(key=lambda x: x['date'])
    
    os.makedirs('data', exist_ok=True)
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history_data, f, ensure_ascii=False, indent=4)

    # --- [ìŠ¬ë™ ì „ì†¡: ì œëª© ë° ì„¹ì…˜ëª… ìˆ˜ì •ë¨] ---
    slack_text = f"""
*[ğŸ“Š {YESTERDAY_FULL} ì•Œëœ°í° ì»¤ë®¤ë‹ˆí‹° ëª¨ë‹ˆí„°ë§]*

*ğŸŒ¡ï¸ ì»¤ë®¤ë‹ˆí‹° í™œì„±ë„*
â€¢ ë½ë¿Œ: {p_status} ({p_cnt}ê°œ)
â€¢ ë””ì‹œ: {d_status} ({d_cnt}ê°œ)

*ğŸ“ˆ ë¸Œëœë“œ ì–¸ê¸‰ëŸ‰*
{sov_msg}

*1ï¸âƒ£ ë½ë¿Œ íœ´ëŒ€í°í¬ëŸ¼ (Top 5)*
{p_msg}

*2ï¸âƒ£ ë””ì‹œ ì•Œëœ°í° ê°¤ëŸ¬ë¦¬ (Top 5)*
{d_msg}

ğŸ‘‰ <https://rodolfochoi911-lgtm.github.io/competitor-monitor/|ì›¹ ëŒ€ì‹œë³´ë“œ í™•ì¸í•˜ê¸°>
    """
    
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json={"text": slack_text})

    # ë°±ì—… ì €ì¥
    os.makedirs('data/monitoring', exist_ok=True)
    with open(f'data/monitoring/data_{YESTERDAY_FULL}.json', 'w', encoding='utf-8') as f:
        json.dump(total_posts, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    driver = get_driver()
    try:
        p_data = get_ppomppu_posts(driver)
        d_data = get_dc_posts(driver)
        analyze_and_notify(p_data, d_data)
        print("âœ… ì‘ì—… ì™„ë£Œ")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        driver.quit()
