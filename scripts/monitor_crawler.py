import os
import json
import time
import random
import datetime
import pytz
import requests
import pandas as pd
from bs4 import BeautifulSoup

# --- [Selenium ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬] ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# --- [ì„¤ì •] ---
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

# í•œêµ­ ì‹œê°„ ê¸°ì¤€ 'ì–´ì œ' ë‚ ì§œ ê³„ì‚°
TZ_KST = pytz.timezone('Asia/Seoul')
NOW = datetime.datetime.now(TZ_KST)
YESTERDAY = NOW - datetime.timedelta(days=1)

# í¬ë§· 1: 2025-02-01 (ì €ì¥ ë° ë¡œì§ìš©)
YESTERDAY_FULL = YESTERDAY.strftime('%Y-%m-%d')
# í¬ë§· 2: 02-01 (ë””ì‹œ ë“± ë¹„êµìš©)
YESTERDAY_SHORT = YESTERDAY.strftime('%m-%d')
# í¬ë§· 3: 25.02.01 (ë½ë¿Œ ë¹„êµìš© - ì—°ë„ 2ìë¦¬)
YESTERDAY_DOT = YESTERDAY.strftime('%y.%m.%d')

print(f"ğŸ“… íƒ€ê²Ÿ ë‚ ì§œ: {YESTERDAY_FULL} (ì–´ì œ ë°ì´í„° ìˆ˜ì§‘)")

# --- [1. ë¸Œë¼ìš°ì € ì„¤ì • (Anti-Bot)] ---
def get_driver():
    chrome_options = Options()
    
    # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (ì„œë²„ìš©)
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    
    # ë´‡ íƒì§€ íšŒí”¼ (User-Agent ë³€ì¡°)
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option("useAutomationExtension", False)
    
    # ë“œë¼ì´ë²„ ìë™ ì„¤ì¹˜ ë° ì‹¤í–‰
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    return driver

# --- [2. í¬ë¡¤ë§ í•¨ìˆ˜] ---

def get_ppomppu_posts(driver):
    """ë½ë¿Œ íœ´ëŒ€í°í¬ëŸ¼ í¬ë¡¤ë§ (Selenium)"""
    print("running ppomppu crawler...")
    posts = []
    base_url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=phone&page={}"
    
    for page in range(1, 11): # 1~10í˜ì´ì§€ íƒìƒ‰
        try:
            driver.get(base_url.format(page))
            time.sleep(random.uniform(2, 4)) # íœ´ë¨¼ í„°ì¹˜ (2~4ì´ˆ ëŒ€ê¸°)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.select('tr.common-list0, tr.common-list1')
            
            if not rows: break
            
            stop_flag = False
            for row in rows:
                date_tag = row.select_one('.board_date')
                if not date_tag: continue
                date_text = date_tag.text.strip() 
                
                # ì˜¤ëŠ˜ ê¸€(ì‹œê°„ í‘œì‹œ)ì€ ìŠ¤í‚µ
                if ":" in date_text: continue
                
                # ë‚ ì§œ ë¹„êµ (ë½ë¿ŒëŠ” YY.MM.DD)
                if date_text == YESTERDAY_DOT:
                    title_elem = row.select_one('font.list_title') or row.select_one('a')
                    title = title_elem.text.strip()
                    link = "https://www.ppomppu.co.kr/zboard/" + row.select_one('a')['href']
                    
                    views = int(row.select_one('.board_hit').text.strip().replace(',', ''))
                    
                    # ëŒ“ê¸€ ìˆ˜ íŒŒì‹±
                    comment_span = row.select_one('.list_comment2')
                    comments = int(comment_span.text.strip()) if comment_span else 0
                    
                    posts.append({
                        'source': 'ppomppu', 'title': title, 'link': link,
                        'views': views, 'comments': comments
                    })
                elif date_text < YESTERDAY_DOT:
                    stop_flag = True
            
            if stop_flag and page > 3: 
                break
                
        except Exception as e:
            print(f"Error on Ppomppu page {page}: {e}")
            
    return posts

def get_dc_posts(driver):
    """ë””ì‹œ ì•Œëœ°í° ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ (Selenium)"""
    print("running dc crawler...")
    posts = []
    base_url = "https://gall.dcinside.com/mgallery/board/lists/?id=mvno&page={}"
    
    target_date_dc = YESTERDAY.strftime('%m.%d') 
    
    for page in range(1, 15):
        try:
            driver.get(base_url.format(page))
            time.sleep(random.uniform(2, 4))
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.select('tr.ub-content')
            
            if not rows: break
            
            for row in rows:
                if 'ub-notice' in row.get('class', []): continue 
                
                date_tag = row.select_one('.gall_date')
                if not date_tag: continue
                date_text = date_tag.text.strip() 
                
                if ":" in date_text: continue
                
                if date_text == target_date_dc:
                    title = row.select_one('.gall_tit > a').text.strip()
                    link = "https://gall.dcinside.com" + row.select_one('.gall_tit > a')['href']
                    views = int(row.select_one('.gall_count').text.strip().replace(',', '') or 0)
                    
                    reply_tag = row.select_one('.reply_num')
                    comments = int(reply_tag.text.strip('[]')) if reply_tag else 0
                    
                    posts.append({
                        'source': 'dc', 'title': title, 'link': link,
                        'views': views, 'comments': comments
                    })
            
            if page > 10 and len(posts) == 0: 
                break
                
        except Exception as e:
            print(f"Error on DC page {page}: {e}")
            
    return posts

# --- [3. ë¶„ì„ ë° ì•Œë¦¼] ---

def analyze_and_notify(p_posts, d_posts):
    total_posts = p_posts + d_posts
    df = pd.DataFrame(total_posts)
    
    # [ìˆ˜ì •] ë°ì´í„°ê°€ ì—†ì–´ë„ ì—ëŸ¬ ì•ˆ ë‚˜ê²Œ ì•ˆì „ì¥ì¹˜ ì¶”ê°€
    if df.empty:
        print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ 0ê±´ì…ë‹ˆë‹¤.")
        if SLACK_WEBHOOK_URL:
            requests.post(SLACK_WEBHOOK_URL, json={"text": f"âš ï¸ [{YESTERDAY_FULL}] ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ 0ê±´ì…ë‹ˆë‹¤. (ì‚¬ì´íŠ¸ ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸ í•„ìš”)"})
        return

    # 1. ì‹œì¥ ì§€ì§„ê³„
    p_cnt = len(p_posts)
    d_cnt = len(d_posts)
    p_status = "ğŸ”´ ê³¼ì—´" if p_cnt >= 100 else ("ğŸŸ¢ í‰ì˜¨" if p_cnt < 50 else "ğŸŸ¡ í™œë°œ")
    d_status = "ğŸ”´ ê³¼ì—´" if d_cnt >= 80 else ("ğŸŸ¢ í‰ì˜¨" if d_cnt < 30 else "ğŸŸ¡ í™œë°œ")

    # 2. ë¸Œëœë“œ ì ìœ ìœ¨
    brands = {
        'ì„¸ë¸ëª¨ë°”ì¼': ['ì„¸ë¸ëª¨ë°”ì¼', '7ëª¨', 'sk7', 'skí…”ë§í¬'],
        'ëª¨ë¹™': ['ëª¨ë¹™'],
        'ë¦¬ë¸Œì— ': ['ë¦¬ë¸Œì— ', 'ë¦¬ë¸Œëª¨ë°”ì¼', 'kb'],
        'ì´ì•¼ê¸°': ['ì´ì•¼ê¸°', 'ì´ì•¼ê¸°ëª¨ë°”ì¼']
    }
    
    sov_lines = []
    for b_name, keywords in brands.items():
        cnt = df[df['title'].apply(lambda x: any(k in x for k in keywords))].shape[0]
        sov_lines.append(f"â€¢ {b_name}: {cnt}ê±´")
    sov_msg = "\n".join(sov_lines)

    # 3. Top 5 í¬ë§·íŒ… (ì•ˆì „ì¥ì¹˜ í¬í•¨)
    def format_list(sub_df):
        if sub_df.empty: return "ì—†ìŒ"
        
        # ì¡°íšŒìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        if 'views' not in sub_df.columns: return "ë°ì´í„° ì—†ìŒ"
            
        top5 = sub_df.sort_values(by='views', ascending=False).head(5)
        lines = []
        for idx, row in top5.iterrows():
            title = row['title']
            icon = ""
            if any(k in title for k in ['0ì›', 'ë¬´ì œí•œ', 'í‰ìƒ', 'ëŒ€ë€', 'ê³µì§œ']):
                icon = " ğŸ’°"
            lines.append(f"{idx+1}. <{row['link']}|{title}>{icon} (ğŸ‘ï¸ {row['views']:,} / ğŸ’¬ {row['comments']})")
        return "\n".join(lines)

    # ë©”ì‹œì§€ ì‘ì„±
    slack_text = f"""
*[ğŸ“Š {YESTERDAY_FULL} ì•Œëœ°í° ì‹œì¥ ëª¨ë‹ˆí„°ë§]*

*ğŸŒ¡ï¸ ì‹œì¥ í™œì„±ë„ (ì–´ì œ ê²Œì‹œê¸€ ìˆ˜)*
â€¢ ë½ë¿Œ: {p_status} ({p_cnt}ê°œ)
â€¢ ë””ì‹œ: {d_status} ({d_cnt}ê°œ)

*ğŸ“ˆ ë¸Œëœë“œ ì–¸ê¸‰ëŸ‰ (SOV)*
{sov_msg}

*1ï¸âƒ£ ë½ë¿Œ íœ´ëŒ€í°í¬ëŸ¼ (Top 5)*
{format_list(pd.DataFrame(p_posts))}

*2ï¸âƒ£ ë””ì‹œ ì•Œëœ°í° ê°¤ëŸ¬ë¦¬ (Top 5)*
{format_list(pd.DataFrame(d_posts))}

ğŸ‘‰ <https://github.com/YOUR_ID/YOUR_REPO|ì›¹ ëŒ€ì‹œë³´ë“œ ë°”ë¡œê°€ê¸°>
    """
    
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json={"text": slack_text})
        print("Slack sent.")
    else:
        print(slack_text)

    # ë°ì´í„° ì €ì¥
    os.makedirs('data/monitoring', exist_ok=True)
    with open(f'data/monitoring/data_{YESTERDAY_FULL}.json', 'w', encoding='utf-8') as f:
        json.dump(total_posts, f, ensure_ascii=False, indent=4)

# --- [ë©”ì¸ ì‹¤í–‰] ---
if __name__ == "__main__":
    driver = get_driver()
    try:
        p_data = get_ppomppu_posts(driver)
        d_data = get_dc_posts(driver)
        analyze_and_notify(p_data, d_data)
    except Exception as e:
        print(f"Critical Error: {e}")
    finally:
        driver.quit()
