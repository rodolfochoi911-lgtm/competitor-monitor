import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import os
import datetime
import pytz
import time

# --- [ì„¤ì •] ---
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
TZ_KST = pytz.timezone('Asia/Seoul')
YESTERDAY = (datetime.datetime.now(TZ_KST) - datetime.timedelta(days=1)).strftime('%Y-%m-%d')
# YESTERDAY = "2024-05-20" # í…ŒìŠ¤íŠ¸ìš© ë‚ ì§œ ê³ ì • ì‹œ ì‚¬ìš©

# í—¤ë” ì„¤ì • (ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# --- [1. í¬ë¡¤ëŸ¬ í•¨ìˆ˜] ---

def get_ppomppu_posts(target_date):
    """ë½ë¿Œ íœ´ëŒ€í°í¬ëŸ¼/ê¸°íƒ€ì •ë³´ í¬ë¡¤ë§"""
    posts = []
    page = 1
    
    # ë½ë¿Œ íœ´ëŒ€í°í¬ëŸ¼ URL (í•„ìš” ì‹œ ê¸°íƒ€ì •ë³´ URLë¡œ ë³€ê²½ ê°€ëŠ¥)
    base_url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=phone&page={}" 
    
    while True:
        res = requests.get(base_url.format(page), headers=HEADERS)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('tr.common-list0, tr.common-list1') # ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ row

        if not rows: break
        
        stop_crawling = False
        for row in rows:
            try:
                # ë‚ ì§œ íŒŒì‹± (ë½ë¿ŒëŠ” ì˜¤ëŠ˜:ì‹œê°„, ê³¼ê±°:YY.MM.DD)
                date_tag = row.select_one('.board_date')
                if not date_tag: continue
                date_text = date_tag.text.strip()
                
                # ë‚ ì§œ ë³€í™˜ ë¡œì§
                if ":" in date_text: # ì˜¤ëŠ˜ ë‚ ì§œ (ì‹œê°„ë§Œ í‘œì‹œë¨) -> ì–´ì œ ë°ì´í„° ì•„ë‹ˆë¯€ë¡œ íŒ¨ìŠ¤
                    continue
                
                # '24.05.20' -> '2024-05-20' ë³€í™˜
                post_date = "20" + date_text.replace('.', '-')
                
                if post_date == target_date:
                    title_tag = row.select_one('font.list_title') or row.select_one('a')
                    title = title_tag.text.strip()
                    link = "https://www.ppomppu.co.kr/zboard/" + row.select_one('a')['href']
                    
                    # ì¡°íšŒìˆ˜ / ëŒ“ê¸€ìˆ˜
                    views = int(row.select_one('.board_hit').text.strip().replace(',', ''))
                    comment_span = row.select_one('.list_comment2')
                    comments = int(comment_span.text.strip()) if comment_span else 0
                    
                    posts.append({
                        'source': 'ppomppu', 'title': title, 'link': link,
                        'views': views, 'comments': comments, 'date': post_date
                    })
                elif post_date < target_date:
                    # ì–´ì œë³´ë‹¤ ì´ì „ ë‚ ì§œê°€ ë‚˜ì˜¤ë©´ í¬ë¡¤ë§ ì¢…ë£Œ (ë” ë³¼ í•„ìš” ì—†ìŒ)
                    stop_crawling = True
                    break
            except Exception as e:
                continue
        
        if stop_crawling or page > 20: # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 20í˜ì´ì§€ê¹Œì§€ë§Œ íƒìƒ‰
            break
        page += 1
        time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
    return posts

def get_dc_posts(target_date):
    """ë””ì‹œ ì•Œëœ°í° ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§"""
    posts = []
    page = 1
    base_url = "https://gall.dcinside.com/mgallery/board/lists/?id=mvnogallery&page={}"
    # ë””ì‹œ ë‚ ì§œ í¬ë§·: MM.DD (ì—°ë„ ì—†ìŒ ì£¼ì˜, í•´ ë„˜ê¸¸ ë•Œ ì˜ˆì™¸ì²˜ë¦¬ í•„ìš”í•˜ë‚˜ ì—¬ê¸°ì„  ìƒëµ)
    target_md = target_date[5:].replace('-', '.') # '05-20' -> '05.20'
    
    while True:
        res = requests.get(base_url.format(page), headers=HEADERS)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('tr.ub-content')
        
        if not rows: break
        
        stop_crawling = False
        for row in rows:
            try:
                if row.select_one('.gall_writer.ub-writer > .ip'): continue # ìœ ë™IP ê¸€ ì œì™¸? (ì„ íƒì‚¬í•­)
                if 'ub-notice' in row.get('class', []): continue # ê³µì§€ ì œì™¸
                
                date_text = row.select_one('.gall_date').text.strip()
                
                # ë””ì‹œ: ì˜¤ëŠ˜(ì‹œê°„), ì–´ì œì´ì „(MM.DD)
                if ":" in date_text: continue
                
                if date_text == target_md:
                    title = row.select_one('.gall_tit > a').text.strip()
                    link = "https://gall.dcinside.com" + row.select_one('.gall_tit > a')['href']
                    views = int(row.select_one('.gall_count').text.strip().replace(',', '') or 0)
                    
                    # ëŒ“ê¸€ìˆ˜ (ì œëª© ì˜†ì— [3] ì´ëŸ°ì‹ìœ¼ë¡œ ìˆê±°ë‚˜ ë³„ë„ íƒœê·¸)
                    reply_tag = row.select_one('.reply_num')
                    comments = int(reply_tag.text.strip('[]')) if reply_tag else 0
                    
                    posts.append({
                        'source': 'dc', 'title': title, 'link': link,
                        'views': views, 'comments': comments, 'date': target_date
                    })
                # ë””ì‹œëŠ” ë‚ ì§œ ì •ë ¬ì´ ê¼¬ì¼ ë•Œê°€ ìˆì–´ì„œ, ë‚ ì§œê°€ ë‹¤ë¥´ë‹¤ê³  ë°”ë¡œ breakí•˜ë©´ ìœ„í—˜í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¼ë°˜ì ìœ¼ë¡  ê°€ëŠ¥
                elif date_text < target_md: 
                    stop_crawling = True
                    break
            except:
                continue
        
        if stop_crawling or page > 30: # ë””ì‹œëŠ” ë¦¬ì  ì´ ë¹ ë¥´ë‹ˆ ì¢€ ë” ê¹Šê²Œ íƒìƒ‰
            break
        page += 1
        time.sleep(0.5)
        
    return posts

# --- [2. ë°ì´í„° ë¶„ì„ ë° ë©”ì‹œì§€ í¬ë§·íŒ…] ---

def analyze_and_notify(p_posts, d_posts):
    total_posts = p_posts + d_posts
    df = pd.DataFrame(total_posts)
    
    if df.empty:
        print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1) ì‹œì¥ í™œì„±ë„ (ì§€ì§„ê³„) - ì„ì˜ ê¸°ì¤€ê°’ ì„¤ì • (ë‚˜ì¤‘ì—” ê³¼ê±° í‰ê· ìœ¼ë¡œ ëŒ€ì²´)
    p_count = len(p_posts)
    d_count = len(d_posts)
    p_status = "ğŸ”´ ê³¼ì—´" if p_count > 100 else ("ğŸŸ¢ í‰ì˜¨" if p_count < 50 else "ğŸŸ¡ í™œë°œ")
    
    # 2) ë¸Œëœë“œ ì ìœ ìœ¨ (SOV)
    brands = {
        'ì„¸ë¸ëª¨ë°”ì¼': ['ì„¸ë¸ëª¨ë°”ì¼', '7ëª¨', 'sk7', 'skí…”ë§í¬'],
        'ëª¨ë¹™': ['ëª¨ë¹™'],
        'ë¦¬ë¸Œì— ': ['ë¦¬ë¸Œì— ', 'ë¦¬ë¸Œëª¨ë°”ì¼'],
        'ì´ì•¼ê¸°': ['ì´ì•¼ê¸°', 'ì´ì•¼ê¸°ëª¨ë°”ì¼']
    }
    sov_msg = ""
    for name, keywords in brands.items():
        count = df[df['title'].apply(lambda x: any(k in x for k in keywords))].shape[0]
        sov_msg += f"â€¢ {name}: {count}ê±´\n"

    # 3) Top 5 ì„ ì • ë° íƒœê¹… í•¨ìˆ˜
    def format_top5(sub_df):
        msg = ""
        # ì¡°íšŒìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        top5 = sub_df.sort_values(by='views', ascending=False).head(5)
        
        for idx, row in top5.iterrows():
            title = row['title']
            # í•˜ì´ë¼ì´íŠ¸ íƒœê¹…
            icon = ""
            if any(k in title for k in ['0ì›', 'ë¬´ì œí•œ', 'í‰ìƒ', 'ëŒ€ë€']): icon = " ğŸ’°"
            
            msg += f"{idx+1}. <{row['link']}|{title}>{icon} (ğŸ‘ï¸ {row['views']:,} / ğŸ’¬ {row['comments']})\n"
        return msg

    # --- [ìŠ¬ë™ ë©”ì‹œì§€ ì¡°í•©] ---
    slack_text = f"""
*[ğŸ“Š {YESTERDAY} ì•Œëœ°í° ì‹œì¥ ëª¨ë‹ˆí„°ë§]*

*ğŸŒ¡ï¸ ì‹œì¥ í™œì„±ë„ (ì–´ì œ ê²Œì‹œê¸€ ìˆ˜)*
â€¢ ë½ë¿Œ: {p_status} ({p_count}ê°œ)
â€¢ ë””ì‹œ: {d_count}ê°œ

*ğŸ“ˆ ë¸Œëœë“œ ì–¸ê¸‰ëŸ‰ (SOV)*
{sov_msg}
*1ï¸âƒ£ ë½ë¿Œ íœ´ëŒ€í°í¬ëŸ¼ (Top 5)*
{format_top5(pd.DataFrame(p_posts).reset_index(drop=True))}

*2ï¸âƒ£ ë””ì‹œ ì•Œëœ°í° ê°¤ëŸ¬ë¦¬ (Top 5)*
{format_top5(pd.DataFrame(d_posts).reset_index(drop=True))}

ğŸ‘‰ <https://your-github-username.github.io/repo-name|ì›¹ ëŒ€ì‹œë³´ë“œ í™•ì¸í•˜ê¸°>
    """
    
    # ìŠ¬ë™ ì „ì†¡
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json={"text": slack_text})
    else:
        print("SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(slack_text)

    # ë°ì´í„° ì €ì¥ (JSON)
    os.makedirs('data/monitoring', exist_ok=True)
    file_path = f'data/monitoring/data_{YESTERDAY}.json'
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(total_posts, f, ensure_ascii=False, indent=4)

# --- [ë©”ì¸ ì‹¤í–‰] ---
if __name__ == "__main__":
    print(f"[{YESTERDAY}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    ppomppu_data = get_ppomppu_posts(YESTERDAY)
    dc_data = get_dc_posts(YESTERDAY)
    
    analyze_and_notify(ppomppu_data, dc_data)
    print("ì™„ë£Œ.")
