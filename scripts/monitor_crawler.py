import os
import json
import time
import random
import datetime
import pytz
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup
from collections import Counter

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# --- [ì„¤ì •] ---
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

TZ_KST = pytz.timezone('Asia/Seoul')
NOW = datetime.datetime.now(TZ_KST)
YESTERDAY = NOW - datetime.timedelta(days=1)

YESTERDAY_FULL = YESTERDAY.strftime('%Y-%m-%d') # 2026-02-01
YESTERDAY_DOT = YESTERDAY.strftime('%y.%m.%d')   # 25.02.01

print(f"ğŸ“… íƒ€ê²Ÿ ë‚ ì§œ: {YESTERDAY_FULL}")

# --- [1. ë¸Œë¼ìš°ì € ì„¤ì •] ---
def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--lang=ko_KR")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

# --- [2. í¬ë¡¤ëŸ¬: ë½ë¿Œ (í•˜ì´ë¸Œë¦¬ë“œ ì •ë°€ ìˆ˜ì§‘)] ---
def get_ppomppu_posts(driver):
    print("running ppomppu crawler...")
    posts = []
    base_url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=phone&page={}"
    
    for page in range(1, 21): 
        try:
            driver.get(base_url.format(page))
            time.sleep(random.uniform(1.0, 2.0))
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.find_all('tr') # ëª¨ë“  í–‰ íƒìƒ‰
            
            valid_cnt_in_page = 0
            
            for row in rows:
                title_elem = row.select_one('font.list_title') or row.select_one('a')
                if not title_elem: continue
                
                # [ìˆ˜ì •] ë‚ ì§œ ì¶”ì¶œ ìš°ì„ ìˆœìœ„ ê°•í™” (ì •í™•ë„ í–¥ìƒ)
                post_date = ""
                
                # 1. .baseList-time í´ë˜ìŠ¤ê°€ ìˆë‹¤ë©´ ê°€ì¥ ì •í™• (ë¶€ëª¨ tdì˜ title ì†ì„±)
                time_span = row.select_one('.baseList-time')
                if time_span:
                    date_td = time_span.find_parent('td')
                    if date_td and date_td.get('title'):
                        raw_date = date_td['title'].split(' ')[0]
                        post_date = "20" + raw_date.replace('.', '-')
                
                # 2. ì—†ë‹¤ë©´ title ì†ì„± ì§ì ‘ ê²€ìƒ‰
                if not post_date:
                    date_td = row.find('td', title=re.compile(r'\d{2}\.\d{2}\.\d{2}'))
                    if date_td:
                        raw_date = date_td['title'].split(' ')[0]
                        post_date = "20" + raw_date.replace('.', '-')
                
                # 3. ê·¸ë˜ë„ ì—†ë‹¤ë©´ í…ìŠ¤íŠ¸ ì •ê·œì‹ (ìµœí›„ì˜ ìˆ˜ë‹¨)
                if not post_date:
                    date_match = re.search(r'\d{2}\.\d{2}\.\d{2}', row.text)
                    if date_match:
                        post_date = "20" + date_match.group().replace('.', '-')

                # ë‚ ì§œ ì¼ì¹˜ í™•ì¸
                if post_date == YESTERDAY_FULL:
                    link_elem = row.select_one('a[href*="view.php"]')
                    if not link_elem: continue
                    
                    title = title_elem.text.strip()
                    link = "https://www.ppomppu.co.kr/zboard/" + link_elem['href']
                    
                    # [ìˆ˜ì •] ì¡°íšŒìˆ˜/ëŒ“ê¸€ìˆ˜ ìš°ì„ ìˆœìœ„ ê°•í™” (ìˆ«ì ê¼¬ì„ ë°©ì§€)
                    views, comments = 0, 0
                    
                    # 1. í´ë˜ìŠ¤ë¡œ ì°¾ê¸° (ê°€ì¥ ì •í™•)
                    view_tag = row.select_one('.baseList-views')
                    cmt_tag = row.select_one('.baseList-c') or row.select_one('.list_comment2')
                    
                    if view_tag:
                        views = int(view_tag.text.strip().replace(',', '') or 0)
                    else:
                        # 2. í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸° (ë¦¬ìŠ¤í¬ ìˆìŒ)
                        views_match = re.findall(r'\d{1,3}(?:,\d{3})*', row.text)
                        if views_match: views = int(views_match[-1].replace(',', ''))
                    
                    if cmt_tag:
                        comments = int(cmt_tag.text.strip().replace(',', '') or 0)

                    posts.append({'source': 'ppomppu', 'title': title, 'link': link, 'views': views, 'comments': comments})
                    valid_cnt_in_page += 1
            
            if valid_cnt_in_page == 0 and page > 10:
                print("  - No more posts found. Stopping.")
                break

        except Exception as e:
            print(f"Err Ppomppu p{page}: {e}")
            
    return posts

# --- [3. í¬ë¡¤ëŸ¬: ë””ì‹œ (ê¸°ì¡´ ìœ ì§€)] ---
def get_dc_posts(driver):
    print("running dc crawler...")
    posts = []
    base_url = "https://gall.dcinside.com/mgallery/board/lists/?id=mvnogallery&page={}"
    
    for page in range(1, 51):
        try:
            driver.get(base_url.format(page))
            time.sleep(random.uniform(1.0, 2.0))
            
            if "ë””ì‹œì¸ì‚¬ì´ë“œì…ë‹ˆë‹¤" in driver.title and "ì•Œëœ°í°" not in driver.title:
                break

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.select('tr.ub-content.us-post')
            
            if not rows: break
                
            stop_crawling = False
            
            for row in rows:
                if row.get('data-type') == 'icon_notice': continue
                date_tag = row.select_one('.gall_date')
                if not date_tag or not date_tag.get('title'): continue
                
                post_date = date_tag['title'].split(' ')[0]
                
                if post_date == YESTERDAY_FULL:
                    title_tag = row.select_one('.gall_tit > a')
                    if not title_tag: continue
                    title = title_tag.text.strip()
                    link = "https://gall.dcinside.com" + title_tag['href']
                    views_tag = row.select_one('.gall_count')
                    views = int(views_tag.text.strip().replace(',', '')) if views_tag and views_tag.text.strip().isdigit() else 0
                    reply_tag = row.select_one('.reply_num')
                    comments = int(reply_tag.text.strip('[]')) if reply_tag else 0
                    
                    posts.append({'source': 'dc', 'title': title, 'link': link, 'views': views, 'comments': comments})
                elif post_date < YESTERDAY_FULL:
                    stop_crawling = True
            
            if stop_crawling: break
            
        except Exception as e:
            print(f"Err DC p{page}: {e}")
            break
            
    return posts

# --- [4. ë¶„ì„ ë¡œì§] ---
def extract_top_keywords(df):
    if df.empty: return []
    all_titles = " ".join(df['title'].tolist())
    all_titles = re.sub(r'[^\w\s]', ' ', all_titles)
    words = all_titles.split()
    
    # [ìˆ˜ì •] ë¶ˆìš©ì–´ ì¶”ê°€ (ìˆìŒ, ì•Œëœ° ë“±)
    stopwords = set([
        'ì§ˆë¬¸', 'í›„ê¸°', 'ì •ë³´', 'ìš”ê¸ˆì œ', 'ì•Œëœ°í°', 'ì¶”ì²œ', 'ìˆë‚˜ìš”', 'ë‚˜ìš”', 'ê°€ìš”', 'ê±´ê°€ìš”',
        'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì´ë²ˆë‹¬', '2ì›”', '1ì›”', 'ê·¼ë°', 'ì§„ì§œ', 'í˜¹ì‹œ', 'ì•„ë‹ˆ', 'ë„ˆë¬´',
        'ìœ ì‹¬', 'ë²ˆí˜¸ì´ë™', 'ê¸°ë³€', 'ì‹ ê·œ', 'ê°œí†µ', 'ëª¨ë°”ì¼', 'ì‚¬ëŒ', 'ìƒê°', 'ì§€ê¸ˆ', 'ì–´ì œ',
        'ì•½ì •', 'ê²°í•©', 'í• ì¸', 'ì¹´ë“œ', 'ë°ì´í„°', 'ë¬´ì œí•œ', 'í‰ìƒ', 'ê°œì›”', 'ë…„',
        'skt', 'kt', 'lg', 'lgu', 'sk', 'ktë§', 'lgu+', 'u+', 'skë§', 'í—¬ë¡œ',
        'vs', 'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ë­ì•¼', 'ì‹œë°œ', 'ì¡´ë‚˜', 'ã…‹ã…‹', 'ã…ã…', 'ã… ã… ',
        'ë¬¸ì˜', 'ì§ˆë¬¸ì¢€', 'ëŒ€í•´', 'ê´€ë ¨', 'ì–´ë–¤ê°€ìš”', 'ë¬´ìŠ¨', 'ì–´ë””', 'ì–´ë–»ê²Œ',
        'ì„ íƒ', 'ìœ„ì•½ê¸ˆ', 'ì¡°ê±´', 'ì •ì±…', 'ë¹„êµ', 'ë³€ê²½', 'ì´ë™', 'ì‚¬ìš©', 'ê°€ì…', 'í•´ì§€',
        'ìˆìŒ', 'ì•Œëœ°', 'ìš”ê¸ˆ', 'ë²ˆí˜¸', 'ì´ë™', 'í†µì‹ ì‚¬' # ì¶”ê°€ëœ ë…¸ì´ì¦ˆ
    ])
    filtered_words = [w for w in words if len(w) >= 2 and w.lower() not in stopwords]
    return Counter(filtered_words).most_common(10)

def analyze_and_notify(p_posts, d_posts):
    total_posts = p_posts + d_posts
    if not total_posts:
        print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„° 0ê±´")
        return

    df = pd.DataFrame(total_posts)
    p_cnt = len(p_posts)
    d_cnt = len(d_posts)
    
    p_status = "ğŸ”´ ê³¼ì—´" if p_cnt >= 180 else ("ğŸŸ¢ í‰ì˜¨" if p_cnt < 80 else "ğŸŸ¡ í™œë°œ")
    d_status = "ğŸ”´ ê³¼ì—´" if d_cnt >= 600 else ("ğŸŸ¢ í‰ì˜¨" if d_cnt < 300 else "ğŸŸ¡ í™œë°œ")

    # 1. ë¸Œëœë“œ ì ìœ ìœ¨ (ì„¸ë¸ëª¨ë°”ì¼ ê³ ì • ë…¸ì¶œ ë¡œì§ ì¶”ê°€)
    brands = {
        'ì„¸ë¸ëª¨ë°”ì¼': ['ì„¸ë¸ëª¨ë°”ì¼', '7ëª¨', 'sk7', 'skí…”ë§í¬'],
        'ëª¨ë¹™': ['ëª¨ë¹™'],
        'ë¦¬ë¸Œì— ': ['ë¦¬ë¸Œì— ', 'ë¦¬ë¸Œëª¨ë°”ì¼', 'kb'],
        'ì´ì•¼ê¸°': ['ì´ì•¼ê¸°', 'ì´ì•¼ê¸°ëª¨ë°”ì¼'],
        'í—¬ë¡œëª¨ë°”ì¼': ['í—¬ë¡œëª¨ë°”ì¼', 'í—¬ëª¨'],
        'í”„ë¦¬í‹°': ['í”„ë¦¬í‹°'],
        'í‹°í”ŒëŸ¬ìŠ¤': ['í‹°í”ŒëŸ¬ìŠ¤', 'í‹°í”Œ'],
        'í‹°ë‹¤ì´ë ‰íŠ¸': ['í‹°ë‹¤ì´ë ‰íŠ¸', 'í‹°ë‹¤', 'të‹¤ì´ë ‰íŠ¸', 'të‹¤'],
        'KTì— ëª¨ë°”ì¼': ['ktì— ëª¨ë°”ì¼', 'ì— ëª¨ë°”ì¼', 'ì— ëª¨', 'ktm'],
        'ìŠ¤ì¹´ì´ë¼ì´í”„': ['ìŠ¤ì¹´ì´ë¼ì´í”„', 'ìŠ¤ì¹´ë¼', 'skylife'],
        'ìœ ëª¨ë°”ì¼': ['ìœ ëª¨ë°”ì¼', 'ìœ ëª¨', 'uëª¨ë°”ì¼', 'ìœ ì•Œëª¨'],
        'SKT_Air': ['sktì—ì–´', 'skt air']
    }
    
    brand_counts = {}
    seven_links = [] # ì„¸ë¸ëª¨ë°”ì¼ ë§í¬ ìˆ˜ì§‘

    # ì¹´ìš´íŒ… ë¨¼ì € ìˆ˜í–‰
    for b_name, keywords in brands.items():
        filtered = df[df['title'].apply(lambda x: any(k in x.lower() for k in keywords))]
        brand_counts[b_name] = int(len(filtered))
        
        if b_name == 'ì„¸ë¸ëª¨ë°”ì¼' and len(filtered) > 0:
            for _, row in filtered.iterrows():
                seven_links.append(f"  â”” <{row['link']}|{row['title']}>")

    # [ìˆ˜ì •] ì¶œë ¥ ìˆœì„œ ì œì–´ (ì„¸ë¸ëª¨ë°”ì¼ 1ìˆœìœ„, ë‚˜ë¨¸ì§€ >0 ê±´ë§Œ)
    sov_lines = []
    
    # 1. ì„¸ë¸ëª¨ë°”ì¼ ë¬´ì¡°ê±´ ì¶œë ¥
    seven_cnt = brand_counts.get('ì„¸ë¸ëª¨ë°”ì¼', 0)
    sov_lines.append(f"â€¢ ì„¸ë¸ëª¨ë°”ì¼: {seven_cnt}ê±´")
    
    # 2. ë‚˜ë¨¸ì§€ ë¸Œëœë“œ (0ê±´ì´ë©´ ì œì™¸)
    for b_name, cnt in brand_counts.items():
        if b_name == 'ì„¸ë¸ëª¨ë°”ì¼': continue
        if cnt > 0:
            sov_lines.append(f"â€¢ {b_name}: {cnt}ê±´")

    sov_msg = "\n".join(sov_lines)

    # 2. í•« í‚¤ì›Œë“œ (Top 10)
    top_keywords = extract_top_keywords(df)
    keyword_msg = ""
    for word, count in top_keywords:
        keyword_msg += f"â€¢ {word}: {count}ê±´\n"
    if not keyword_msg: keyword_msg = "â€¢ íŠ¹ì´ì‚¬í•­ ì—†ìŒ"

    # Top 5 í¬ë§·íŒ…
    def format_list(sub_df):
        if sub_df.empty: return "ì—†ìŒ"
        top5 = sub_df.sort_values(by='views', ascending=False).head(5)
        lines = []
        for idx, row in top5.iterrows():
            title = row['title']
            icon = ""
            if any(k in title for k in ['0ì›', 'ë¬´ì œí•œ', 'í‰ìƒ', 'ëŒ€ë€', 'ê³µì§œ']): icon = " ğŸ’°"
            lines.append(f"â€¢ <{row['link']}|{title}>{icon} (ğŸ‘ï¸ {row['views']:,} / ğŸ’¬ {row['comments']})")
        top5_data = top5[['title', 'link', 'views', 'comments']].to_dict('records')
        return "\n".join(lines), top5_data

    p_msg, p_top5 = format_list(pd.DataFrame(p_posts))
    d_msg, d_top5 = format_list(pd.DataFrame(d_posts))

    # ë°ì´í„° ì €ì¥
    history_file = 'data/dashboard_history.json'
    history_data = []
    if os.path.exists(history_file):
        with open(history_file, 'r', encoding='utf-8') as f:
            try: history_data = json.load(f)
            except: pass
    
    today_entry = {
        "date": YESTERDAY_FULL,
        "total_volume": { "ppomppu": p_cnt, "dc": d_cnt },
        "brand_sov": brand_counts,
        "top_keywords": dict(top_keywords),
        "top_posts": { "ppomppu": p_top5, "dc": d_top5 }
    }
    
    history_data = [d for d in history_data if d['date'] != YESTERDAY_FULL]
    history_data.append(today_entry)
    history_data.sort(key=lambda x: x['date'])
    
    os.makedirs('data', exist_ok=True)
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history_data, f, ensure_ascii=False, indent=4)

    # ìŠ¬ë™ ì „ì†¡
    seven_block = ""
    if seven_links:
        seven_block = f"\n*ğŸ“Œ ì„¸ë¸ëª¨ë°”ì¼ ì–¸ê¸‰ ({len(seven_links)}ê±´)*\n" + "\n".join(seven_links)

    slack_text = f"""
*[ğŸ“Š {YESTERDAY_FULL} ì•Œëœ°í° ì»¤ë®¤ë‹ˆí‹° ëª¨ë‹ˆí„°ë§]*

*ğŸŒ¡ï¸ ì»¤ë®¤ë‹ˆí‹° í™œì„±ë„*
â€¢ ë½ë¿Œ: {p_status} ({p_cnt}ê°œ)
â€¢ ë””ì‹œ: {d_status} ({d_cnt}ê°œ)

*ğŸ“ˆ ë¸Œëœë“œ ì–¸ê¸‰ëŸ‰ (SOV)*
{sov_msg}{seven_block}

*ğŸ”¥ í•« í‚¤ì›Œë“œ (Top 10)*
{keyword_msg}

*1ï¸âƒ£ ë½ë¿Œ íœ´ëŒ€í°í¬ëŸ¼ (Top 5)*
{p_msg}

*2ï¸âƒ£ ë””ì‹œ ì•Œëœ°í° ê°¤ëŸ¬ë¦¬ (Top 5)*
{d_msg}

ğŸ‘‰ <https://rodolfochoi911-lgtm.github.io/competitor-monitor/|ì›¹ ëŒ€ì‹œë³´ë“œ í™•ì¸í•˜ê¸°>
    """
    
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json={"text": slack_text})

    # ë°±ì—… ì €ì¥
    os.makedirs('data/monitoring', exist_ok=True)
    with open(f'data/monitoring/data_{YESTERDAY_FULL}.json', 'w', encoding='utf-8') as f:
        json.dump(total_posts, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    driver = get_driver()
    try:
        p_data = get_ppomppu_posts(driver)
        d_data = get_dc_posts(driver)
        analyze_and_notify(p_data, d_data)
        print("âœ… ì‘ì—… ì™„ë£Œ")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        driver.quit()
