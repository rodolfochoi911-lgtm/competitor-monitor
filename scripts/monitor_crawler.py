import os
import json
import time
import random
import datetime
import pytz
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# --- [ì„¤ì •] ---
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")

# í•œêµ­ ì‹œê°„ ê¸°ì¤€ 'ì–´ì œ' ë‚ ì§œ ê³„ì‚°
TZ_KST = pytz.timezone('Asia/Seoul')
NOW = datetime.datetime.now(TZ_KST)
YESTERDAY = NOW - datetime.timedelta(days=1)

# í¬ë§· ì •ì˜
YESTERDAY_FULL = YESTERDAY.strftime('%Y-%m-%d') # 2026-02-01
YESTERDAY_DOT = YESTERDAY.strftime('%y.%m.%d')   # 25.02.01

print(f"ğŸ“… íƒ€ê²Ÿ ë‚ ì§œ: {YESTERDAY_FULL}")

# --- [1. ë¸Œë¼ìš°ì € ì„¤ì •] ---
def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--lang=ko_KR")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

# --- [2. í¬ë¡¤ëŸ¬: ë½ë¿Œ (ë¬´í•œ í˜ì´ì§• ìˆ˜ì •)] ---
def get_ppomppu_posts(driver):
    print("running ppomppu crawler...")
    posts = []
    base_url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=phone&page={}"
    page = 1
    
    while True:
        try:
            print(f"  - Ppomppu page {page} scanning...")
            driver.get(base_url.format(page))
            time.sleep(random.uniform(1.5, 3)) # ì†ë„ ì•½ê°„ ìƒí–¥
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.select('tr.baseList')
            
            if not rows:
                print("  - No rows found, stopping.")
                break
            
            page_data_count = 0
            stop_crawling = False
            
            for row in rows:
                time_span = row.select_one('.baseList-time')
                if not time_span: continue
                date_td = time_span.find_parent('td')
                if not date_td or not date_td.get('title'): continue
                
                raw_date = date_td['title'].split(' ')[0] # 26.02.01
                post_date = "20" + raw_date.replace('.', '-') # 2026-02-01
                
                if post_date == YESTERDAY_FULL:
                    title_tag = row.select_one('.baseList-title')
                    if not title_tag: continue
                    
                    title = title_tag.text.strip()
                    link = "https://www.ppomppu.co.kr/zboard/" + title_tag['href']
                    views = int(row.select_one('.baseList-views').text.strip() or 0)
                    comments = int(row.select_one('.baseList-c').text.strip() or 0)
                    
                    posts.append({'source': 'ppomppu', 'title': title, 'link': link, 'views': views, 'comments': comments})
                    page_data_count += 1
                
                elif post_date < YESTERDAY_FULL:
                    # ì–´ì œë³´ë‹¤ ì´ì „ ë‚ ì§œê°€ ë‚˜ì˜¤ë©´ ì¢…ë£Œ (ë‹¨, ì •ë ¬ ê¼¬ì„ ë°©ì§€ë¥¼ ìœ„í•´ í•´ë‹¹ í˜ì´ì§€ëŠ” ë‹¤ í›‘ê±°ë‚˜ ë°”ë¡œ ì¢…ë£Œ)
                    stop_crawling = True
            
            if stop_crawling:
                print("  - Reached past date. Stopping.")
                break
                
            if page_data_count == 0 and page > 10: # ì•ˆì „ì¥ì¹˜
                print("  - Too many empty pages.")
                break
                
            page += 1
            
        except Exception as e:
            print(f"Err Ppomppu p{page}: {e}")
            break
            
    return posts

# --- [3. í¬ë¡¤ëŸ¬: ë””ì‹œ (ë¬´í•œ í˜ì´ì§• ìˆ˜ì •)] ---
def get_dc_posts(driver):
    print("running dc crawler...")
    posts = []
    base_url = "https://gall.dcinside.com/mgallery/board/lists/?id=mvnogallery&page={}"
    page = 1
    
    while True:
        try:
            print(f"  - DC page {page} scanning...")
            driver.get(base_url.format(page))
            time.sleep(random.uniform(1.5, 3))
            
            if "ë””ì‹œì¸ì‚¬ì´ë“œì…ë‹ˆë‹¤" in driver.title and "ì•Œëœ°í°" not in driver.title:
                print("  - Blocked by DC.")
                break

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.select('tr.ub-content.us-post')
            
            if not rows:
                break
                
            stop_crawling = False
            
            for row in rows:
                if row.get('data-type') == 'icon_notice': continue
                date_tag = row.select_one('.gall_date')
                if not date_tag or not date_tag.get('title'): continue
                
                post_date = date_tag['title'].split(' ')[0] # 2026-02-01
                
                if post_date == YESTERDAY_FULL:
                    title_tag = row.select_one('.gall_tit > a')
                    if not title_tag: continue
                    
                    title = title_tag.text.strip()
                    link = "https://gall.dcinside.com" + title_tag['href']
                    
                    views_tag = row.select_one('.gall_count')
                    views = int(views_tag.text.strip().replace(',', '')) if views_tag and views_tag.text.strip().isdigit() else 0
                    
                    reply_tag = row.select_one('.reply_num')
                    comments = int(reply_tag.text.strip('[]')) if reply_tag else 0
                    
                    posts.append({'source': 'dc', 'title': title, 'link': link, 'views': views, 'comments': comments})
                
                elif post_date < YESTERDAY_FULL:
                    stop_crawling = True
            
            if stop_crawling:
                print("  - Reached past date. Stopping.")
                break
            
            page += 1
            if page > 50: break # ì•ˆì „ì¥ì¹˜
            
        except Exception as e:
            print(f"Err DC p{page}: {e}")
            break
            
    return posts

# --- [4. ë¶„ì„, ì €ì¥, ì•Œë¦¼] ---
def analyze_and_notify(p_posts, d_posts):
    total_posts = p_posts + d_posts
    
    # 1. ìˆ˜ì§‘ ë°ì´í„°ê°€ ì—†ì–´ë„ íˆìŠ¤í† ë¦¬ íŒŒì¼ì€ ê°±ì‹ í•´ì•¼ ëŒ€ì‹œë³´ë“œê°€ ì•ˆ ê¹¨ì§
    
    df = pd.DataFrame(total_posts)
    p_cnt = len(p_posts)
    d_cnt = len(d_posts)
    
    # ì§€ì§„ê³„
    p_status = "ğŸ”´ ê³¼ì—´" if p_cnt >= 100 else ("ğŸŸ¢ í‰ì˜¨" if p_cnt < 50 else "ğŸŸ¡ í™œë°œ")
    d_status = "ğŸ”´ ê³¼ì—´" if d_cnt >= 80 else ("ğŸŸ¢ í‰ì˜¨" if d_cnt < 30 else "ğŸŸ¡ í™œë°œ")

    # ë¸Œëœë“œ ì ìœ ìœ¨
    brands = {
        'ì„¸ë¸ëª¨ë°”ì¼': ['ì„¸ë¸ëª¨ë°”ì¼', '7ëª¨', 'sk7', 'skí…”ë§í¬'],
        'ëª¨ë¹™': ['ëª¨ë¹™'],
        'ë¦¬ë¸Œì— ': ['ë¦¬ë¸Œì— ', 'ë¦¬ë¸Œëª¨ë°”ì¼', 'kb'],
        'ì´ì•¼ê¸°': ['ì´ì•¼ê¸°', 'ì´ì•¼ê¸°ëª¨ë°”ì¼'],
        'í—¬ë¡œëª¨ë°”ì¼': ['í—¬ë¡œëª¨ë°”ì¼', 'í—¬ëª¨'],
        'í”„ë¦¬í‹°': ['í”„ë¦¬í‹°'],
        'í‹°í”ŒëŸ¬ìŠ¤': ['í‹°í”ŒëŸ¬ìŠ¤', 'í‹°í”Œ'],
        'í‹°ë‹¤ì´ë ‰íŠ¸': ['í‹°ë‹¤ì´ë ‰íŠ¸', 'í‹°ë‹¤', 'të‹¤ì´ë ‰íŠ¸', 'të‹¤'],
        'KTì— ëª¨ë°”ì¼': ['ktì— ëª¨ë°”ì¼', 'ì— ëª¨ë°”ì¼', 'ì— ëª¨', 'ktm'],
        'ìŠ¤ì¹´ì´ë¼ì´í”„': ['ìŠ¤ì¹´ì´ë¼ì´í”„', 'ìŠ¤ì¹´ë¼ì´í”„', 'ìŠ¤ì¹´ë¼', 'skylife'],
        'ìœ ëª¨ë°”ì¼': ['ìœ ëª¨ë°”ì¼', 'ìœ ëª¨', 'uëª¨ë°”ì¼', 'ìœ ì•Œëª¨'],
        'SKT_Air': ['sktì—ì–´', 'skt air']
    }
    
    brand_counts = {}
    sov_lines = []
    
    if not df.empty:
        for b_name, keywords in brands.items():
            cnt = df[df['title'].apply(lambda x: any(k in x.lower() for k in keywords))].shape[0]
            brand_counts[b_name] = int(cnt) # json ì§ë ¬í™” ìœ„í•´ int ë³€í™˜
            sov_lines.append(f"â€¢ {b_name}: {cnt}ê±´")
    else:
        for b_name in brands: brand_counts[b_name] = 0
        sov_lines = ["ë°ì´í„° ì—†ìŒ"]

    sov_msg = "\n".join(sov_lines)

    # Top 5 í¬ë§·íŒ… (ë¶ˆë › í¬ì¸íŠ¸ë¡œ ë³€ê²½)
    def format_list(sub_df):
        if sub_df.empty: return "ì—†ìŒ"
        top5 = sub_df.sort_values(by='views', ascending=False).head(5)
        lines = []
        for idx, row in top5.iterrows():
            title = row['title']
            icon = ""
            if any(k in title for k in ['0ì›', 'ë¬´ì œí•œ', 'í‰ìƒ', 'ëŒ€ë€', 'ê³µì§œ']): icon = " ğŸ’°"
            # ë²ˆí˜¸ ì œê±°í•˜ê³  ë¶ˆë › ì‚¬ìš©
            lines.append(f"â€¢ <{row['link']}|{title}>{icon} (ğŸ‘ï¸ {row['views']:,} / ğŸ’¬ {row['comments']})")
        
        # ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° ë°˜í™˜ (dict list)
        top5_data = top5[['title', 'link', 'views', 'comments']].to_dict('records')
        return "\n".join(lines), top5_data

    p_msg, p_top5 = format_list(pd.DataFrame(p_posts))
    d_msg, d_top5 = format_list(pd.DataFrame(d_posts))

    # --- [ëŒ€ì‹œë³´ë“œ ë°ì´í„° ëˆ„ì  ì €ì¥] ---
    history_file = 'data/dashboard_history.json'
    history_data = []
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    if os.path.exists(history_file):
        with open(history_file, 'r', encoding='utf-8') as f:
            try: history_data = json.load(f)
            except: pass
    
    # ì˜¤ëŠ˜ ë°ì´í„° ì¶”ê°€
    today_entry = {
        "date": YESTERDAY_FULL,
        "total_volume": {
            "ppomppu": p_cnt,
            "dc": d_cnt
        },
        "brand_sov": brand_counts,
        "top_posts": {
            "ppomppu": p_top5,
            "dc": d_top5
        }
    }
    
    # ì¤‘ë³µ ë‚ ì§œ ì œê±° (ë®ì–´ì“°ê¸°)
    history_data = [d for d in history_data if d['date'] != YESTERDAY_FULL]
    history_data.append(today_entry)
    # ë‚ ì§œìˆœ ì •ë ¬
    history_data.sort(key=lambda x: x['date'])
    
    os.makedirs('data', exist_ok=True)
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history_data, f, ensure_ascii=False, indent=4)
    print("âœ… Dashboard history updated.")

    # --- [ìŠ¬ë™ ì „ì†¡] ---
    slack_text = f"""
*[ğŸ“Š {YESTERDAY_FULL} ì•Œëœ°í° ì‹œì¥ ëª¨ë‹ˆí„°ë§]*

*ğŸŒ¡ï¸ ì‹œì¥ í™œì„±ë„*
â€¢ ë½ë¿Œ: {p_status} ({p_cnt}ê°œ)
â€¢ ë””ì‹œ: {d_status} ({d_cnt}ê°œ)

*ğŸ“ˆ ë¸Œëœë“œ ì–¸ê¸‰ëŸ‰ (SOV)*
{sov_msg}

*1ï¸âƒ£ ë½ë¿Œ íœ´ëŒ€í°í¬ëŸ¼ (Top 5)*
{p_msg}

*2ï¸âƒ£ ë””ì‹œ ì•Œëœ°í° ê°¤ëŸ¬ë¦¬ (Top 5)*
{d_msg}

ğŸ‘‰ <https://rodolfochoi911-lgtm.github.io/competitor-monitor/|ì›¹ ëŒ€ì‹œë³´ë“œ í™•ì¸í•˜ê¸°>
    """
    
    if SLACK_WEBHOOK_URL:
        requests.post(SLACK_WEBHOOK_URL, json={"text": slack_text})

    # ê°œë³„ ë‚ ì§œ ë°±ì—… ì €ì¥
    os.makedirs('data/monitoring', exist_ok=True)
    with open(f'data/monitoring/data_{YESTERDAY_FULL}.json', 'w', encoding='utf-8') as f:
        json.dump(total_posts, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    driver = get_driver()
    try:
        p_data = get_ppomppu_posts(driver)
        d_data = get_dc_posts(driver)
        analyze_and_notify(p_data, d_data)
        print("âœ… ì‘ì—… ì™„ë£Œ")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        driver.quit()
